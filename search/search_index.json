{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"Neural-Cherche <p>Neural Search</p> <p></p>"},{"location":"#installation","title":"Installation","text":"<p>To install neural_cherche use:</p> <pre><code>pip install neural-cherche\n</code></pre> <p>To run evaluation while training install:</p> <pre><code>pip install \"neural-cherche[eval]\"\n</code></pre>"},{"location":"api/overview/","title":"Overview","text":""},{"location":"api/overview/#losses","title":"losses","text":"<ul> <li>Flops</li> <li>FlopsScheduler</li> <li>Ranking</li> </ul>"},{"location":"api/overview/#models","title":"models","text":"<ul> <li>ColBERT</li> <li>SparseEmbed</li> <li>Splade</li> </ul>"},{"location":"api/overview/#rank","title":"rank","text":"<ul> <li>ColBERT</li> </ul>"},{"location":"api/overview/#retrieve","title":"retrieve","text":"<ul> <li>SparseEmbed</li> <li>Splade</li> <li>TfIdf</li> </ul>"},{"location":"api/overview/#train","title":"train","text":"<ul> <li>train_colbert</li> <li>train_sparse_embed</li> <li>train_splade</li> </ul>"},{"location":"api/overview/#utils","title":"utils","text":"<ul> <li>batchify</li> <li>colbert_scores</li> <li>dense_scores</li> <li>evaluate</li> <li>in_batch_sparse_scores</li> <li>iter</li> <li>load_beir</li> <li>pairs_dense_scores</li> <li>sparse_scores</li> </ul>"},{"location":"api/losses/Flops/","title":"Flops","text":"<p>Flops loss, act as regularization loss over sparse activations.</p>"},{"location":"api/losses/Flops/#examples","title":"Examples","text":"<pre><code>&gt;&gt;&gt; from neural_cherche import models, utils, losses\n&gt;&gt;&gt; import torch\n&gt;&gt;&gt; _ = torch.manual_seed(42)\n&gt;&gt;&gt; model = models.Splade(\n...     model_name_or_path=\"raphaelsty/neural-cherche-sparse-embed\",\n...     device=\"mps\",\n... )\n&gt;&gt;&gt; anchor_activations = model(\n...     [\"Sports\", \"Music\"],\n...     query_mode=True,\n... )\n&gt;&gt;&gt; positive_activations = model(\n...    [\"Sports\", \"Music\"],\n...     query_mode=False,\n... )\n&gt;&gt;&gt; negative_activations = model(\n...    [\"Cinema\", \"Movie\"],\n...     query_mode=False,\n... )\n&gt;&gt;&gt; losses.Flops()(\n...     anchor_activations=anchor_activations[\"sparse_activations\"],\n...     positive_activations=positive_activations[\"sparse_activations\"],\n...     negative_activations=negative_activations[\"sparse_activations\"],\n... )\ntensor(1., device='mps:0', grad_fn=&lt;ClampBackward1&gt;)\n</code></pre>"},{"location":"api/losses/Flops/#methods","title":"Methods","text":"call <p>Loss which tend to reduce sparse activation.</p> <p>Parameters</p> <ul> <li>anchor_activations     (torch.Tensor)    </li> <li>positive_activations     (torch.Tensor)    </li> <li>negative_activations     (torch.Tensor)    </li> <li>threshold     (float)     \u2013 defaults to <code>30.0</code> </li> </ul> add_module <p>Adds a child module to the current module.</p> <p>The module can be accessed as an attribute using the given name.  Args:     name (str): name of the child module. The child module can be         accessed from this module using the given name     module (Module): child module to be added to the module.</p> <p>Parameters</p> <ul> <li>name     (str)    </li> <li>module     (Optional[ForwardRef('Module')])    </li> </ul> apply <p>Applies <code>fn</code> recursively to every submodule (as returned by <code>.children()</code>) as well as self. Typical use includes initializing the parameters of a model (see also :ref:<code>nn-init-doc</code>).</p> <p>Args:     fn (:class:<code>Module</code> -&gt; None): function to be applied to each submodule  Returns:     Module: self  Example::      &gt;&gt;&gt; @torch.no_grad()     &gt;&gt;&gt; def init_weights(m):     &gt;&gt;&gt;     print(m)     &gt;&gt;&gt;     if type(m) == nn.Linear:     &gt;&gt;&gt;         m.weight.fill_(1.0)     &gt;&gt;&gt;         print(m.weight)     &gt;&gt;&gt; net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2))     &gt;&gt;&gt; net.apply(init_weights)     Linear(in_features=2, out_features=2, bias=True)     Parameter containing:     tensor([[1., 1.],             [1., 1.]], requires_grad=True)     Linear(in_features=2, out_features=2, bias=True)     Parameter containing:     tensor([[1., 1.],             [1., 1.]], requires_grad=True)     Sequential(       (0): Linear(in_features=2, out_features=2, bias=True)       (1): Linear(in_features=2, out_features=2, bias=True)     )</p> <p>Parameters</p> <ul> <li>fn     (Callable[[ForwardRef('Module')], NoneType])    </li> </ul> bfloat16 <p>Casts all floating point parameters and buffers to <code>bfloat16</code> datatype.</p> <p>.. note::     This method modifies the module in-place.  Returns:     Module: self</p> buffers <p>Returns an iterator over module buffers.</p> <p>Args:     recurse (bool): if True, then yields buffers of this module         and all submodules. Otherwise, yields only buffers that         are direct members of this module.  Yields:     torch.Tensor: module buffer  Example::      &gt;&gt;&gt; # xdoctest: +SKIP(\"undefined vars\")     &gt;&gt;&gt; for buf in model.buffers():     &gt;&gt;&gt;     print(type(buf), buf.size())      (20L,)      (20L, 1L, 5L, 5L) <p>Parameters</p> <ul> <li>recurse     (bool)     \u2013 defaults to <code>True</code> </li> </ul> children <p>Returns an iterator over immediate children modules.</p> <p>Yields:     Module: a child module</p> cpu <p>Moves all model parameters and buffers to the CPU.</p> <p>.. note::     This method modifies the module in-place.  Returns:     Module: self</p> cuda <p>Moves all model parameters and buffers to the GPU.</p> <p>This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on GPU while being optimized.  .. note::     This method modifies the module in-place.  Args:     device (int, optional): if specified, all parameters will be         copied to that device  Returns:     Module: self</p> <p>Parameters</p> <ul> <li>device     (Union[int, torch.device, NoneType])     \u2013 defaults to <code>None</code> </li> </ul> double <p>Casts all floating point parameters and buffers to <code>double</code> datatype.</p> <p>.. note::     This method modifies the module in-place.  Returns:     Module: self</p> eval <p>Sets the module in evaluation mode.</p> <p>This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class:<code>Dropout</code>, :class:<code>BatchNorm</code>, etc.  This is equivalent with :meth:<code>self.train(False) &lt;torch.nn.Module.train&gt;</code>.  See :ref:<code>locally-disable-grad-doc</code> for a comparison between <code>.eval()</code> and several similar mechanisms that may be confused with it.  Returns:     Module: self</p> extra_repr <p>Set the extra representation of the module</p> <p>To print customized extra information, you should re-implement this method in your own modules. Both single-line and multi-line strings are acceptable.</p> float <p>Casts all floating point parameters and buffers to <code>float</code> datatype.</p> <p>.. note::     This method modifies the module in-place.  Returns:     Module: self</p> forward <p>Defines the computation performed at every call.</p> <p>Should be overridden by all subclasses.  .. note::     Although the recipe for forward pass needs to be defined within     this function, one should call the :class:<code>Module</code> instance afterwards     instead of this since the former takes care of running the     registered hooks while the latter silently ignores them.</p> <p>Parameters</p> <ul> <li>input     (Any)    </li> </ul> get_buffer <p>Returns the buffer given by <code>target</code> if it exists, otherwise throws an error.</p> <p>See the docstring for <code>get_submodule</code> for a more detailed explanation of this method's functionality as well as how to correctly specify <code>target</code>.  Args:     target: The fully-qualified string name of the buffer         to look for. (See <code>get_submodule</code> for how to specify a         fully-qualified string.)  Returns:     torch.Tensor: The buffer referenced by <code>target</code>  Raises:     AttributeError: If the target string references an invalid         path or resolves to something that is not a         buffer</p> <p>Parameters</p> <ul> <li>target     (str)    </li> </ul> get_extra_state <p>Returns any extra state to include in the module's state_dict. Implement this and a corresponding :func:<code>set_extra_state</code> for your module if you need to store extra state. This function is called when building the module's <code>state_dict()</code>.</p> <p>Note that extra state should be picklable to ensure working serialization of the state_dict. We only provide provide backwards compatibility guarantees for serializing Tensors; other objects may break backwards compatibility if their serialized pickled form changes.  Returns:     object: Any extra state to store in the module's state_dict</p> get_parameter <p>Returns the parameter given by <code>target</code> if it exists, otherwise throws an error.</p> <p>See the docstring for <code>get_submodule</code> for a more detailed explanation of this method's functionality as well as how to correctly specify <code>target</code>.  Args:     target: The fully-qualified string name of the Parameter         to look for. (See <code>get_submodule</code> for how to specify a         fully-qualified string.)  Returns:     torch.nn.Parameter: The Parameter referenced by <code>target</code>  Raises:     AttributeError: If the target string references an invalid         path or resolves to something that is not an         <code>nn.Parameter</code></p> <p>Parameters</p> <ul> <li>target     (str)    </li> </ul> get_submodule <p>Returns the submodule given by <code>target</code> if it exists, otherwise throws an error.</p> <p>For example, let's say you have an <code>nn.Module</code> <code>A</code> that looks like this:  .. code-block:: text      A(         (net_b): Module(             (net_c): Module(                 (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2))             )             (linear): Linear(in_features=100, out_features=200, bias=True)         )     )  (The diagram shows an <code>nn.Module</code> <code>A</code>. <code>A</code> has a nested submodule <code>net_b</code>, which itself has two submodules <code>net_c</code> and <code>linear</code>. <code>net_c</code> then has a submodule <code>conv</code>.)  To check whether or not we have the <code>linear</code> submodule, we would call <code>get_submodule(\"net_b.linear\")</code>. To check whether we have the <code>conv</code> submodule, we would call <code>get_submodule(\"net_b.net_c.conv\")</code>.  The runtime of <code>get_submodule</code> is bounded by the degree of module nesting in <code>target</code>. A query against <code>named_modules</code> achieves the same result, but it is O(N) in the number of transitive modules. So, for a simple check to see if some submodule exists, <code>get_submodule</code> should always be used.  Args:     target: The fully-qualified string name of the submodule         to look for. (See above example for how to specify a         fully-qualified string.)  Returns:     torch.nn.Module: The submodule referenced by <code>target</code>  Raises:     AttributeError: If the target string references an invalid         path or resolves to something that is not an         <code>nn.Module</code></p> <p>Parameters</p> <ul> <li>target     (str)    </li> </ul> half <p>Casts all floating point parameters and buffers to <code>half</code> datatype.</p> <p>.. note::     This method modifies the module in-place.  Returns:     Module: self</p> ipu <p>Moves all model parameters and buffers to the IPU.</p> <p>This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on IPU while being optimized.  .. note::     This method modifies the module in-place.  Arguments:     device (int, optional): if specified, all parameters will be         copied to that device  Returns:     Module: self</p> <p>Parameters</p> <ul> <li>device     (Union[int, torch.device, NoneType])     \u2013 defaults to <code>None</code> </li> </ul> load_state_dict <p>Copies parameters and buffers from :attr:<code>state_dict</code> into this module and its descendants. If :attr:<code>strict</code> is <code>True</code>, then the keys of :attr:<code>state_dict</code> must exactly match the keys returned by this module's :meth:<code>~torch.nn.Module.state_dict</code> function.</p> <p>Args:     state_dict (dict): a dict containing parameters and         persistent buffers.     strict (bool, optional): whether to strictly enforce that the keys         in :attr:<code>state_dict</code> match the keys returned by this module's         :meth:<code>~torch.nn.Module.state_dict</code> function. Default: <code>True</code>  Returns:     <code>NamedTuple</code> with <code>missing_keys</code> and <code>unexpected_keys</code> fields:         * missing_keys is a list of str containing the missing keys         * unexpected_keys is a list of str containing the unexpected keys  Note:     If a parameter or buffer is registered as <code>None</code> and its corresponding key     exists in :attr:<code>state_dict</code>, :meth:<code>load_state_dict</code> will raise a     <code>RuntimeError</code>.</p> <p>Parameters</p> <ul> <li>state_dict     (Mapping[str, Any])    </li> <li>strict     (bool)     \u2013 defaults to <code>True</code> </li> </ul> modules <p>Returns an iterator over all modules in the network.</p> <p>Yields:     Module: a module in the network  Note:     Duplicate modules are returned only once. In the following     example, <code>l</code> will be returned only once.  Example::      &gt;&gt;&gt; l = nn.Linear(2, 2)     &gt;&gt;&gt; net = nn.Sequential(l, l)     &gt;&gt;&gt; for idx, m in enumerate(net.modules()):     ...     print(idx, '-&gt;', m)      0 -&gt; Sequential(       (0): Linear(in_features=2, out_features=2, bias=True)       (1): Linear(in_features=2, out_features=2, bias=True)     )     1 -&gt; Linear(in_features=2, out_features=2, bias=True)</p> named_buffers <p>Returns an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself.</p> <p>Args:     prefix (str): prefix to prepend to all buffer names.     recurse (bool, optional): if True, then yields buffers of this module         and all submodules. Otherwise, yields only buffers that         are direct members of this module. Defaults to True.     remove_duplicate (bool, optional): whether to remove the duplicated buffers in the result. Defaults to True.  Yields:     (str, torch.Tensor): Tuple containing the name and buffer  Example::      &gt;&gt;&gt; # xdoctest: +SKIP(\"undefined vars\")     &gt;&gt;&gt; for name, buf in self.named_buffers():     &gt;&gt;&gt;     if name in ['running_var']:     &gt;&gt;&gt;         print(buf.size())</p> <p>Parameters</p> <ul> <li>prefix     (str)     \u2013 defaults to ``    </li> <li>recurse     (bool)     \u2013 defaults to <code>True</code> </li> <li>remove_duplicate     (bool)     \u2013 defaults to <code>True</code> </li> </ul> named_children <p>Returns an iterator over immediate children modules, yielding both the name of the module as well as the module itself.</p> <p>Yields:     (str, Module): Tuple containing a name and child module  Example::      &gt;&gt;&gt; # xdoctest: +SKIP(\"undefined vars\")     &gt;&gt;&gt; for name, module in model.named_children():     &gt;&gt;&gt;     if name in ['conv4', 'conv5']:     &gt;&gt;&gt;         print(module)</p> named_modules <p>Returns an iterator over all modules in the network, yielding both the name of the module as well as the module itself.</p> <p>Args:     memo: a memo to store the set of modules already added to the result     prefix: a prefix that will be added to the name of the module     remove_duplicate: whether to remove the duplicated module instances in the result         or not  Yields:     (str, Module): Tuple of name and module  Note:     Duplicate modules are returned only once. In the following     example, <code>l</code> will be returned only once.  Example::      &gt;&gt;&gt; l = nn.Linear(2, 2)     &gt;&gt;&gt; net = nn.Sequential(l, l)     &gt;&gt;&gt; for idx, m in enumerate(net.named_modules()):     ...     print(idx, '-&gt;', m)      0 -&gt; ('', Sequential(       (0): Linear(in_features=2, out_features=2, bias=True)       (1): Linear(in_features=2, out_features=2, bias=True)     ))     1 -&gt; ('0', Linear(in_features=2, out_features=2, bias=True))</p> <p>Parameters</p> <ul> <li>memo     (Optional[Set[ForwardRef('Module')]])     \u2013 defaults to <code>None</code> </li> <li>prefix     (str)     \u2013 defaults to ``    </li> <li>remove_duplicate     (bool)     \u2013 defaults to <code>True</code> </li> </ul> named_parameters <p>Returns an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself.</p> <p>Args:     prefix (str): prefix to prepend to all parameter names.     recurse (bool): if True, then yields parameters of this module         and all submodules. Otherwise, yields only parameters that         are direct members of this module.     remove_duplicate (bool, optional): whether to remove the duplicated         parameters in the result. Defaults to True.  Yields:     (str, Parameter): Tuple containing the name and parameter  Example::      &gt;&gt;&gt; # xdoctest: +SKIP(\"undefined vars\")     &gt;&gt;&gt; for name, param in self.named_parameters():     &gt;&gt;&gt;     if name in ['bias']:     &gt;&gt;&gt;         print(param.size())</p> <p>Parameters</p> <ul> <li>prefix     (str)     \u2013 defaults to ``    </li> <li>recurse     (bool)     \u2013 defaults to <code>True</code> </li> <li>remove_duplicate     (bool)     \u2013 defaults to <code>True</code> </li> </ul> parameters <p>Returns an iterator over module parameters.</p> <p>This is typically passed to an optimizer.  Args:     recurse (bool): if True, then yields parameters of this module         and all submodules. Otherwise, yields only parameters that         are direct members of this module.  Yields:     Parameter: module parameter  Example::      &gt;&gt;&gt; # xdoctest: +SKIP(\"undefined vars\")     &gt;&gt;&gt; for param in model.parameters():     &gt;&gt;&gt;     print(type(param), param.size())      (20L,)      (20L, 1L, 5L, 5L) <p>Parameters</p> <ul> <li>recurse     (bool)     \u2013 defaults to <code>True</code> </li> </ul> register_backward_hook <p>Registers a backward hook on the module.</p> <p>This function is deprecated in favor of :meth:<code>~torch.nn.Module.register_full_backward_hook</code> and the behavior of this function will change in future versions.  Returns:     :class:<code>torch.utils.hooks.RemovableHandle</code>:         a handle that can be used to remove the added hook by calling         <code>handle.remove()</code></p> <p>Parameters</p> <ul> <li>hook     (Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, Tuple[torch.Tensor, ...], torch.Tensor]])    </li> </ul> register_buffer <p>Adds a buffer to the module.</p> <p>This is typically used to register a buffer that should not to be considered a model parameter. For example, BatchNorm's <code>running_mean</code> is not a parameter, but is part of the module's state. Buffers, by default, are persistent and will be saved alongside parameters. This behavior can be changed by setting :attr:<code>persistent</code> to <code>False</code>. The only difference between a persistent buffer and a non-persistent buffer is that the latter will not be a part of this module's :attr:<code>state_dict</code>.  Buffers can be accessed as attributes using given names.  Args:     name (str): name of the buffer. The buffer can be accessed         from this module using the given name     tensor (Tensor or None): buffer to be registered. If <code>None</code>, then operations         that run on buffers, such as :attr:<code>cuda</code>, are ignored. If <code>None</code>,         the buffer is not included in the module's :attr:<code>state_dict</code>.     persistent (bool): whether the buffer is part of this module's         :attr:<code>state_dict</code>.  Example::      &gt;&gt;&gt; # xdoctest: +SKIP(\"undefined vars\")     &gt;&gt;&gt; self.register_buffer('running_mean', torch.zeros(num_features))</p> <p>Parameters</p> <ul> <li>name     (str)    </li> <li>tensor     (Optional[torch.Tensor])    </li> <li>persistent     (bool)     \u2013 defaults to <code>True</code> </li> </ul> register_forward_hook <p>Registers a forward hook on the module.</p> <p>The hook will be called every time after :func:<code>forward</code> has computed an output.  If <code>with_kwargs</code> is <code>False</code> or not specified, the input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the <code>forward</code>. The hook can modify the output. It can modify the input inplace but it will not have effect on forward since this is called after :func:<code>forward</code> is called. The hook should have the following signature::      hook(module, args, output) -&gt; None or modified output  If <code>with_kwargs</code> is <code>True</code>, the forward hook will be passed the <code>kwargs</code> given to the forward function and be expected to return the output possibly modified. The hook should have the following signature::      hook(module, args, kwargs, output) -&gt; None or modified output  Args:     hook (Callable): The user defined hook to be registered.     prepend (bool): If <code>True</code>, the provided <code>hook</code> will be fired         before all existing <code>forward</code> hooks on this         :class:<code>torch.nn.modules.Module</code>. Otherwise, the provided         <code>hook</code> will be fired after all existing <code>forward</code> hooks on         this :class:<code>torch.nn.modules.Module</code>. Note that global         <code>forward</code> hooks registered with         :func:<code>register_module_forward_hook</code> will fire before all hooks         registered by this method.         Default: <code>False</code>     with_kwargs (bool): If <code>True</code>, the <code>hook</code> will be passed the         kwargs given to the forward function.         Default: <code>False</code>  Returns:     :class:<code>torch.utils.hooks.RemovableHandle</code>:         a handle that can be used to remove the added hook by calling         <code>handle.remove()</code></p> <p>Parameters</p> <ul> <li>hook     (Union[Callable[[~T, Tuple[Any, ...], Any], Optional[Any]], Callable[[~T, Tuple[Any, ...], Dict[str, Any], Any], Optional[Any]]])    </li> <li>prepend     (bool)     \u2013 defaults to <code>False</code> </li> <li>with_kwargs     (bool)     \u2013 defaults to <code>False</code> </li> </ul> register_forward_pre_hook <p>Registers a forward pre-hook on the module.</p> <p>The hook will be called every time before :func:<code>forward</code> is invoked.  If <code>with_kwargs</code> is false or not specified, the input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the <code>forward</code>. The hook can modify the input. User can either return a tuple or a single modified value in the hook. We will wrap the value into a tuple if a single value is returned (unless that value is already a tuple). The hook should have the following signature::      hook(module, args) -&gt; None or modified input  If <code>with_kwargs</code> is true, the forward pre-hook will be passed the kwargs given to the forward function. And if the hook modifies the input, both the args and kwargs should be returned. The hook should have the following signature::      hook(module, args, kwargs) -&gt; None or a tuple of modified input and kwargs  Args:     hook (Callable): The user defined hook to be registered.     prepend (bool): If true, the provided <code>hook</code> will be fired before         all existing <code>forward_pre</code> hooks on this         :class:<code>torch.nn.modules.Module</code>. Otherwise, the provided         <code>hook</code> will be fired after all existing <code>forward_pre</code> hooks         on this :class:<code>torch.nn.modules.Module</code>. Note that global         <code>forward_pre</code> hooks registered with         :func:<code>register_module_forward_pre_hook</code> will fire before all         hooks registered by this method.         Default: <code>False</code>     with_kwargs (bool): If true, the <code>hook</code> will be passed the kwargs         given to the forward function.         Default: <code>False</code>  Returns:     :class:<code>torch.utils.hooks.RemovableHandle</code>:         a handle that can be used to remove the added hook by calling         <code>handle.remove()</code></p> <p>Parameters</p> <ul> <li>hook     (Union[Callable[[~T, Tuple[Any, ...]], Optional[Any]], Callable[[~T, Tuple[Any, ...], Dict[str, Any]], Optional[Tuple[Any, Dict[str, Any]]]]])    </li> <li>prepend     (bool)     \u2013 defaults to <code>False</code> </li> <li>with_kwargs     (bool)     \u2013 defaults to <code>False</code> </li> </ul> register_full_backward_hook <p>Registers a backward hook on the module.</p> <p>The hook will be called every time the gradients with respect to a module are computed, i.e. the hook will execute if and only if the gradients with respect to module outputs are computed. The hook should have the following signature::      hook(module, grad_input, grad_output) -&gt; tuple(Tensor) or None  The :attr:<code>grad_input</code> and :attr:<code>grad_output</code> are tuples that contain the gradients with respect to the inputs and outputs respectively. The hook should not modify its arguments, but it can optionally return a new gradient with respect to the input that will be used in place of :attr:<code>grad_input</code> in subsequent computations. :attr:<code>grad_input</code> will only correspond to the inputs given as positional arguments and all kwarg arguments are ignored. Entries in :attr:<code>grad_input</code> and :attr:<code>grad_output</code> will be <code>None</code> for all non-Tensor arguments.  For technical reasons, when this hook is applied to a Module, its forward function will receive a view of each Tensor passed to the Module. Similarly the caller will receive a view of each Tensor returned by the Module's forward function.  .. warning ::     Modifying inputs or outputs inplace is not allowed when using backward hooks and     will raise an error.  Args:     hook (Callable): The user-defined hook to be registered.     prepend (bool): If true, the provided <code>hook</code> will be fired before         all existing <code>backward</code> hooks on this         :class:<code>torch.nn.modules.Module</code>. Otherwise, the provided         <code>hook</code> will be fired after all existing <code>backward</code> hooks on         this :class:<code>torch.nn.modules.Module</code>. Note that global         <code>backward</code> hooks registered with         :func:<code>register_module_full_backward_hook</code> will fire before         all hooks registered by this method.  Returns:     :class:<code>torch.utils.hooks.RemovableHandle</code>:         a handle that can be used to remove the added hook by calling         <code>handle.remove()</code></p> <p>Parameters</p> <ul> <li>hook     (Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, Tuple[torch.Tensor, ...], torch.Tensor]])    </li> <li>prepend     (bool)     \u2013 defaults to <code>False</code> </li> </ul> register_full_backward_pre_hook <p>Registers a backward pre-hook on the module.</p> <p>The hook will be called every time the gradients for the module are computed. The hook should have the following signature::      hook(module, grad_output) -&gt; Tensor or None  The :attr:<code>grad_output</code> is a tuple. The hook should not modify its arguments, but it can optionally return a new gradient with respect to the output that will be used in place of :attr:<code>grad_output</code> in subsequent computations. Entries in :attr:<code>grad_output</code> will be <code>None</code> for all non-Tensor arguments.  For technical reasons, when this hook is applied to a Module, its forward function will receive a view of each Tensor passed to the Module. Similarly the caller will receive a view of each Tensor returned by the Module's forward function.  .. warning ::     Modifying inputs inplace is not allowed when using backward hooks and     will raise an error.  Args:     hook (Callable): The user-defined hook to be registered.     prepend (bool): If true, the provided <code>hook</code> will be fired before         all existing <code>backward_pre</code> hooks on this         :class:<code>torch.nn.modules.Module</code>. Otherwise, the provided         <code>hook</code> will be fired after all existing <code>backward_pre</code> hooks         on this :class:<code>torch.nn.modules.Module</code>. Note that global         <code>backward_pre</code> hooks registered with         :func:<code>register_module_full_backward_pre_hook</code> will fire before         all hooks registered by this method.  Returns:     :class:<code>torch.utils.hooks.RemovableHandle</code>:         a handle that can be used to remove the added hook by calling         <code>handle.remove()</code></p> <p>Parameters</p> <ul> <li>hook     (Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, Tuple[torch.Tensor, ...], torch.Tensor]])    </li> <li>prepend     (bool)     \u2013 defaults to <code>False</code> </li> </ul> register_load_state_dict_post_hook <p>Registers a post hook to be run after module's <code>load_state_dict</code> is called.</p> <p>It should have the following signature::     hook(module, incompatible_keys) -&gt; None  The <code>module</code> argument is the current module that this hook is registered on, and the <code>incompatible_keys</code> argument is a <code>NamedTuple</code> consisting of attributes <code>missing_keys</code> and <code>unexpected_keys</code>. <code>missing_keys</code> is a <code>list</code> of <code>str</code> containing the missing keys and <code>unexpected_keys</code> is a <code>list</code> of <code>str</code> containing the unexpected keys.  The given incompatible_keys can be modified inplace if needed.  Note that the checks performed when calling :func:<code>load_state_dict</code> with <code>strict=True</code> are affected by modifications the hook makes to <code>missing_keys</code> or <code>unexpected_keys</code>, as expected. Additions to either set of keys will result in an error being thrown when <code>strict=True</code>, and clearing out both missing and unexpected keys will avoid an error.  Returns:     :class:<code>torch.utils.hooks.RemovableHandle</code>:         a handle that can be used to remove the added hook by calling         <code>handle.remove()</code></p> <p>Parameters</p> <ul> <li>hook </li> </ul> register_module <p>Alias for :func:<code>add_module</code>.</p> <p>Parameters</p> <ul> <li>name     (str)    </li> <li>module     (Optional[ForwardRef('Module')])    </li> </ul> register_parameter <p>Adds a parameter to the module.</p> <p>The parameter can be accessed as an attribute using given name.  Args:     name (str): name of the parameter. The parameter can be accessed         from this module using the given name     param (Parameter or None): parameter to be added to the module. If         <code>None</code>, then operations that run on parameters, such as :attr:<code>cuda</code>,         are ignored. If <code>None</code>, the parameter is not included in the         module's :attr:<code>state_dict</code>.</p> <p>Parameters</p> <ul> <li>name     (str)    </li> <li>param     (Optional[torch.nn.parameter.Parameter])    </li> </ul> register_state_dict_pre_hook <p>These hooks will be called with arguments: <code>self</code>, <code>prefix</code>, and <code>keep_vars</code> before calling <code>state_dict</code> on <code>self</code>. The registered hooks can be used to perform pre-processing before the <code>state_dict</code> call is made.</p> <p>Parameters</p> <ul> <li>hook </li> </ul> requires_grad_ <p>Change if autograd should record operations on parameters in this module.</p> <p>This method sets the parameters' :attr:<code>requires_grad</code> attributes in-place.  This method is helpful for freezing part of the module for finetuning or training parts of a model individually (e.g., GAN training).  See :ref:<code>locally-disable-grad-doc</code> for a comparison between <code>.requires_grad_()</code> and several similar mechanisms that may be confused with it.  Args:     requires_grad (bool): whether autograd should record operations on                           parameters in this module. Default: <code>True</code>.  Returns:     Module: self</p> <p>Parameters</p> <ul> <li>requires_grad     (bool)     \u2013 defaults to <code>True</code> </li> </ul> set_extra_state <p>This function is called from :func:<code>load_state_dict</code> to handle any extra state found within the <code>state_dict</code>. Implement this function and a corresponding :func:<code>get_extra_state</code> for your module if you need to store extra state within its <code>state_dict</code>.</p> <p>Args:     state (dict): Extra state from the <code>state_dict</code></p> <p>Parameters</p> <ul> <li>state     (Any)    </li> </ul> share_memory <p>See :meth:<code>torch.Tensor.share_memory_</code></p> state_dict <p>Returns a dictionary containing references to the whole state of the module.</p> <p>Both parameters and persistent buffers (e.g. running averages) are included. Keys are corresponding parameter and buffer names. Parameters and buffers set to <code>None</code> are not included.  .. note::     The returned object is a shallow copy. It contains references     to the module's parameters and buffers.  .. warning::     Currently <code>state_dict()</code> also accepts positional arguments for     <code>destination</code>, <code>prefix</code> and <code>keep_vars</code> in order. However,     this is being deprecated and keyword arguments will be enforced in     future releases.  .. warning::     Please avoid the use of argument <code>destination</code> as it is not     designed for end-users.  Args:     destination (dict, optional): If provided, the state of module will         be updated into the dict and the same object is returned.         Otherwise, an <code>OrderedDict</code> will be created and returned.         Default: <code>None</code>.     prefix (str, optional): a prefix added to parameter and buffer         names to compose the keys in state_dict. Default: <code>''</code>.     keep_vars (bool, optional): by default the :class:<code>~torch.Tensor</code> s         returned in the state dict are detached from autograd. If it's         set to <code>True</code>, detaching will not be performed.         Default: <code>False</code>.  Returns:     dict:         a dictionary containing a whole state of the module  Example::      &gt;&gt;&gt; # xdoctest: +SKIP(\"undefined vars\")     &gt;&gt;&gt; module.state_dict().keys()     ['bias', 'weight']</p> <p>Parameters</p> <ul> <li>args </li> <li>destination     \u2013 defaults to <code>None</code> </li> <li>prefix     \u2013 defaults to ``    </li> <li>keep_vars     \u2013 defaults to <code>False</code> </li> </ul> to <p>Moves and/or casts the parameters and buffers.</p> <p>This can be called as  .. function:: to(device=None, dtype=None, non_blocking=False)    :noindex:  .. function:: to(dtype, non_blocking=False)    :noindex:  .. function:: to(tensor, non_blocking=False)    :noindex:  .. function:: to(memory_format=torch.channels_last)    :noindex:  Its signature is similar to :meth:<code>torch.Tensor.to</code>, but only accepts floating point or complex :attr:<code>dtype</code>\\ s. In addition, this method will only cast the floating point or complex parameters and buffers to :attr:<code>dtype</code> (if given). The integral parameters and buffers will be moved :attr:<code>device</code>, if that is given, but with dtypes unchanged. When :attr:<code>non_blocking</code> is set, it tries to convert/move asynchronously with respect to the host if possible, e.g., moving CPU Tensors with pinned memory to CUDA devices.  See below for examples.  .. note::     This method modifies the module in-place.  Args:     device (:class:<code>torch.device</code>): the desired device of the parameters         and buffers in this module     dtype (:class:<code>torch.dtype</code>): the desired floating point or complex dtype of         the parameters and buffers in this module     tensor (torch.Tensor): Tensor whose dtype and device are the desired         dtype and device for all parameters and buffers in this module     memory_format (:class:<code>torch.memory_format</code>): the desired memory         format for 4D parameters and buffers in this module (keyword         only argument)  Returns:     Module: self  Examples::      &gt;&gt;&gt; # xdoctest: +IGNORE_WANT(\"non-deterministic\")     &gt;&gt;&gt; linear = nn.Linear(2, 2)     &gt;&gt;&gt; linear.weight     Parameter containing:     tensor([[ 0.1913, -0.3420],             [-0.5113, -0.2325]])     &gt;&gt;&gt; linear.to(torch.double)     Linear(in_features=2, out_features=2, bias=True)     &gt;&gt;&gt; linear.weight     Parameter containing:     tensor([[ 0.1913, -0.3420],             [-0.5113, -0.2325]], dtype=torch.float64)     &gt;&gt;&gt; # xdoctest: +REQUIRES(env:TORCH_DOCTEST_CUDA1)     &gt;&gt;&gt; gpu1 = torch.device(\"cuda:1\")     &gt;&gt;&gt; linear.to(gpu1, dtype=torch.half, non_blocking=True)     Linear(in_features=2, out_features=2, bias=True)     &gt;&gt;&gt; linear.weight     Parameter containing:     tensor([[ 0.1914, -0.3420],             [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1')     &gt;&gt;&gt; cpu = torch.device(\"cpu\")     &gt;&gt;&gt; linear.to(cpu)     Linear(in_features=2, out_features=2, bias=True)     &gt;&gt;&gt; linear.weight     Parameter containing:     tensor([[ 0.1914, -0.3420],             [-0.5112, -0.2324]], dtype=torch.float16)      &gt;&gt;&gt; linear = nn.Linear(2, 2, bias=None).to(torch.cdouble)     &gt;&gt;&gt; linear.weight     Parameter containing:     tensor([[ 0.3741+0.j,  0.2382+0.j],             [ 0.5593+0.j, -0.4443+0.j]], dtype=torch.complex128)     &gt;&gt;&gt; linear(torch.ones(3, 2, dtype=torch.cdouble))     tensor([[0.6122+0.j, 0.1150+0.j],             [0.6122+0.j, 0.1150+0.j],             [0.6122+0.j, 0.1150+0.j]], dtype=torch.complex128)</p> <p>Parameters</p> <ul> <li>args </li> <li>kwargs </li> </ul> to_empty <p>Moves the parameters and buffers to the specified device without copying storage.</p> <p>Args:     device (:class:<code>torch.device</code>): The desired device of the parameters         and buffers in this module.  Returns:     Module: self</p> <p>Parameters</p> <ul> <li>device     (Union[str, torch.device])    </li> </ul> train <p>Sets the module in training mode.</p> <p>This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class:<code>Dropout</code>, :class:<code>BatchNorm</code>, etc.  Args:     mode (bool): whether to set training mode (<code>True</code>) or evaluation                  mode (<code>False</code>). Default: <code>True</code>.  Returns:     Module: self</p> <p>Parameters</p> <ul> <li>mode     (bool)     \u2013 defaults to <code>True</code> </li> </ul> type <p>Casts all parameters and buffers to :attr:<code>dst_type</code>.</p> <p>.. note::     This method modifies the module in-place.  Args:     dst_type (type or string): the desired type  Returns:     Module: self</p> <p>Parameters</p> <ul> <li>dst_type     (Union[torch.dtype, str])    </li> </ul> xpu <p>Moves all model parameters and buffers to the XPU.</p> <p>This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on XPU while being optimized.  .. note::     This method modifies the module in-place.  Arguments:     device (int, optional): if specified, all parameters will be         copied to that device  Returns:     Module: self</p> <p>Parameters</p> <ul> <li>device     (Union[int, torch.device, NoneType])     \u2013 defaults to <code>None</code> </li> </ul> zero_grad <p>Sets gradients of all model parameters to zero. See similar function under :class:<code>torch.optim.Optimizer</code> for more context.</p> <p>Args:     set_to_none (bool): instead of setting to zero, set the grads to None.         See :meth:<code>torch.optim.Optimizer.zero_grad</code> for details.</p> <p>Parameters</p> <ul> <li>set_to_none     (bool)     \u2013 defaults to <code>True</code> </li> </ul>"},{"location":"api/losses/Flops/#references","title":"References","text":"<ol> <li>MINIMIZING FLOPS TO LEARN EFFICIENT SPARSE REPRESENTATIONS</li> <li>SPLADE: Sparse Lexical and Expansion Model for First Stage Ranking</li> </ol>"},{"location":"api/losses/FlopsScheduler/","title":"FlopsScheduler","text":"<p>Flops scheduler.</p>"},{"location":"api/losses/FlopsScheduler/#parameters","title":"Parameters","text":"<ul> <li> <p>weight (float) \u2013 defaults to <code>3e-05</code></p> </li> <li> <p>steps (int) \u2013 defaults to <code>10000</code></p> </li> </ul>"},{"location":"api/losses/FlopsScheduler/#methods","title":"Methods","text":"get step"},{"location":"api/losses/FlopsScheduler/#references","title":"References","text":"<ol> <li>MINIMIZING FLOPS TO LEARN EFFICIENT SPARSE REPRESENTATIONS</li> <li>SPLADE: Sparse Lexical and Expansion Model for First Stage Ranking</li> </ol>"},{"location":"api/losses/Ranking/","title":"Ranking","text":"<p>Ranking loss.</p>"},{"location":"api/losses/Ranking/#examples","title":"Examples","text":"<pre><code>&gt;&gt;&gt; from neural_cherche import models, utils, losses\n&gt;&gt;&gt; from pprint import pprint as print\n&gt;&gt;&gt; model = models.Splade(\n...     model_name_or_path=\"raphaelsty/splade-max\",\n...     device=\"mps\",\n... )\n&gt;&gt;&gt; queries_activations = model(\n...     [\"Sports\", \"Music\"],\n...     query_mode=True,\n... )\n&gt;&gt;&gt; positive_activations = model(\n...    [\"Sports\", \"Music\"],\n...    query_mode=False,\n... )\n&gt;&gt;&gt; negative_activations = model(\n...    [\"Cinema\", \"Movie\"],\n...    query_mode=False,\n... )\n&gt;&gt;&gt; scores = utils.sparse_scores(\n...     anchor_activations=queries_activations[\"sparse_activations\"],\n...     positive_activations=positive_activations[\"sparse_activations\"],\n...     negative_activations=negative_activations[\"sparse_activations\"],\n...     in_batch_negatives=True,\n... )\n&gt;&gt;&gt; losses.Ranking()(**scores)\ntensor(1., device='mps:0', grad_fn=&lt;ClampBackward1&gt;)\n</code></pre>"},{"location":"api/losses/Ranking/#methods","title":"Methods","text":"call <p>Ranking loss.</p> <p>Parameters</p> <ul> <li>positive_scores     (torch.Tensor)    </li> <li>negative_scores     (torch.Tensor)    </li> </ul> add_module <p>Adds a child module to the current module.</p> <p>The module can be accessed as an attribute using the given name.  Args:     name (str): name of the child module. The child module can be         accessed from this module using the given name     module (Module): child module to be added to the module.</p> <p>Parameters</p> <ul> <li>name     (str)    </li> <li>module     (Optional[ForwardRef('Module')])    </li> </ul> apply <p>Applies <code>fn</code> recursively to every submodule (as returned by <code>.children()</code>) as well as self. Typical use includes initializing the parameters of a model (see also :ref:<code>nn-init-doc</code>).</p> <p>Args:     fn (:class:<code>Module</code> -&gt; None): function to be applied to each submodule  Returns:     Module: self  Example::      &gt;&gt;&gt; @torch.no_grad()     &gt;&gt;&gt; def init_weights(m):     &gt;&gt;&gt;     print(m)     &gt;&gt;&gt;     if type(m) == nn.Linear:     &gt;&gt;&gt;         m.weight.fill_(1.0)     &gt;&gt;&gt;         print(m.weight)     &gt;&gt;&gt; net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2))     &gt;&gt;&gt; net.apply(init_weights)     Linear(in_features=2, out_features=2, bias=True)     Parameter containing:     tensor([[1., 1.],             [1., 1.]], requires_grad=True)     Linear(in_features=2, out_features=2, bias=True)     Parameter containing:     tensor([[1., 1.],             [1., 1.]], requires_grad=True)     Sequential(       (0): Linear(in_features=2, out_features=2, bias=True)       (1): Linear(in_features=2, out_features=2, bias=True)     )</p> <p>Parameters</p> <ul> <li>fn     (Callable[[ForwardRef('Module')], NoneType])    </li> </ul> bfloat16 <p>Casts all floating point parameters and buffers to <code>bfloat16</code> datatype.</p> <p>.. note::     This method modifies the module in-place.  Returns:     Module: self</p> buffers <p>Returns an iterator over module buffers.</p> <p>Args:     recurse (bool): if True, then yields buffers of this module         and all submodules. Otherwise, yields only buffers that         are direct members of this module.  Yields:     torch.Tensor: module buffer  Example::      &gt;&gt;&gt; # xdoctest: +SKIP(\"undefined vars\")     &gt;&gt;&gt; for buf in model.buffers():     &gt;&gt;&gt;     print(type(buf), buf.size())      (20L,)      (20L, 1L, 5L, 5L) <p>Parameters</p> <ul> <li>recurse     (bool)     \u2013 defaults to <code>True</code> </li> </ul> children <p>Returns an iterator over immediate children modules.</p> <p>Yields:     Module: a child module</p> cpu <p>Moves all model parameters and buffers to the CPU.</p> <p>.. note::     This method modifies the module in-place.  Returns:     Module: self</p> cuda <p>Moves all model parameters and buffers to the GPU.</p> <p>This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on GPU while being optimized.  .. note::     This method modifies the module in-place.  Args:     device (int, optional): if specified, all parameters will be         copied to that device  Returns:     Module: self</p> <p>Parameters</p> <ul> <li>device     (Union[int, torch.device, NoneType])     \u2013 defaults to <code>None</code> </li> </ul> double <p>Casts all floating point parameters and buffers to <code>double</code> datatype.</p> <p>.. note::     This method modifies the module in-place.  Returns:     Module: self</p> eval <p>Sets the module in evaluation mode.</p> <p>This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class:<code>Dropout</code>, :class:<code>BatchNorm</code>, etc.  This is equivalent with :meth:<code>self.train(False) &lt;torch.nn.Module.train&gt;</code>.  See :ref:<code>locally-disable-grad-doc</code> for a comparison between <code>.eval()</code> and several similar mechanisms that may be confused with it.  Returns:     Module: self</p> extra_repr <p>Set the extra representation of the module</p> <p>To print customized extra information, you should re-implement this method in your own modules. Both single-line and multi-line strings are acceptable.</p> float <p>Casts all floating point parameters and buffers to <code>float</code> datatype.</p> <p>.. note::     This method modifies the module in-place.  Returns:     Module: self</p> forward <p>Defines the computation performed at every call.</p> <p>Should be overridden by all subclasses.  .. note::     Although the recipe for forward pass needs to be defined within     this function, one should call the :class:<code>Module</code> instance afterwards     instead of this since the former takes care of running the     registered hooks while the latter silently ignores them.</p> <p>Parameters</p> <ul> <li>input     (Any)    </li> </ul> get_buffer <p>Returns the buffer given by <code>target</code> if it exists, otherwise throws an error.</p> <p>See the docstring for <code>get_submodule</code> for a more detailed explanation of this method's functionality as well as how to correctly specify <code>target</code>.  Args:     target: The fully-qualified string name of the buffer         to look for. (See <code>get_submodule</code> for how to specify a         fully-qualified string.)  Returns:     torch.Tensor: The buffer referenced by <code>target</code>  Raises:     AttributeError: If the target string references an invalid         path or resolves to something that is not a         buffer</p> <p>Parameters</p> <ul> <li>target     (str)    </li> </ul> get_extra_state <p>Returns any extra state to include in the module's state_dict. Implement this and a corresponding :func:<code>set_extra_state</code> for your module if you need to store extra state. This function is called when building the module's <code>state_dict()</code>.</p> <p>Note that extra state should be picklable to ensure working serialization of the state_dict. We only provide provide backwards compatibility guarantees for serializing Tensors; other objects may break backwards compatibility if their serialized pickled form changes.  Returns:     object: Any extra state to store in the module's state_dict</p> get_parameter <p>Returns the parameter given by <code>target</code> if it exists, otherwise throws an error.</p> <p>See the docstring for <code>get_submodule</code> for a more detailed explanation of this method's functionality as well as how to correctly specify <code>target</code>.  Args:     target: The fully-qualified string name of the Parameter         to look for. (See <code>get_submodule</code> for how to specify a         fully-qualified string.)  Returns:     torch.nn.Parameter: The Parameter referenced by <code>target</code>  Raises:     AttributeError: If the target string references an invalid         path or resolves to something that is not an         <code>nn.Parameter</code></p> <p>Parameters</p> <ul> <li>target     (str)    </li> </ul> get_submodule <p>Returns the submodule given by <code>target</code> if it exists, otherwise throws an error.</p> <p>For example, let's say you have an <code>nn.Module</code> <code>A</code> that looks like this:  .. code-block:: text      A(         (net_b): Module(             (net_c): Module(                 (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2))             )             (linear): Linear(in_features=100, out_features=200, bias=True)         )     )  (The diagram shows an <code>nn.Module</code> <code>A</code>. <code>A</code> has a nested submodule <code>net_b</code>, which itself has two submodules <code>net_c</code> and <code>linear</code>. <code>net_c</code> then has a submodule <code>conv</code>.)  To check whether or not we have the <code>linear</code> submodule, we would call <code>get_submodule(\"net_b.linear\")</code>. To check whether we have the <code>conv</code> submodule, we would call <code>get_submodule(\"net_b.net_c.conv\")</code>.  The runtime of <code>get_submodule</code> is bounded by the degree of module nesting in <code>target</code>. A query against <code>named_modules</code> achieves the same result, but it is O(N) in the number of transitive modules. So, for a simple check to see if some submodule exists, <code>get_submodule</code> should always be used.  Args:     target: The fully-qualified string name of the submodule         to look for. (See above example for how to specify a         fully-qualified string.)  Returns:     torch.nn.Module: The submodule referenced by <code>target</code>  Raises:     AttributeError: If the target string references an invalid         path or resolves to something that is not an         <code>nn.Module</code></p> <p>Parameters</p> <ul> <li>target     (str)    </li> </ul> half <p>Casts all floating point parameters and buffers to <code>half</code> datatype.</p> <p>.. note::     This method modifies the module in-place.  Returns:     Module: self</p> ipu <p>Moves all model parameters and buffers to the IPU.</p> <p>This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on IPU while being optimized.  .. note::     This method modifies the module in-place.  Arguments:     device (int, optional): if specified, all parameters will be         copied to that device  Returns:     Module: self</p> <p>Parameters</p> <ul> <li>device     (Union[int, torch.device, NoneType])     \u2013 defaults to <code>None</code> </li> </ul> load_state_dict <p>Copies parameters and buffers from :attr:<code>state_dict</code> into this module and its descendants. If :attr:<code>strict</code> is <code>True</code>, then the keys of :attr:<code>state_dict</code> must exactly match the keys returned by this module's :meth:<code>~torch.nn.Module.state_dict</code> function.</p> <p>Args:     state_dict (dict): a dict containing parameters and         persistent buffers.     strict (bool, optional): whether to strictly enforce that the keys         in :attr:<code>state_dict</code> match the keys returned by this module's         :meth:<code>~torch.nn.Module.state_dict</code> function. Default: <code>True</code>  Returns:     <code>NamedTuple</code> with <code>missing_keys</code> and <code>unexpected_keys</code> fields:         * missing_keys is a list of str containing the missing keys         * unexpected_keys is a list of str containing the unexpected keys  Note:     If a parameter or buffer is registered as <code>None</code> and its corresponding key     exists in :attr:<code>state_dict</code>, :meth:<code>load_state_dict</code> will raise a     <code>RuntimeError</code>.</p> <p>Parameters</p> <ul> <li>state_dict     (Mapping[str, Any])    </li> <li>strict     (bool)     \u2013 defaults to <code>True</code> </li> </ul> modules <p>Returns an iterator over all modules in the network.</p> <p>Yields:     Module: a module in the network  Note:     Duplicate modules are returned only once. In the following     example, <code>l</code> will be returned only once.  Example::      &gt;&gt;&gt; l = nn.Linear(2, 2)     &gt;&gt;&gt; net = nn.Sequential(l, l)     &gt;&gt;&gt; for idx, m in enumerate(net.modules()):     ...     print(idx, '-&gt;', m)      0 -&gt; Sequential(       (0): Linear(in_features=2, out_features=2, bias=True)       (1): Linear(in_features=2, out_features=2, bias=True)     )     1 -&gt; Linear(in_features=2, out_features=2, bias=True)</p> named_buffers <p>Returns an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself.</p> <p>Args:     prefix (str): prefix to prepend to all buffer names.     recurse (bool, optional): if True, then yields buffers of this module         and all submodules. Otherwise, yields only buffers that         are direct members of this module. Defaults to True.     remove_duplicate (bool, optional): whether to remove the duplicated buffers in the result. Defaults to True.  Yields:     (str, torch.Tensor): Tuple containing the name and buffer  Example::      &gt;&gt;&gt; # xdoctest: +SKIP(\"undefined vars\")     &gt;&gt;&gt; for name, buf in self.named_buffers():     &gt;&gt;&gt;     if name in ['running_var']:     &gt;&gt;&gt;         print(buf.size())</p> <p>Parameters</p> <ul> <li>prefix     (str)     \u2013 defaults to ``    </li> <li>recurse     (bool)     \u2013 defaults to <code>True</code> </li> <li>remove_duplicate     (bool)     \u2013 defaults to <code>True</code> </li> </ul> named_children <p>Returns an iterator over immediate children modules, yielding both the name of the module as well as the module itself.</p> <p>Yields:     (str, Module): Tuple containing a name and child module  Example::      &gt;&gt;&gt; # xdoctest: +SKIP(\"undefined vars\")     &gt;&gt;&gt; for name, module in model.named_children():     &gt;&gt;&gt;     if name in ['conv4', 'conv5']:     &gt;&gt;&gt;         print(module)</p> named_modules <p>Returns an iterator over all modules in the network, yielding both the name of the module as well as the module itself.</p> <p>Args:     memo: a memo to store the set of modules already added to the result     prefix: a prefix that will be added to the name of the module     remove_duplicate: whether to remove the duplicated module instances in the result         or not  Yields:     (str, Module): Tuple of name and module  Note:     Duplicate modules are returned only once. In the following     example, <code>l</code> will be returned only once.  Example::      &gt;&gt;&gt; l = nn.Linear(2, 2)     &gt;&gt;&gt; net = nn.Sequential(l, l)     &gt;&gt;&gt; for idx, m in enumerate(net.named_modules()):     ...     print(idx, '-&gt;', m)      0 -&gt; ('', Sequential(       (0): Linear(in_features=2, out_features=2, bias=True)       (1): Linear(in_features=2, out_features=2, bias=True)     ))     1 -&gt; ('0', Linear(in_features=2, out_features=2, bias=True))</p> <p>Parameters</p> <ul> <li>memo     (Optional[Set[ForwardRef('Module')]])     \u2013 defaults to <code>None</code> </li> <li>prefix     (str)     \u2013 defaults to ``    </li> <li>remove_duplicate     (bool)     \u2013 defaults to <code>True</code> </li> </ul> named_parameters <p>Returns an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself.</p> <p>Args:     prefix (str): prefix to prepend to all parameter names.     recurse (bool): if True, then yields parameters of this module         and all submodules. Otherwise, yields only parameters that         are direct members of this module.     remove_duplicate (bool, optional): whether to remove the duplicated         parameters in the result. Defaults to True.  Yields:     (str, Parameter): Tuple containing the name and parameter  Example::      &gt;&gt;&gt; # xdoctest: +SKIP(\"undefined vars\")     &gt;&gt;&gt; for name, param in self.named_parameters():     &gt;&gt;&gt;     if name in ['bias']:     &gt;&gt;&gt;         print(param.size())</p> <p>Parameters</p> <ul> <li>prefix     (str)     \u2013 defaults to ``    </li> <li>recurse     (bool)     \u2013 defaults to <code>True</code> </li> <li>remove_duplicate     (bool)     \u2013 defaults to <code>True</code> </li> </ul> parameters <p>Returns an iterator over module parameters.</p> <p>This is typically passed to an optimizer.  Args:     recurse (bool): if True, then yields parameters of this module         and all submodules. Otherwise, yields only parameters that         are direct members of this module.  Yields:     Parameter: module parameter  Example::      &gt;&gt;&gt; # xdoctest: +SKIP(\"undefined vars\")     &gt;&gt;&gt; for param in model.parameters():     &gt;&gt;&gt;     print(type(param), param.size())      (20L,)      (20L, 1L, 5L, 5L) <p>Parameters</p> <ul> <li>recurse     (bool)     \u2013 defaults to <code>True</code> </li> </ul> register_backward_hook <p>Registers a backward hook on the module.</p> <p>This function is deprecated in favor of :meth:<code>~torch.nn.Module.register_full_backward_hook</code> and the behavior of this function will change in future versions.  Returns:     :class:<code>torch.utils.hooks.RemovableHandle</code>:         a handle that can be used to remove the added hook by calling         <code>handle.remove()</code></p> <p>Parameters</p> <ul> <li>hook     (Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, Tuple[torch.Tensor, ...], torch.Tensor]])    </li> </ul> register_buffer <p>Adds a buffer to the module.</p> <p>This is typically used to register a buffer that should not to be considered a model parameter. For example, BatchNorm's <code>running_mean</code> is not a parameter, but is part of the module's state. Buffers, by default, are persistent and will be saved alongside parameters. This behavior can be changed by setting :attr:<code>persistent</code> to <code>False</code>. The only difference between a persistent buffer and a non-persistent buffer is that the latter will not be a part of this module's :attr:<code>state_dict</code>.  Buffers can be accessed as attributes using given names.  Args:     name (str): name of the buffer. The buffer can be accessed         from this module using the given name     tensor (Tensor or None): buffer to be registered. If <code>None</code>, then operations         that run on buffers, such as :attr:<code>cuda</code>, are ignored. If <code>None</code>,         the buffer is not included in the module's :attr:<code>state_dict</code>.     persistent (bool): whether the buffer is part of this module's         :attr:<code>state_dict</code>.  Example::      &gt;&gt;&gt; # xdoctest: +SKIP(\"undefined vars\")     &gt;&gt;&gt; self.register_buffer('running_mean', torch.zeros(num_features))</p> <p>Parameters</p> <ul> <li>name     (str)    </li> <li>tensor     (Optional[torch.Tensor])    </li> <li>persistent     (bool)     \u2013 defaults to <code>True</code> </li> </ul> register_forward_hook <p>Registers a forward hook on the module.</p> <p>The hook will be called every time after :func:<code>forward</code> has computed an output.  If <code>with_kwargs</code> is <code>False</code> or not specified, the input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the <code>forward</code>. The hook can modify the output. It can modify the input inplace but it will not have effect on forward since this is called after :func:<code>forward</code> is called. The hook should have the following signature::      hook(module, args, output) -&gt; None or modified output  If <code>with_kwargs</code> is <code>True</code>, the forward hook will be passed the <code>kwargs</code> given to the forward function and be expected to return the output possibly modified. The hook should have the following signature::      hook(module, args, kwargs, output) -&gt; None or modified output  Args:     hook (Callable): The user defined hook to be registered.     prepend (bool): If <code>True</code>, the provided <code>hook</code> will be fired         before all existing <code>forward</code> hooks on this         :class:<code>torch.nn.modules.Module</code>. Otherwise, the provided         <code>hook</code> will be fired after all existing <code>forward</code> hooks on         this :class:<code>torch.nn.modules.Module</code>. Note that global         <code>forward</code> hooks registered with         :func:<code>register_module_forward_hook</code> will fire before all hooks         registered by this method.         Default: <code>False</code>     with_kwargs (bool): If <code>True</code>, the <code>hook</code> will be passed the         kwargs given to the forward function.         Default: <code>False</code>  Returns:     :class:<code>torch.utils.hooks.RemovableHandle</code>:         a handle that can be used to remove the added hook by calling         <code>handle.remove()</code></p> <p>Parameters</p> <ul> <li>hook     (Union[Callable[[~T, Tuple[Any, ...], Any], Optional[Any]], Callable[[~T, Tuple[Any, ...], Dict[str, Any], Any], Optional[Any]]])    </li> <li>prepend     (bool)     \u2013 defaults to <code>False</code> </li> <li>with_kwargs     (bool)     \u2013 defaults to <code>False</code> </li> </ul> register_forward_pre_hook <p>Registers a forward pre-hook on the module.</p> <p>The hook will be called every time before :func:<code>forward</code> is invoked.  If <code>with_kwargs</code> is false or not specified, the input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the <code>forward</code>. The hook can modify the input. User can either return a tuple or a single modified value in the hook. We will wrap the value into a tuple if a single value is returned (unless that value is already a tuple). The hook should have the following signature::      hook(module, args) -&gt; None or modified input  If <code>with_kwargs</code> is true, the forward pre-hook will be passed the kwargs given to the forward function. And if the hook modifies the input, both the args and kwargs should be returned. The hook should have the following signature::      hook(module, args, kwargs) -&gt; None or a tuple of modified input and kwargs  Args:     hook (Callable): The user defined hook to be registered.     prepend (bool): If true, the provided <code>hook</code> will be fired before         all existing <code>forward_pre</code> hooks on this         :class:<code>torch.nn.modules.Module</code>. Otherwise, the provided         <code>hook</code> will be fired after all existing <code>forward_pre</code> hooks         on this :class:<code>torch.nn.modules.Module</code>. Note that global         <code>forward_pre</code> hooks registered with         :func:<code>register_module_forward_pre_hook</code> will fire before all         hooks registered by this method.         Default: <code>False</code>     with_kwargs (bool): If true, the <code>hook</code> will be passed the kwargs         given to the forward function.         Default: <code>False</code>  Returns:     :class:<code>torch.utils.hooks.RemovableHandle</code>:         a handle that can be used to remove the added hook by calling         <code>handle.remove()</code></p> <p>Parameters</p> <ul> <li>hook     (Union[Callable[[~T, Tuple[Any, ...]], Optional[Any]], Callable[[~T, Tuple[Any, ...], Dict[str, Any]], Optional[Tuple[Any, Dict[str, Any]]]]])    </li> <li>prepend     (bool)     \u2013 defaults to <code>False</code> </li> <li>with_kwargs     (bool)     \u2013 defaults to <code>False</code> </li> </ul> register_full_backward_hook <p>Registers a backward hook on the module.</p> <p>The hook will be called every time the gradients with respect to a module are computed, i.e. the hook will execute if and only if the gradients with respect to module outputs are computed. The hook should have the following signature::      hook(module, grad_input, grad_output) -&gt; tuple(Tensor) or None  The :attr:<code>grad_input</code> and :attr:<code>grad_output</code> are tuples that contain the gradients with respect to the inputs and outputs respectively. The hook should not modify its arguments, but it can optionally return a new gradient with respect to the input that will be used in place of :attr:<code>grad_input</code> in subsequent computations. :attr:<code>grad_input</code> will only correspond to the inputs given as positional arguments and all kwarg arguments are ignored. Entries in :attr:<code>grad_input</code> and :attr:<code>grad_output</code> will be <code>None</code> for all non-Tensor arguments.  For technical reasons, when this hook is applied to a Module, its forward function will receive a view of each Tensor passed to the Module. Similarly the caller will receive a view of each Tensor returned by the Module's forward function.  .. warning ::     Modifying inputs or outputs inplace is not allowed when using backward hooks and     will raise an error.  Args:     hook (Callable): The user-defined hook to be registered.     prepend (bool): If true, the provided <code>hook</code> will be fired before         all existing <code>backward</code> hooks on this         :class:<code>torch.nn.modules.Module</code>. Otherwise, the provided         <code>hook</code> will be fired after all existing <code>backward</code> hooks on         this :class:<code>torch.nn.modules.Module</code>. Note that global         <code>backward</code> hooks registered with         :func:<code>register_module_full_backward_hook</code> will fire before         all hooks registered by this method.  Returns:     :class:<code>torch.utils.hooks.RemovableHandle</code>:         a handle that can be used to remove the added hook by calling         <code>handle.remove()</code></p> <p>Parameters</p> <ul> <li>hook     (Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, Tuple[torch.Tensor, ...], torch.Tensor]])    </li> <li>prepend     (bool)     \u2013 defaults to <code>False</code> </li> </ul> register_full_backward_pre_hook <p>Registers a backward pre-hook on the module.</p> <p>The hook will be called every time the gradients for the module are computed. The hook should have the following signature::      hook(module, grad_output) -&gt; Tensor or None  The :attr:<code>grad_output</code> is a tuple. The hook should not modify its arguments, but it can optionally return a new gradient with respect to the output that will be used in place of :attr:<code>grad_output</code> in subsequent computations. Entries in :attr:<code>grad_output</code> will be <code>None</code> for all non-Tensor arguments.  For technical reasons, when this hook is applied to a Module, its forward function will receive a view of each Tensor passed to the Module. Similarly the caller will receive a view of each Tensor returned by the Module's forward function.  .. warning ::     Modifying inputs inplace is not allowed when using backward hooks and     will raise an error.  Args:     hook (Callable): The user-defined hook to be registered.     prepend (bool): If true, the provided <code>hook</code> will be fired before         all existing <code>backward_pre</code> hooks on this         :class:<code>torch.nn.modules.Module</code>. Otherwise, the provided         <code>hook</code> will be fired after all existing <code>backward_pre</code> hooks         on this :class:<code>torch.nn.modules.Module</code>. Note that global         <code>backward_pre</code> hooks registered with         :func:<code>register_module_full_backward_pre_hook</code> will fire before         all hooks registered by this method.  Returns:     :class:<code>torch.utils.hooks.RemovableHandle</code>:         a handle that can be used to remove the added hook by calling         <code>handle.remove()</code></p> <p>Parameters</p> <ul> <li>hook     (Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, Tuple[torch.Tensor, ...], torch.Tensor]])    </li> <li>prepend     (bool)     \u2013 defaults to <code>False</code> </li> </ul> register_load_state_dict_post_hook <p>Registers a post hook to be run after module's <code>load_state_dict</code> is called.</p> <p>It should have the following signature::     hook(module, incompatible_keys) -&gt; None  The <code>module</code> argument is the current module that this hook is registered on, and the <code>incompatible_keys</code> argument is a <code>NamedTuple</code> consisting of attributes <code>missing_keys</code> and <code>unexpected_keys</code>. <code>missing_keys</code> is a <code>list</code> of <code>str</code> containing the missing keys and <code>unexpected_keys</code> is a <code>list</code> of <code>str</code> containing the unexpected keys.  The given incompatible_keys can be modified inplace if needed.  Note that the checks performed when calling :func:<code>load_state_dict</code> with <code>strict=True</code> are affected by modifications the hook makes to <code>missing_keys</code> or <code>unexpected_keys</code>, as expected. Additions to either set of keys will result in an error being thrown when <code>strict=True</code>, and clearing out both missing and unexpected keys will avoid an error.  Returns:     :class:<code>torch.utils.hooks.RemovableHandle</code>:         a handle that can be used to remove the added hook by calling         <code>handle.remove()</code></p> <p>Parameters</p> <ul> <li>hook </li> </ul> register_module <p>Alias for :func:<code>add_module</code>.</p> <p>Parameters</p> <ul> <li>name     (str)    </li> <li>module     (Optional[ForwardRef('Module')])    </li> </ul> register_parameter <p>Adds a parameter to the module.</p> <p>The parameter can be accessed as an attribute using given name.  Args:     name (str): name of the parameter. The parameter can be accessed         from this module using the given name     param (Parameter or None): parameter to be added to the module. If         <code>None</code>, then operations that run on parameters, such as :attr:<code>cuda</code>,         are ignored. If <code>None</code>, the parameter is not included in the         module's :attr:<code>state_dict</code>.</p> <p>Parameters</p> <ul> <li>name     (str)    </li> <li>param     (Optional[torch.nn.parameter.Parameter])    </li> </ul> register_state_dict_pre_hook <p>These hooks will be called with arguments: <code>self</code>, <code>prefix</code>, and <code>keep_vars</code> before calling <code>state_dict</code> on <code>self</code>. The registered hooks can be used to perform pre-processing before the <code>state_dict</code> call is made.</p> <p>Parameters</p> <ul> <li>hook </li> </ul> requires_grad_ <p>Change if autograd should record operations on parameters in this module.</p> <p>This method sets the parameters' :attr:<code>requires_grad</code> attributes in-place.  This method is helpful for freezing part of the module for finetuning or training parts of a model individually (e.g., GAN training).  See :ref:<code>locally-disable-grad-doc</code> for a comparison between <code>.requires_grad_()</code> and several similar mechanisms that may be confused with it.  Args:     requires_grad (bool): whether autograd should record operations on                           parameters in this module. Default: <code>True</code>.  Returns:     Module: self</p> <p>Parameters</p> <ul> <li>requires_grad     (bool)     \u2013 defaults to <code>True</code> </li> </ul> set_extra_state <p>This function is called from :func:<code>load_state_dict</code> to handle any extra state found within the <code>state_dict</code>. Implement this function and a corresponding :func:<code>get_extra_state</code> for your module if you need to store extra state within its <code>state_dict</code>.</p> <p>Args:     state (dict): Extra state from the <code>state_dict</code></p> <p>Parameters</p> <ul> <li>state     (Any)    </li> </ul> share_memory <p>See :meth:<code>torch.Tensor.share_memory_</code></p> state_dict <p>Returns a dictionary containing references to the whole state of the module.</p> <p>Both parameters and persistent buffers (e.g. running averages) are included. Keys are corresponding parameter and buffer names. Parameters and buffers set to <code>None</code> are not included.  .. note::     The returned object is a shallow copy. It contains references     to the module's parameters and buffers.  .. warning::     Currently <code>state_dict()</code> also accepts positional arguments for     <code>destination</code>, <code>prefix</code> and <code>keep_vars</code> in order. However,     this is being deprecated and keyword arguments will be enforced in     future releases.  .. warning::     Please avoid the use of argument <code>destination</code> as it is not     designed for end-users.  Args:     destination (dict, optional): If provided, the state of module will         be updated into the dict and the same object is returned.         Otherwise, an <code>OrderedDict</code> will be created and returned.         Default: <code>None</code>.     prefix (str, optional): a prefix added to parameter and buffer         names to compose the keys in state_dict. Default: <code>''</code>.     keep_vars (bool, optional): by default the :class:<code>~torch.Tensor</code> s         returned in the state dict are detached from autograd. If it's         set to <code>True</code>, detaching will not be performed.         Default: <code>False</code>.  Returns:     dict:         a dictionary containing a whole state of the module  Example::      &gt;&gt;&gt; # xdoctest: +SKIP(\"undefined vars\")     &gt;&gt;&gt; module.state_dict().keys()     ['bias', 'weight']</p> <p>Parameters</p> <ul> <li>args </li> <li>destination     \u2013 defaults to <code>None</code> </li> <li>prefix     \u2013 defaults to ``    </li> <li>keep_vars     \u2013 defaults to <code>False</code> </li> </ul> to <p>Moves and/or casts the parameters and buffers.</p> <p>This can be called as  .. function:: to(device=None, dtype=None, non_blocking=False)    :noindex:  .. function:: to(dtype, non_blocking=False)    :noindex:  .. function:: to(tensor, non_blocking=False)    :noindex:  .. function:: to(memory_format=torch.channels_last)    :noindex:  Its signature is similar to :meth:<code>torch.Tensor.to</code>, but only accepts floating point or complex :attr:<code>dtype</code>\\ s. In addition, this method will only cast the floating point or complex parameters and buffers to :attr:<code>dtype</code> (if given). The integral parameters and buffers will be moved :attr:<code>device</code>, if that is given, but with dtypes unchanged. When :attr:<code>non_blocking</code> is set, it tries to convert/move asynchronously with respect to the host if possible, e.g., moving CPU Tensors with pinned memory to CUDA devices.  See below for examples.  .. note::     This method modifies the module in-place.  Args:     device (:class:<code>torch.device</code>): the desired device of the parameters         and buffers in this module     dtype (:class:<code>torch.dtype</code>): the desired floating point or complex dtype of         the parameters and buffers in this module     tensor (torch.Tensor): Tensor whose dtype and device are the desired         dtype and device for all parameters and buffers in this module     memory_format (:class:<code>torch.memory_format</code>): the desired memory         format for 4D parameters and buffers in this module (keyword         only argument)  Returns:     Module: self  Examples::      &gt;&gt;&gt; # xdoctest: +IGNORE_WANT(\"non-deterministic\")     &gt;&gt;&gt; linear = nn.Linear(2, 2)     &gt;&gt;&gt; linear.weight     Parameter containing:     tensor([[ 0.1913, -0.3420],             [-0.5113, -0.2325]])     &gt;&gt;&gt; linear.to(torch.double)     Linear(in_features=2, out_features=2, bias=True)     &gt;&gt;&gt; linear.weight     Parameter containing:     tensor([[ 0.1913, -0.3420],             [-0.5113, -0.2325]], dtype=torch.float64)     &gt;&gt;&gt; # xdoctest: +REQUIRES(env:TORCH_DOCTEST_CUDA1)     &gt;&gt;&gt; gpu1 = torch.device(\"cuda:1\")     &gt;&gt;&gt; linear.to(gpu1, dtype=torch.half, non_blocking=True)     Linear(in_features=2, out_features=2, bias=True)     &gt;&gt;&gt; linear.weight     Parameter containing:     tensor([[ 0.1914, -0.3420],             [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1')     &gt;&gt;&gt; cpu = torch.device(\"cpu\")     &gt;&gt;&gt; linear.to(cpu)     Linear(in_features=2, out_features=2, bias=True)     &gt;&gt;&gt; linear.weight     Parameter containing:     tensor([[ 0.1914, -0.3420],             [-0.5112, -0.2324]], dtype=torch.float16)      &gt;&gt;&gt; linear = nn.Linear(2, 2, bias=None).to(torch.cdouble)     &gt;&gt;&gt; linear.weight     Parameter containing:     tensor([[ 0.3741+0.j,  0.2382+0.j],             [ 0.5593+0.j, -0.4443+0.j]], dtype=torch.complex128)     &gt;&gt;&gt; linear(torch.ones(3, 2, dtype=torch.cdouble))     tensor([[0.6122+0.j, 0.1150+0.j],             [0.6122+0.j, 0.1150+0.j],             [0.6122+0.j, 0.1150+0.j]], dtype=torch.complex128)</p> <p>Parameters</p> <ul> <li>args </li> <li>kwargs </li> </ul> to_empty <p>Moves the parameters and buffers to the specified device without copying storage.</p> <p>Args:     device (:class:<code>torch.device</code>): The desired device of the parameters         and buffers in this module.  Returns:     Module: self</p> <p>Parameters</p> <ul> <li>device     (Union[str, torch.device])    </li> </ul> train <p>Sets the module in training mode.</p> <p>This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class:<code>Dropout</code>, :class:<code>BatchNorm</code>, etc.  Args:     mode (bool): whether to set training mode (<code>True</code>) or evaluation                  mode (<code>False</code>). Default: <code>True</code>.  Returns:     Module: self</p> <p>Parameters</p> <ul> <li>mode     (bool)     \u2013 defaults to <code>True</code> </li> </ul> type <p>Casts all parameters and buffers to :attr:<code>dst_type</code>.</p> <p>.. note::     This method modifies the module in-place.  Args:     dst_type (type or string): the desired type  Returns:     Module: self</p> <p>Parameters</p> <ul> <li>dst_type     (Union[torch.dtype, str])    </li> </ul> xpu <p>Moves all model parameters and buffers to the XPU.</p> <p>This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on XPU while being optimized.  .. note::     This method modifies the module in-place.  Arguments:     device (int, optional): if specified, all parameters will be         copied to that device  Returns:     Module: self</p> <p>Parameters</p> <ul> <li>device     (Union[int, torch.device, NoneType])     \u2013 defaults to <code>None</code> </li> </ul> zero_grad <p>Sets gradients of all model parameters to zero. See similar function under :class:<code>torch.optim.Optimizer</code> for more context.</p> <p>Args:     set_to_none (bool): instead of setting to zero, set the grads to None.         See :meth:<code>torch.optim.Optimizer.zero_grad</code> for details.</p> <p>Parameters</p> <ul> <li>set_to_none     (bool)     \u2013 defaults to <code>True</code> </li> </ul>"},{"location":"api/losses/Ranking/#references","title":"References","text":"<ol> <li>SPLADE: Sparse Lexical and Expansion Model for First Stage Ranking</li> </ol>"},{"location":"api/models/ColBERT/","title":"ColBERT","text":"<p>ColBERT model.</p>"},{"location":"api/models/ColBERT/#parameters","title":"Parameters","text":"<ul> <li> <p>model_name_or_path (str)</p> <p>Path to the model or the model name.</p> </li> <li> <p>embedding_size (int) \u2013 defaults to <code>128</code></p> <p>Size of the embeddings in output of ColBERT model.</p> </li> <li> <p>device (str) \u2013 defaults to <code>None</code></p> <p>Device to use for the model. CPU or CUDA.</p> </li> <li> <p>max_length_query (int) \u2013 defaults to <code>32</code></p> </li> <li> <p>max_length_document (int) \u2013 defaults to <code>350</code></p> </li> <li> <p>kwargs</p> <p>Additional parameters to the SentenceTransformer model.</p> </li> </ul>"},{"location":"api/models/ColBERT/#examples","title":"Examples","text":"<pre><code>&gt;&gt;&gt; from neural_cherche import models\n&gt;&gt;&gt; import torch\n&gt;&gt;&gt; _ = torch.manual_seed(42)\n&gt;&gt;&gt; queries = [\"Berlin\", \"Paris\", \"London\"]\n&gt;&gt;&gt; documents = [\n...     \"Berlin is the capital of Germany\",\n...     \"Paris is the capital of France and France is in Europe\",\n...     \"London is the capital of England\",\n... ]\n&gt;&gt;&gt; encoder = models.ColBERT(\n...     model_name_or_path=\"raphaelsty/neural-cherche-colbert\",\n...     embedding_size=128,\n...     max_length_query=32,\n...     max_length_document=350,\n...     device=\"mps\",\n... )\n&gt;&gt;&gt; scores = encoder.scores(\n...    queries=queries,\n...    documents=documents,\n... )\n&gt;&gt;&gt; scores\ntensor([24.5880, 19.3780, 22.6769], device='mps:0', grad_fn=&lt;CatBackward0&gt;)\n&gt;&gt;&gt; _ = encoder.save_pretrained(\"checkpoint\")\n&gt;&gt;&gt; encoder = models.ColBERT(\n...     model_name_or_path=\"checkpoint\",\n...     embedding_size=64,\n...     device=\"cpu\",\n... )\n&gt;&gt;&gt; scores = encoder.scores(\n...    queries=queries,\n...    documents=documents,\n... )\n&gt;&gt;&gt; scores\ntensor([24.5880, 19.3781, 22.6769], grad_fn=&lt;CatBackward0&gt;)\n&gt;&gt;&gt; embeddings = encoder(\n...     texts=queries,\n...     query_mode=True\n... )\n&gt;&gt;&gt; embeddings[\"embeddings\"].shape\ntorch.Size([3, 32, 128])\n&gt;&gt;&gt; embeddings = encoder(\n...     texts=queries,\n...     query_mode=False\n... )\n&gt;&gt;&gt; embeddings[\"embeddings\"].shape\ntorch.Size([3, 350, 128])\n</code></pre>"},{"location":"api/models/ColBERT/#methods","title":"Methods","text":"call <p>Call self as a function.</p> <p>Parameters</p> <ul> <li>args </li> <li>kwargs      Additional parameters to the SentenceTransformer model.</li> </ul> add_module <p>Adds a child module to the current module.</p> <p>The module can be accessed as an attribute using the given name.  Args:     name (str): name of the child module. The child module can be         accessed from this module using the given name     module (Module): child module to be added to the module.</p> <p>Parameters</p> <ul> <li>name     (str)    </li> <li>module     (Optional[ForwardRef('Module')])    </li> </ul> apply <p>Applies <code>fn</code> recursively to every submodule (as returned by <code>.children()</code>) as well as self. Typical use includes initializing the parameters of a model (see also :ref:<code>nn-init-doc</code>).</p> <p>Args:     fn (:class:<code>Module</code> -&gt; None): function to be applied to each submodule  Returns:     Module: self  Example::      &gt;&gt;&gt; @torch.no_grad()     &gt;&gt;&gt; def init_weights(m):     &gt;&gt;&gt;     print(m)     &gt;&gt;&gt;     if type(m) == nn.Linear:     &gt;&gt;&gt;         m.weight.fill_(1.0)     &gt;&gt;&gt;         print(m.weight)     &gt;&gt;&gt; net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2))     &gt;&gt;&gt; net.apply(init_weights)     Linear(in_features=2, out_features=2, bias=True)     Parameter containing:     tensor([[1., 1.],             [1., 1.]], requires_grad=True)     Linear(in_features=2, out_features=2, bias=True)     Parameter containing:     tensor([[1., 1.],             [1., 1.]], requires_grad=True)     Sequential(       (0): Linear(in_features=2, out_features=2, bias=True)       (1): Linear(in_features=2, out_features=2, bias=True)     )</p> <p>Parameters</p> <ul> <li>fn     (Callable[[ForwardRef('Module')], NoneType])    </li> </ul> bfloat16 <p>Casts all floating point parameters and buffers to <code>bfloat16</code> datatype.</p> <p>.. note::     This method modifies the module in-place.  Returns:     Module: self</p> buffers <p>Returns an iterator over module buffers.</p> <p>Args:     recurse (bool): if True, then yields buffers of this module         and all submodules. Otherwise, yields only buffers that         are direct members of this module.  Yields:     torch.Tensor: module buffer  Example::      &gt;&gt;&gt; # xdoctest: +SKIP(\"undefined vars\")     &gt;&gt;&gt; for buf in model.buffers():     &gt;&gt;&gt;     print(type(buf), buf.size())      (20L,)      (20L, 1L, 5L, 5L) <p>Parameters</p> <ul> <li>recurse     (bool)     \u2013 defaults to <code>True</code> </li> </ul> children <p>Returns an iterator over immediate children modules.</p> <p>Yields:     Module: a child module</p> cpu <p>Moves all model parameters and buffers to the CPU.</p> <p>.. note::     This method modifies the module in-place.  Returns:     Module: self</p> cuda <p>Moves all model parameters and buffers to the GPU.</p> <p>This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on GPU while being optimized.  .. note::     This method modifies the module in-place.  Args:     device (int, optional): if specified, all parameters will be         copied to that device  Returns:     Module: self</p> <p>Parameters</p> <ul> <li>device     (Union[int, torch.device, NoneType])     \u2013 defaults to <code>None</code>      Device to use for the model. CPU or CUDA.</li> </ul> double <p>Casts all floating point parameters and buffers to <code>double</code> datatype.</p> <p>.. note::     This method modifies the module in-place.  Returns:     Module: self</p> encode <p>Encode documents</p> <p>Parameters</p> <ul> <li>texts     (list[str])    </li> <li>truncation     (bool)     \u2013 defaults to <code>True</code> </li> <li>add_special_tokens     (bool)     \u2013 defaults to <code>False</code> </li> <li>query_mode     (bool)     \u2013 defaults to <code>True</code> </li> <li>kwargs      Additional parameters to the SentenceTransformer model.</li> </ul> eval <p>Sets the module in evaluation mode.</p> <p>This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class:<code>Dropout</code>, :class:<code>BatchNorm</code>, etc.  This is equivalent with :meth:<code>self.train(False) &lt;torch.nn.Module.train&gt;</code>.  See :ref:<code>locally-disable-grad-doc</code> for a comparison between <code>.eval()</code> and several similar mechanisms that may be confused with it.  Returns:     Module: self</p> extra_repr <p>Set the extra representation of the module</p> <p>To print customized extra information, you should re-implement this method in your own modules. Both single-line and multi-line strings are acceptable.</p> float <p>Casts all floating point parameters and buffers to <code>float</code> datatype.</p> <p>.. note::     This method modifies the module in-place.  Returns:     Module: self</p> forward <p>Pytorch forward method.</p> <p>Parameters</p> <ul> <li>texts     (list[str])    </li> <li>query_mode     (bool)     \u2013 defaults to <code>True</code> </li> <li>kwargs      Additional parameters to the SentenceTransformer model.</li> </ul> get_buffer <p>Returns the buffer given by <code>target</code> if it exists, otherwise throws an error.</p> <p>See the docstring for <code>get_submodule</code> for a more detailed explanation of this method's functionality as well as how to correctly specify <code>target</code>.  Args:     target: The fully-qualified string name of the buffer         to look for. (See <code>get_submodule</code> for how to specify a         fully-qualified string.)  Returns:     torch.Tensor: The buffer referenced by <code>target</code>  Raises:     AttributeError: If the target string references an invalid         path or resolves to something that is not a         buffer</p> <p>Parameters</p> <ul> <li>target     (str)    </li> </ul> get_extra_state <p>Returns any extra state to include in the module's state_dict. Implement this and a corresponding :func:<code>set_extra_state</code> for your module if you need to store extra state. This function is called when building the module's <code>state_dict()</code>.</p> <p>Note that extra state should be picklable to ensure working serialization of the state_dict. We only provide provide backwards compatibility guarantees for serializing Tensors; other objects may break backwards compatibility if their serialized pickled form changes.  Returns:     object: Any extra state to store in the module's state_dict</p> get_parameter <p>Returns the parameter given by <code>target</code> if it exists, otherwise throws an error.</p> <p>See the docstring for <code>get_submodule</code> for a more detailed explanation of this method's functionality as well as how to correctly specify <code>target</code>.  Args:     target: The fully-qualified string name of the Parameter         to look for. (See <code>get_submodule</code> for how to specify a         fully-qualified string.)  Returns:     torch.nn.Parameter: The Parameter referenced by <code>target</code>  Raises:     AttributeError: If the target string references an invalid         path or resolves to something that is not an         <code>nn.Parameter</code></p> <p>Parameters</p> <ul> <li>target     (str)    </li> </ul> get_submodule <p>Returns the submodule given by <code>target</code> if it exists, otherwise throws an error.</p> <p>For example, let's say you have an <code>nn.Module</code> <code>A</code> that looks like this:  .. code-block:: text      A(         (net_b): Module(             (net_c): Module(                 (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2))             )             (linear): Linear(in_features=100, out_features=200, bias=True)         )     )  (The diagram shows an <code>nn.Module</code> <code>A</code>. <code>A</code> has a nested submodule <code>net_b</code>, which itself has two submodules <code>net_c</code> and <code>linear</code>. <code>net_c</code> then has a submodule <code>conv</code>.)  To check whether or not we have the <code>linear</code> submodule, we would call <code>get_submodule(\"net_b.linear\")</code>. To check whether we have the <code>conv</code> submodule, we would call <code>get_submodule(\"net_b.net_c.conv\")</code>.  The runtime of <code>get_submodule</code> is bounded by the degree of module nesting in <code>target</code>. A query against <code>named_modules</code> achieves the same result, but it is O(N) in the number of transitive modules. So, for a simple check to see if some submodule exists, <code>get_submodule</code> should always be used.  Args:     target: The fully-qualified string name of the submodule         to look for. (See above example for how to specify a         fully-qualified string.)  Returns:     torch.nn.Module: The submodule referenced by <code>target</code>  Raises:     AttributeError: If the target string references an invalid         path or resolves to something that is not an         <code>nn.Module</code></p> <p>Parameters</p> <ul> <li>target     (str)    </li> </ul> half <p>Casts all floating point parameters and buffers to <code>half</code> datatype.</p> <p>.. note::     This method modifies the module in-place.  Returns:     Module: self</p> ipu <p>Moves all model parameters and buffers to the IPU.</p> <p>This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on IPU while being optimized.  .. note::     This method modifies the module in-place.  Arguments:     device (int, optional): if specified, all parameters will be         copied to that device  Returns:     Module: self</p> <p>Parameters</p> <ul> <li>device     (Union[int, torch.device, NoneType])     \u2013 defaults to <code>None</code>      Device to use for the model. CPU or CUDA.</li> </ul> load_state_dict <p>Copies parameters and buffers from :attr:<code>state_dict</code> into this module and its descendants. If :attr:<code>strict</code> is <code>True</code>, then the keys of :attr:<code>state_dict</code> must exactly match the keys returned by this module's :meth:<code>~torch.nn.Module.state_dict</code> function.</p> <p>Args:     state_dict (dict): a dict containing parameters and         persistent buffers.     strict (bool, optional): whether to strictly enforce that the keys         in :attr:<code>state_dict</code> match the keys returned by this module's         :meth:<code>~torch.nn.Module.state_dict</code> function. Default: <code>True</code>  Returns:     <code>NamedTuple</code> with <code>missing_keys</code> and <code>unexpected_keys</code> fields:         * missing_keys is a list of str containing the missing keys         * unexpected_keys is a list of str containing the unexpected keys  Note:     If a parameter or buffer is registered as <code>None</code> and its corresponding key     exists in :attr:<code>state_dict</code>, :meth:<code>load_state_dict</code> will raise a     <code>RuntimeError</code>.</p> <p>Parameters</p> <ul> <li>state_dict     (Mapping[str, Any])    </li> <li>strict     (bool)     \u2013 defaults to <code>True</code> </li> </ul> modules <p>Returns an iterator over all modules in the network.</p> <p>Yields:     Module: a module in the network  Note:     Duplicate modules are returned only once. In the following     example, <code>l</code> will be returned only once.  Example::      &gt;&gt;&gt; l = nn.Linear(2, 2)     &gt;&gt;&gt; net = nn.Sequential(l, l)     &gt;&gt;&gt; for idx, m in enumerate(net.modules()):     ...     print(idx, '-&gt;', m)      0 -&gt; Sequential(       (0): Linear(in_features=2, out_features=2, bias=True)       (1): Linear(in_features=2, out_features=2, bias=True)     )     1 -&gt; Linear(in_features=2, out_features=2, bias=True)</p> named_buffers <p>Returns an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself.</p> <p>Args:     prefix (str): prefix to prepend to all buffer names.     recurse (bool, optional): if True, then yields buffers of this module         and all submodules. Otherwise, yields only buffers that         are direct members of this module. Defaults to True.     remove_duplicate (bool, optional): whether to remove the duplicated buffers in the result. Defaults to True.  Yields:     (str, torch.Tensor): Tuple containing the name and buffer  Example::      &gt;&gt;&gt; # xdoctest: +SKIP(\"undefined vars\")     &gt;&gt;&gt; for name, buf in self.named_buffers():     &gt;&gt;&gt;     if name in ['running_var']:     &gt;&gt;&gt;         print(buf.size())</p> <p>Parameters</p> <ul> <li>prefix     (str)     \u2013 defaults to ``    </li> <li>recurse     (bool)     \u2013 defaults to <code>True</code> </li> <li>remove_duplicate     (bool)     \u2013 defaults to <code>True</code> </li> </ul> named_children <p>Returns an iterator over immediate children modules, yielding both the name of the module as well as the module itself.</p> <p>Yields:     (str, Module): Tuple containing a name and child module  Example::      &gt;&gt;&gt; # xdoctest: +SKIP(\"undefined vars\")     &gt;&gt;&gt; for name, module in model.named_children():     &gt;&gt;&gt;     if name in ['conv4', 'conv5']:     &gt;&gt;&gt;         print(module)</p> named_modules <p>Returns an iterator over all modules in the network, yielding both the name of the module as well as the module itself.</p> <p>Args:     memo: a memo to store the set of modules already added to the result     prefix: a prefix that will be added to the name of the module     remove_duplicate: whether to remove the duplicated module instances in the result         or not  Yields:     (str, Module): Tuple of name and module  Note:     Duplicate modules are returned only once. In the following     example, <code>l</code> will be returned only once.  Example::      &gt;&gt;&gt; l = nn.Linear(2, 2)     &gt;&gt;&gt; net = nn.Sequential(l, l)     &gt;&gt;&gt; for idx, m in enumerate(net.named_modules()):     ...     print(idx, '-&gt;', m)      0 -&gt; ('', Sequential(       (0): Linear(in_features=2, out_features=2, bias=True)       (1): Linear(in_features=2, out_features=2, bias=True)     ))     1 -&gt; ('0', Linear(in_features=2, out_features=2, bias=True))</p> <p>Parameters</p> <ul> <li>memo     (Optional[Set[ForwardRef('Module')]])     \u2013 defaults to <code>None</code> </li> <li>prefix     (str)     \u2013 defaults to ``    </li> <li>remove_duplicate     (bool)     \u2013 defaults to <code>True</code> </li> </ul> named_parameters <p>Returns an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself.</p> <p>Args:     prefix (str): prefix to prepend to all parameter names.     recurse (bool): if True, then yields parameters of this module         and all submodules. Otherwise, yields only parameters that         are direct members of this module.     remove_duplicate (bool, optional): whether to remove the duplicated         parameters in the result. Defaults to True.  Yields:     (str, Parameter): Tuple containing the name and parameter  Example::      &gt;&gt;&gt; # xdoctest: +SKIP(\"undefined vars\")     &gt;&gt;&gt; for name, param in self.named_parameters():     &gt;&gt;&gt;     if name in ['bias']:     &gt;&gt;&gt;         print(param.size())</p> <p>Parameters</p> <ul> <li>prefix     (str)     \u2013 defaults to ``    </li> <li>recurse     (bool)     \u2013 defaults to <code>True</code> </li> <li>remove_duplicate     (bool)     \u2013 defaults to <code>True</code> </li> </ul> parameters <p>Returns an iterator over module parameters.</p> <p>This is typically passed to an optimizer.  Args:     recurse (bool): if True, then yields parameters of this module         and all submodules. Otherwise, yields only parameters that         are direct members of this module.  Yields:     Parameter: module parameter  Example::      &gt;&gt;&gt; # xdoctest: +SKIP(\"undefined vars\")     &gt;&gt;&gt; for param in model.parameters():     &gt;&gt;&gt;     print(type(param), param.size())      (20L,)      (20L, 1L, 5L, 5L) <p>Parameters</p> <ul> <li>recurse     (bool)     \u2013 defaults to <code>True</code> </li> </ul> register_backward_hook <p>Registers a backward hook on the module.</p> <p>This function is deprecated in favor of :meth:<code>~torch.nn.Module.register_full_backward_hook</code> and the behavior of this function will change in future versions.  Returns:     :class:<code>torch.utils.hooks.RemovableHandle</code>:         a handle that can be used to remove the added hook by calling         <code>handle.remove()</code></p> <p>Parameters</p> <ul> <li>hook     (Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, Tuple[torch.Tensor, ...], torch.Tensor]])    </li> </ul> register_buffer <p>Adds a buffer to the module.</p> <p>This is typically used to register a buffer that should not to be considered a model parameter. For example, BatchNorm's <code>running_mean</code> is not a parameter, but is part of the module's state. Buffers, by default, are persistent and will be saved alongside parameters. This behavior can be changed by setting :attr:<code>persistent</code> to <code>False</code>. The only difference between a persistent buffer and a non-persistent buffer is that the latter will not be a part of this module's :attr:<code>state_dict</code>.  Buffers can be accessed as attributes using given names.  Args:     name (str): name of the buffer. The buffer can be accessed         from this module using the given name     tensor (Tensor or None): buffer to be registered. If <code>None</code>, then operations         that run on buffers, such as :attr:<code>cuda</code>, are ignored. If <code>None</code>,         the buffer is not included in the module's :attr:<code>state_dict</code>.     persistent (bool): whether the buffer is part of this module's         :attr:<code>state_dict</code>.  Example::      &gt;&gt;&gt; # xdoctest: +SKIP(\"undefined vars\")     &gt;&gt;&gt; self.register_buffer('running_mean', torch.zeros(num_features))</p> <p>Parameters</p> <ul> <li>name     (str)    </li> <li>tensor     (Optional[torch.Tensor])    </li> <li>persistent     (bool)     \u2013 defaults to <code>True</code> </li> </ul> register_forward_hook <p>Registers a forward hook on the module.</p> <p>The hook will be called every time after :func:<code>forward</code> has computed an output.  If <code>with_kwargs</code> is <code>False</code> or not specified, the input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the <code>forward</code>. The hook can modify the output. It can modify the input inplace but it will not have effect on forward since this is called after :func:<code>forward</code> is called. The hook should have the following signature::      hook(module, args, output) -&gt; None or modified output  If <code>with_kwargs</code> is <code>True</code>, the forward hook will be passed the <code>kwargs</code> given to the forward function and be expected to return the output possibly modified. The hook should have the following signature::      hook(module, args, kwargs, output) -&gt; None or modified output  Args:     hook (Callable): The user defined hook to be registered.     prepend (bool): If <code>True</code>, the provided <code>hook</code> will be fired         before all existing <code>forward</code> hooks on this         :class:<code>torch.nn.modules.Module</code>. Otherwise, the provided         <code>hook</code> will be fired after all existing <code>forward</code> hooks on         this :class:<code>torch.nn.modules.Module</code>. Note that global         <code>forward</code> hooks registered with         :func:<code>register_module_forward_hook</code> will fire before all hooks         registered by this method.         Default: <code>False</code>     with_kwargs (bool): If <code>True</code>, the <code>hook</code> will be passed the         kwargs given to the forward function.         Default: <code>False</code>  Returns:     :class:<code>torch.utils.hooks.RemovableHandle</code>:         a handle that can be used to remove the added hook by calling         <code>handle.remove()</code></p> <p>Parameters</p> <ul> <li>hook     (Union[Callable[[~T, Tuple[Any, ...], Any], Optional[Any]], Callable[[~T, Tuple[Any, ...], Dict[str, Any], Any], Optional[Any]]])    </li> <li>prepend     (bool)     \u2013 defaults to <code>False</code> </li> <li>with_kwargs     (bool)     \u2013 defaults to <code>False</code> </li> </ul> register_forward_pre_hook <p>Registers a forward pre-hook on the module.</p> <p>The hook will be called every time before :func:<code>forward</code> is invoked.  If <code>with_kwargs</code> is false or not specified, the input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the <code>forward</code>. The hook can modify the input. User can either return a tuple or a single modified value in the hook. We will wrap the value into a tuple if a single value is returned (unless that value is already a tuple). The hook should have the following signature::      hook(module, args) -&gt; None or modified input  If <code>with_kwargs</code> is true, the forward pre-hook will be passed the kwargs given to the forward function. And if the hook modifies the input, both the args and kwargs should be returned. The hook should have the following signature::      hook(module, args, kwargs) -&gt; None or a tuple of modified input and kwargs  Args:     hook (Callable): The user defined hook to be registered.     prepend (bool): If true, the provided <code>hook</code> will be fired before         all existing <code>forward_pre</code> hooks on this         :class:<code>torch.nn.modules.Module</code>. Otherwise, the provided         <code>hook</code> will be fired after all existing <code>forward_pre</code> hooks         on this :class:<code>torch.nn.modules.Module</code>. Note that global         <code>forward_pre</code> hooks registered with         :func:<code>register_module_forward_pre_hook</code> will fire before all         hooks registered by this method.         Default: <code>False</code>     with_kwargs (bool): If true, the <code>hook</code> will be passed the kwargs         given to the forward function.         Default: <code>False</code>  Returns:     :class:<code>torch.utils.hooks.RemovableHandle</code>:         a handle that can be used to remove the added hook by calling         <code>handle.remove()</code></p> <p>Parameters</p> <ul> <li>hook     (Union[Callable[[~T, Tuple[Any, ...]], Optional[Any]], Callable[[~T, Tuple[Any, ...], Dict[str, Any]], Optional[Tuple[Any, Dict[str, Any]]]]])    </li> <li>prepend     (bool)     \u2013 defaults to <code>False</code> </li> <li>with_kwargs     (bool)     \u2013 defaults to <code>False</code> </li> </ul> register_full_backward_hook <p>Registers a backward hook on the module.</p> <p>The hook will be called every time the gradients with respect to a module are computed, i.e. the hook will execute if and only if the gradients with respect to module outputs are computed. The hook should have the following signature::      hook(module, grad_input, grad_output) -&gt; tuple(Tensor) or None  The :attr:<code>grad_input</code> and :attr:<code>grad_output</code> are tuples that contain the gradients with respect to the inputs and outputs respectively. The hook should not modify its arguments, but it can optionally return a new gradient with respect to the input that will be used in place of :attr:<code>grad_input</code> in subsequent computations. :attr:<code>grad_input</code> will only correspond to the inputs given as positional arguments and all kwarg arguments are ignored. Entries in :attr:<code>grad_input</code> and :attr:<code>grad_output</code> will be <code>None</code> for all non-Tensor arguments.  For technical reasons, when this hook is applied to a Module, its forward function will receive a view of each Tensor passed to the Module. Similarly the caller will receive a view of each Tensor returned by the Module's forward function.  .. warning ::     Modifying inputs or outputs inplace is not allowed when using backward hooks and     will raise an error.  Args:     hook (Callable): The user-defined hook to be registered.     prepend (bool): If true, the provided <code>hook</code> will be fired before         all existing <code>backward</code> hooks on this         :class:<code>torch.nn.modules.Module</code>. Otherwise, the provided         <code>hook</code> will be fired after all existing <code>backward</code> hooks on         this :class:<code>torch.nn.modules.Module</code>. Note that global         <code>backward</code> hooks registered with         :func:<code>register_module_full_backward_hook</code> will fire before         all hooks registered by this method.  Returns:     :class:<code>torch.utils.hooks.RemovableHandle</code>:         a handle that can be used to remove the added hook by calling         <code>handle.remove()</code></p> <p>Parameters</p> <ul> <li>hook     (Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, Tuple[torch.Tensor, ...], torch.Tensor]])    </li> <li>prepend     (bool)     \u2013 defaults to <code>False</code> </li> </ul> register_full_backward_pre_hook <p>Registers a backward pre-hook on the module.</p> <p>The hook will be called every time the gradients for the module are computed. The hook should have the following signature::      hook(module, grad_output) -&gt; Tensor or None  The :attr:<code>grad_output</code> is a tuple. The hook should not modify its arguments, but it can optionally return a new gradient with respect to the output that will be used in place of :attr:<code>grad_output</code> in subsequent computations. Entries in :attr:<code>grad_output</code> will be <code>None</code> for all non-Tensor arguments.  For technical reasons, when this hook is applied to a Module, its forward function will receive a view of each Tensor passed to the Module. Similarly the caller will receive a view of each Tensor returned by the Module's forward function.  .. warning ::     Modifying inputs inplace is not allowed when using backward hooks and     will raise an error.  Args:     hook (Callable): The user-defined hook to be registered.     prepend (bool): If true, the provided <code>hook</code> will be fired before         all existing <code>backward_pre</code> hooks on this         :class:<code>torch.nn.modules.Module</code>. Otherwise, the provided         <code>hook</code> will be fired after all existing <code>backward_pre</code> hooks         on this :class:<code>torch.nn.modules.Module</code>. Note that global         <code>backward_pre</code> hooks registered with         :func:<code>register_module_full_backward_pre_hook</code> will fire before         all hooks registered by this method.  Returns:     :class:<code>torch.utils.hooks.RemovableHandle</code>:         a handle that can be used to remove the added hook by calling         <code>handle.remove()</code></p> <p>Parameters</p> <ul> <li>hook     (Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, Tuple[torch.Tensor, ...], torch.Tensor]])    </li> <li>prepend     (bool)     \u2013 defaults to <code>False</code> </li> </ul> register_load_state_dict_post_hook <p>Registers a post hook to be run after module's <code>load_state_dict</code> is called.</p> <p>It should have the following signature::     hook(module, incompatible_keys) -&gt; None  The <code>module</code> argument is the current module that this hook is registered on, and the <code>incompatible_keys</code> argument is a <code>NamedTuple</code> consisting of attributes <code>missing_keys</code> and <code>unexpected_keys</code>. <code>missing_keys</code> is a <code>list</code> of <code>str</code> containing the missing keys and <code>unexpected_keys</code> is a <code>list</code> of <code>str</code> containing the unexpected keys.  The given incompatible_keys can be modified inplace if needed.  Note that the checks performed when calling :func:<code>load_state_dict</code> with <code>strict=True</code> are affected by modifications the hook makes to <code>missing_keys</code> or <code>unexpected_keys</code>, as expected. Additions to either set of keys will result in an error being thrown when <code>strict=True</code>, and clearing out both missing and unexpected keys will avoid an error.  Returns:     :class:<code>torch.utils.hooks.RemovableHandle</code>:         a handle that can be used to remove the added hook by calling         <code>handle.remove()</code></p> <p>Parameters</p> <ul> <li>hook </li> </ul> register_module <p>Alias for :func:<code>add_module</code>.</p> <p>Parameters</p> <ul> <li>name     (str)    </li> <li>module     (Optional[ForwardRef('Module')])    </li> </ul> register_parameter <p>Adds a parameter to the module.</p> <p>The parameter can be accessed as an attribute using given name.  Args:     name (str): name of the parameter. The parameter can be accessed         from this module using the given name     param (Parameter or None): parameter to be added to the module. If         <code>None</code>, then operations that run on parameters, such as :attr:<code>cuda</code>,         are ignored. If <code>None</code>, the parameter is not included in the         module's :attr:<code>state_dict</code>.</p> <p>Parameters</p> <ul> <li>name     (str)    </li> <li>param     (Optional[torch.nn.parameter.Parameter])    </li> </ul> register_state_dict_pre_hook <p>These hooks will be called with arguments: <code>self</code>, <code>prefix</code>, and <code>keep_vars</code> before calling <code>state_dict</code> on <code>self</code>. The registered hooks can be used to perform pre-processing before the <code>state_dict</code> call is made.</p> <p>Parameters</p> <ul> <li>hook </li> </ul> requires_grad_ <p>Change if autograd should record operations on parameters in this module.</p> <p>This method sets the parameters' :attr:<code>requires_grad</code> attributes in-place.  This method is helpful for freezing part of the module for finetuning or training parts of a model individually (e.g., GAN training).  See :ref:<code>locally-disable-grad-doc</code> for a comparison between <code>.requires_grad_()</code> and several similar mechanisms that may be confused with it.  Args:     requires_grad (bool): whether autograd should record operations on                           parameters in this module. Default: <code>True</code>.  Returns:     Module: self</p> <p>Parameters</p> <ul> <li>requires_grad     (bool)     \u2013 defaults to <code>True</code> </li> </ul> save_pretrained <p>Save model the model.</p> <p>Parameters</p> <ul> <li>path     (str)    </li> </ul> scores <p>Score queries and documents.</p> <p>Parameters</p> <ul> <li>queries     (list[str])    </li> <li>documents     (list[str])    </li> <li>batch_size     (int)     \u2013 defaults to <code>2</code> </li> <li>tqdm_bar     (bool)     \u2013 defaults to <code>True</code> </li> <li>kwargs      Additional parameters to the SentenceTransformer model.</li> </ul> set_extra_state <p>This function is called from :func:<code>load_state_dict</code> to handle any extra state found within the <code>state_dict</code>. Implement this function and a corresponding :func:<code>get_extra_state</code> for your module if you need to store extra state within its <code>state_dict</code>.</p> <p>Args:     state (dict): Extra state from the <code>state_dict</code></p> <p>Parameters</p> <ul> <li>state     (Any)    </li> </ul> share_memory <p>See :meth:<code>torch.Tensor.share_memory_</code></p> state_dict <p>Returns a dictionary containing references to the whole state of the module.</p> <p>Both parameters and persistent buffers (e.g. running averages) are included. Keys are corresponding parameter and buffer names. Parameters and buffers set to <code>None</code> are not included.  .. note::     The returned object is a shallow copy. It contains references     to the module's parameters and buffers.  .. warning::     Currently <code>state_dict()</code> also accepts positional arguments for     <code>destination</code>, <code>prefix</code> and <code>keep_vars</code> in order. However,     this is being deprecated and keyword arguments will be enforced in     future releases.  .. warning::     Please avoid the use of argument <code>destination</code> as it is not     designed for end-users.  Args:     destination (dict, optional): If provided, the state of module will         be updated into the dict and the same object is returned.         Otherwise, an <code>OrderedDict</code> will be created and returned.         Default: <code>None</code>.     prefix (str, optional): a prefix added to parameter and buffer         names to compose the keys in state_dict. Default: <code>''</code>.     keep_vars (bool, optional): by default the :class:<code>~torch.Tensor</code> s         returned in the state dict are detached from autograd. If it's         set to <code>True</code>, detaching will not be performed.         Default: <code>False</code>.  Returns:     dict:         a dictionary containing a whole state of the module  Example::      &gt;&gt;&gt; # xdoctest: +SKIP(\"undefined vars\")     &gt;&gt;&gt; module.state_dict().keys()     ['bias', 'weight']</p> <p>Parameters</p> <ul> <li>args </li> <li>destination     \u2013 defaults to <code>None</code> </li> <li>prefix     \u2013 defaults to ``    </li> <li>keep_vars     \u2013 defaults to <code>False</code> </li> </ul> to <p>Moves and/or casts the parameters and buffers.</p> <p>This can be called as  .. function:: to(device=None, dtype=None, non_blocking=False)    :noindex:  .. function:: to(dtype, non_blocking=False)    :noindex:  .. function:: to(tensor, non_blocking=False)    :noindex:  .. function:: to(memory_format=torch.channels_last)    :noindex:  Its signature is similar to :meth:<code>torch.Tensor.to</code>, but only accepts floating point or complex :attr:<code>dtype</code>\\ s. In addition, this method will only cast the floating point or complex parameters and buffers to :attr:<code>dtype</code> (if given). The integral parameters and buffers will be moved :attr:<code>device</code>, if that is given, but with dtypes unchanged. When :attr:<code>non_blocking</code> is set, it tries to convert/move asynchronously with respect to the host if possible, e.g., moving CPU Tensors with pinned memory to CUDA devices.  See below for examples.  .. note::     This method modifies the module in-place.  Args:     device (:class:<code>torch.device</code>): the desired device of the parameters         and buffers in this module     dtype (:class:<code>torch.dtype</code>): the desired floating point or complex dtype of         the parameters and buffers in this module     tensor (torch.Tensor): Tensor whose dtype and device are the desired         dtype and device for all parameters and buffers in this module     memory_format (:class:<code>torch.memory_format</code>): the desired memory         format for 4D parameters and buffers in this module (keyword         only argument)  Returns:     Module: self  Examples::      &gt;&gt;&gt; # xdoctest: +IGNORE_WANT(\"non-deterministic\")     &gt;&gt;&gt; linear = nn.Linear(2, 2)     &gt;&gt;&gt; linear.weight     Parameter containing:     tensor([[ 0.1913, -0.3420],             [-0.5113, -0.2325]])     &gt;&gt;&gt; linear.to(torch.double)     Linear(in_features=2, out_features=2, bias=True)     &gt;&gt;&gt; linear.weight     Parameter containing:     tensor([[ 0.1913, -0.3420],             [-0.5113, -0.2325]], dtype=torch.float64)     &gt;&gt;&gt; # xdoctest: +REQUIRES(env:TORCH_DOCTEST_CUDA1)     &gt;&gt;&gt; gpu1 = torch.device(\"cuda:1\")     &gt;&gt;&gt; linear.to(gpu1, dtype=torch.half, non_blocking=True)     Linear(in_features=2, out_features=2, bias=True)     &gt;&gt;&gt; linear.weight     Parameter containing:     tensor([[ 0.1914, -0.3420],             [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1')     &gt;&gt;&gt; cpu = torch.device(\"cpu\")     &gt;&gt;&gt; linear.to(cpu)     Linear(in_features=2, out_features=2, bias=True)     &gt;&gt;&gt; linear.weight     Parameter containing:     tensor([[ 0.1914, -0.3420],             [-0.5112, -0.2324]], dtype=torch.float16)      &gt;&gt;&gt; linear = nn.Linear(2, 2, bias=None).to(torch.cdouble)     &gt;&gt;&gt; linear.weight     Parameter containing:     tensor([[ 0.3741+0.j,  0.2382+0.j],             [ 0.5593+0.j, -0.4443+0.j]], dtype=torch.complex128)     &gt;&gt;&gt; linear(torch.ones(3, 2, dtype=torch.cdouble))     tensor([[0.6122+0.j, 0.1150+0.j],             [0.6122+0.j, 0.1150+0.j],             [0.6122+0.j, 0.1150+0.j]], dtype=torch.complex128)</p> <p>Parameters</p> <ul> <li>args </li> <li>kwargs      Additional parameters to the SentenceTransformer model.</li> </ul> to_empty <p>Moves the parameters and buffers to the specified device without copying storage.</p> <p>Args:     device (:class:<code>torch.device</code>): The desired device of the parameters         and buffers in this module.  Returns:     Module: self</p> <p>Parameters</p> <ul> <li>device     (Union[str, torch.device])       Device to use for the model. CPU or CUDA.</li> </ul> train <p>Sets the module in training mode.</p> <p>This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class:<code>Dropout</code>, :class:<code>BatchNorm</code>, etc.  Args:     mode (bool): whether to set training mode (<code>True</code>) or evaluation                  mode (<code>False</code>). Default: <code>True</code>.  Returns:     Module: self</p> <p>Parameters</p> <ul> <li>mode     (bool)     \u2013 defaults to <code>True</code> </li> </ul> type <p>Casts all parameters and buffers to :attr:<code>dst_type</code>.</p> <p>.. note::     This method modifies the module in-place.  Args:     dst_type (type or string): the desired type  Returns:     Module: self</p> <p>Parameters</p> <ul> <li>dst_type     (Union[torch.dtype, str])    </li> </ul> xpu <p>Moves all model parameters and buffers to the XPU.</p> <p>This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on XPU while being optimized.  .. note::     This method modifies the module in-place.  Arguments:     device (int, optional): if specified, all parameters will be         copied to that device  Returns:     Module: self</p> <p>Parameters</p> <ul> <li>device     (Union[int, torch.device, NoneType])     \u2013 defaults to <code>None</code>      Device to use for the model. CPU or CUDA.</li> </ul> zero_grad <p>Sets gradients of all model parameters to zero. See similar function under :class:<code>torch.optim.Optimizer</code> for more context.</p> <p>Args:     set_to_none (bool): instead of setting to zero, set the grads to None.         See :meth:<code>torch.optim.Optimizer.zero_grad</code> for details.</p> <p>Parameters</p> <ul> <li>set_to_none     (bool)     \u2013 defaults to <code>True</code> </li> </ul>"},{"location":"api/models/SparseEmbed/","title":"SparseEmbed","text":"<p>SparseEmbed model.</p>"},{"location":"api/models/SparseEmbed/#parameters","title":"Parameters","text":"<ul> <li> <p>model_name_or_path (str) \u2013 defaults to <code>None</code></p> <p>Path to the model or the model name. It should be a SentenceTransformer model.</p> </li> <li> <p>embedding_size (int) \u2013 defaults to <code>128</code></p> <p>Size of the embeddings in output of SparsEmbed model.</p> </li> <li> <p>max_length_query (int) \u2013 defaults to <code>128</code></p> </li> <li> <p>max_length_document (int) \u2013 defaults to <code>256</code></p> </li> <li> <p>device (str) \u2013 defaults to <code>None</code></p> </li> <li> <p>kwargs</p> <p>Additional parameters to the pre-trained model.</p> </li> </ul>"},{"location":"api/models/SparseEmbed/#examples","title":"Examples","text":"<pre><code>&gt;&gt;&gt; from neural_cherche import models\n&gt;&gt;&gt; import torch\n&gt;&gt;&gt; _ = torch.manual_seed(42)\n&gt;&gt;&gt; device = \"mps\"\n&gt;&gt;&gt; model = models.SparseEmbed(\n...     model_name_or_path=\"raphaelsty/neural-cherche-sparse-embed\",\n...     device=device,\n... )\n&gt;&gt;&gt; queries_embeddings = model.encode(\n...     [\"Sports\", \"Music\"],\n... )\n&gt;&gt;&gt; queries_embeddings[\"activations\"].shape\ntorch.Size([2, 128])\n&gt;&gt;&gt; queries_embeddings[\"sparse_activations\"].shape\ntorch.Size([2, 30522])\n&gt;&gt;&gt; queries_embeddings[\"embeddings\"].shape\ntorch.Size([2, 128, 128])\n&gt;&gt;&gt; documents_embeddings = model.encode(\n...    [\"Music is great.\", \"Sports is great.\"],\n...    query_mode=False,\n... )\n&gt;&gt;&gt; documents_embeddings[\"activations\"].shape\ntorch.Size([2, 256])\n&gt;&gt;&gt; documents_embeddings[\"sparse_activations\"].shape\ntorch.Size([2, 30522])\n&gt;&gt;&gt; documents_embeddings[\"embeddings\"].shape\ntorch.Size([2, 256, 128])\n&gt;&gt;&gt; model.scores(\n...     queries=[\"Sports\", \"Music\"],\n...     documents=[\"Sports is great.\", \"Music is great.\"],\n...     batch_size=1,\n... )\ntensor([64.2330, 54.0180], device='mps:0')\n&gt;&gt;&gt; _ = model.save_pretrained(\"checkpoint\")\n&gt;&gt;&gt; model = models.SparseEmbed(\n...     model_name_or_path=\"checkpoint\",\n...     device=\"cpu\",\n... )\n&gt;&gt;&gt; model.scores(\n...     queries=[\"Sports\", \"Music\"],\n...     documents=[\"Sports is great.\", \"Music is great.\"],\n...     batch_size=2,\n... )\ntensor([64.2330, 54.0180])\n</code></pre>"},{"location":"api/models/SparseEmbed/#methods","title":"Methods","text":"call <p>Call self as a function.</p> <p>Parameters</p> <ul> <li>args </li> <li>kwargs      Additional parameters to the pre-trained model.</li> </ul> add_module <p>Adds a child module to the current module.</p> <p>The module can be accessed as an attribute using the given name.  Args:     name (str): name of the child module. The child module can be         accessed from this module using the given name     module (Module): child module to be added to the module.</p> <p>Parameters</p> <ul> <li>name     (str)    </li> <li>module     (Optional[ForwardRef('Module')])    </li> </ul> apply <p>Applies <code>fn</code> recursively to every submodule (as returned by <code>.children()</code>) as well as self. Typical use includes initializing the parameters of a model (see also :ref:<code>nn-init-doc</code>).</p> <p>Args:     fn (:class:<code>Module</code> -&gt; None): function to be applied to each submodule  Returns:     Module: self  Example::      &gt;&gt;&gt; @torch.no_grad()     &gt;&gt;&gt; def init_weights(m):     &gt;&gt;&gt;     print(m)     &gt;&gt;&gt;     if type(m) == nn.Linear:     &gt;&gt;&gt;         m.weight.fill_(1.0)     &gt;&gt;&gt;         print(m.weight)     &gt;&gt;&gt; net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2))     &gt;&gt;&gt; net.apply(init_weights)     Linear(in_features=2, out_features=2, bias=True)     Parameter containing:     tensor([[1., 1.],             [1., 1.]], requires_grad=True)     Linear(in_features=2, out_features=2, bias=True)     Parameter containing:     tensor([[1., 1.],             [1., 1.]], requires_grad=True)     Sequential(       (0): Linear(in_features=2, out_features=2, bias=True)       (1): Linear(in_features=2, out_features=2, bias=True)     )</p> <p>Parameters</p> <ul> <li>fn     (Callable[[ForwardRef('Module')], NoneType])    </li> </ul> bfloat16 <p>Casts all floating point parameters and buffers to <code>bfloat16</code> datatype.</p> <p>.. note::     This method modifies the module in-place.  Returns:     Module: self</p> buffers <p>Returns an iterator over module buffers.</p> <p>Args:     recurse (bool): if True, then yields buffers of this module         and all submodules. Otherwise, yields only buffers that         are direct members of this module.  Yields:     torch.Tensor: module buffer  Example::      &gt;&gt;&gt; # xdoctest: +SKIP(\"undefined vars\")     &gt;&gt;&gt; for buf in model.buffers():     &gt;&gt;&gt;     print(type(buf), buf.size())      (20L,)      (20L, 1L, 5L, 5L) <p>Parameters</p> <ul> <li>recurse     (bool)     \u2013 defaults to <code>True</code> </li> </ul> children <p>Returns an iterator over immediate children modules.</p> <p>Yields:     Module: a child module</p> cpu <p>Moves all model parameters and buffers to the CPU.</p> <p>.. note::     This method modifies the module in-place.  Returns:     Module: self</p> cuda <p>Moves all model parameters and buffers to the GPU.</p> <p>This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on GPU while being optimized.  .. note::     This method modifies the module in-place.  Args:     device (int, optional): if specified, all parameters will be         copied to that device  Returns:     Module: self</p> <p>Parameters</p> <ul> <li>device     (Union[int, torch.device, NoneType])     \u2013 defaults to <code>None</code> </li> </ul> decode <p>Decode activated tokens ids where activated value &gt; 0.</p> <p>Parameters</p> <ul> <li>sparse_activations     (torch.Tensor)    </li> <li>clean_up_tokenization_spaces     (bool)     \u2013 defaults to <code>False</code> </li> <li>skip_special_tokens     (bool)     \u2013 defaults to <code>True</code> </li> <li>k_tokens     (int)     \u2013 defaults to <code>96</code> </li> </ul> double <p>Casts all floating point parameters and buffers to <code>double</code> datatype.</p> <p>.. note::     This method modifies the module in-place.  Returns:     Module: self</p> encode <p>Encode documents</p> <p>Parameters</p> <ul> <li>texts     (list[str])    </li> <li>query_mode     (bool)     \u2013 defaults to <code>True</code> </li> <li>kwargs      Additional parameters to the pre-trained model.</li> </ul> eval <p>Sets the module in evaluation mode.</p> <p>This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class:<code>Dropout</code>, :class:<code>BatchNorm</code>, etc.  This is equivalent with :meth:<code>self.train(False) &lt;torch.nn.Module.train&gt;</code>.  See :ref:<code>locally-disable-grad-doc</code> for a comparison between <code>.eval()</code> and several similar mechanisms that may be confused with it.  Returns:     Module: self</p> extra_repr <p>Set the extra representation of the module</p> <p>To print customized extra information, you should re-implement this method in your own modules. Both single-line and multi-line strings are acceptable.</p> float <p>Casts all floating point parameters and buffers to <code>float</code> datatype.</p> <p>.. note::     This method modifies the module in-place.  Returns:     Module: self</p> forward <p>Pytorch forward method.</p> <p>Parameters</p> <ul> <li>texts     (list[str])    </li> <li>query_mode     (bool)     \u2013 defaults to <code>True</code> </li> <li>kwargs      Additional parameters to the pre-trained model.</li> </ul> get_buffer <p>Returns the buffer given by <code>target</code> if it exists, otherwise throws an error.</p> <p>See the docstring for <code>get_submodule</code> for a more detailed explanation of this method's functionality as well as how to correctly specify <code>target</code>.  Args:     target: The fully-qualified string name of the buffer         to look for. (See <code>get_submodule</code> for how to specify a         fully-qualified string.)  Returns:     torch.Tensor: The buffer referenced by <code>target</code>  Raises:     AttributeError: If the target string references an invalid         path or resolves to something that is not a         buffer</p> <p>Parameters</p> <ul> <li>target     (str)    </li> </ul> get_extra_state <p>Returns any extra state to include in the module's state_dict. Implement this and a corresponding :func:<code>set_extra_state</code> for your module if you need to store extra state. This function is called when building the module's <code>state_dict()</code>.</p> <p>Note that extra state should be picklable to ensure working serialization of the state_dict. We only provide provide backwards compatibility guarantees for serializing Tensors; other objects may break backwards compatibility if their serialized pickled form changes.  Returns:     object: Any extra state to store in the module's state_dict</p> get_parameter <p>Returns the parameter given by <code>target</code> if it exists, otherwise throws an error.</p> <p>See the docstring for <code>get_submodule</code> for a more detailed explanation of this method's functionality as well as how to correctly specify <code>target</code>.  Args:     target: The fully-qualified string name of the Parameter         to look for. (See <code>get_submodule</code> for how to specify a         fully-qualified string.)  Returns:     torch.nn.Parameter: The Parameter referenced by <code>target</code>  Raises:     AttributeError: If the target string references an invalid         path or resolves to something that is not an         <code>nn.Parameter</code></p> <p>Parameters</p> <ul> <li>target     (str)    </li> </ul> get_submodule <p>Returns the submodule given by <code>target</code> if it exists, otherwise throws an error.</p> <p>For example, let's say you have an <code>nn.Module</code> <code>A</code> that looks like this:  .. code-block:: text      A(         (net_b): Module(             (net_c): Module(                 (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2))             )             (linear): Linear(in_features=100, out_features=200, bias=True)         )     )  (The diagram shows an <code>nn.Module</code> <code>A</code>. <code>A</code> has a nested submodule <code>net_b</code>, which itself has two submodules <code>net_c</code> and <code>linear</code>. <code>net_c</code> then has a submodule <code>conv</code>.)  To check whether or not we have the <code>linear</code> submodule, we would call <code>get_submodule(\"net_b.linear\")</code>. To check whether we have the <code>conv</code> submodule, we would call <code>get_submodule(\"net_b.net_c.conv\")</code>.  The runtime of <code>get_submodule</code> is bounded by the degree of module nesting in <code>target</code>. A query against <code>named_modules</code> achieves the same result, but it is O(N) in the number of transitive modules. So, for a simple check to see if some submodule exists, <code>get_submodule</code> should always be used.  Args:     target: The fully-qualified string name of the submodule         to look for. (See above example for how to specify a         fully-qualified string.)  Returns:     torch.nn.Module: The submodule referenced by <code>target</code>  Raises:     AttributeError: If the target string references an invalid         path or resolves to something that is not an         <code>nn.Module</code></p> <p>Parameters</p> <ul> <li>target     (str)    </li> </ul> half <p>Casts all floating point parameters and buffers to <code>half</code> datatype.</p> <p>.. note::     This method modifies the module in-place.  Returns:     Module: self</p> ipu <p>Moves all model parameters and buffers to the IPU.</p> <p>This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on IPU while being optimized.  .. note::     This method modifies the module in-place.  Arguments:     device (int, optional): if specified, all parameters will be         copied to that device  Returns:     Module: self</p> <p>Parameters</p> <ul> <li>device     (Union[int, torch.device, NoneType])     \u2013 defaults to <code>None</code> </li> </ul> load_state_dict <p>Copies parameters and buffers from :attr:<code>state_dict</code> into this module and its descendants. If :attr:<code>strict</code> is <code>True</code>, then the keys of :attr:<code>state_dict</code> must exactly match the keys returned by this module's :meth:<code>~torch.nn.Module.state_dict</code> function.</p> <p>Args:     state_dict (dict): a dict containing parameters and         persistent buffers.     strict (bool, optional): whether to strictly enforce that the keys         in :attr:<code>state_dict</code> match the keys returned by this module's         :meth:<code>~torch.nn.Module.state_dict</code> function. Default: <code>True</code>  Returns:     <code>NamedTuple</code> with <code>missing_keys</code> and <code>unexpected_keys</code> fields:         * missing_keys is a list of str containing the missing keys         * unexpected_keys is a list of str containing the unexpected keys  Note:     If a parameter or buffer is registered as <code>None</code> and its corresponding key     exists in :attr:<code>state_dict</code>, :meth:<code>load_state_dict</code> will raise a     <code>RuntimeError</code>.</p> <p>Parameters</p> <ul> <li>state_dict     (Mapping[str, Any])    </li> <li>strict     (bool)     \u2013 defaults to <code>True</code> </li> </ul> modules <p>Returns an iterator over all modules in the network.</p> <p>Yields:     Module: a module in the network  Note:     Duplicate modules are returned only once. In the following     example, <code>l</code> will be returned only once.  Example::      &gt;&gt;&gt; l = nn.Linear(2, 2)     &gt;&gt;&gt; net = nn.Sequential(l, l)     &gt;&gt;&gt; for idx, m in enumerate(net.modules()):     ...     print(idx, '-&gt;', m)      0 -&gt; Sequential(       (0): Linear(in_features=2, out_features=2, bias=True)       (1): Linear(in_features=2, out_features=2, bias=True)     )     1 -&gt; Linear(in_features=2, out_features=2, bias=True)</p> named_buffers <p>Returns an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself.</p> <p>Args:     prefix (str): prefix to prepend to all buffer names.     recurse (bool, optional): if True, then yields buffers of this module         and all submodules. Otherwise, yields only buffers that         are direct members of this module. Defaults to True.     remove_duplicate (bool, optional): whether to remove the duplicated buffers in the result. Defaults to True.  Yields:     (str, torch.Tensor): Tuple containing the name and buffer  Example::      &gt;&gt;&gt; # xdoctest: +SKIP(\"undefined vars\")     &gt;&gt;&gt; for name, buf in self.named_buffers():     &gt;&gt;&gt;     if name in ['running_var']:     &gt;&gt;&gt;         print(buf.size())</p> <p>Parameters</p> <ul> <li>prefix     (str)     \u2013 defaults to ``    </li> <li>recurse     (bool)     \u2013 defaults to <code>True</code> </li> <li>remove_duplicate     (bool)     \u2013 defaults to <code>True</code> </li> </ul> named_children <p>Returns an iterator over immediate children modules, yielding both the name of the module as well as the module itself.</p> <p>Yields:     (str, Module): Tuple containing a name and child module  Example::      &gt;&gt;&gt; # xdoctest: +SKIP(\"undefined vars\")     &gt;&gt;&gt; for name, module in model.named_children():     &gt;&gt;&gt;     if name in ['conv4', 'conv5']:     &gt;&gt;&gt;         print(module)</p> named_modules <p>Returns an iterator over all modules in the network, yielding both the name of the module as well as the module itself.</p> <p>Args:     memo: a memo to store the set of modules already added to the result     prefix: a prefix that will be added to the name of the module     remove_duplicate: whether to remove the duplicated module instances in the result         or not  Yields:     (str, Module): Tuple of name and module  Note:     Duplicate modules are returned only once. In the following     example, <code>l</code> will be returned only once.  Example::      &gt;&gt;&gt; l = nn.Linear(2, 2)     &gt;&gt;&gt; net = nn.Sequential(l, l)     &gt;&gt;&gt; for idx, m in enumerate(net.named_modules()):     ...     print(idx, '-&gt;', m)      0 -&gt; ('', Sequential(       (0): Linear(in_features=2, out_features=2, bias=True)       (1): Linear(in_features=2, out_features=2, bias=True)     ))     1 -&gt; ('0', Linear(in_features=2, out_features=2, bias=True))</p> <p>Parameters</p> <ul> <li>memo     (Optional[Set[ForwardRef('Module')]])     \u2013 defaults to <code>None</code> </li> <li>prefix     (str)     \u2013 defaults to ``    </li> <li>remove_duplicate     (bool)     \u2013 defaults to <code>True</code> </li> </ul> named_parameters <p>Returns an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself.</p> <p>Args:     prefix (str): prefix to prepend to all parameter names.     recurse (bool): if True, then yields parameters of this module         and all submodules. Otherwise, yields only parameters that         are direct members of this module.     remove_duplicate (bool, optional): whether to remove the duplicated         parameters in the result. Defaults to True.  Yields:     (str, Parameter): Tuple containing the name and parameter  Example::      &gt;&gt;&gt; # xdoctest: +SKIP(\"undefined vars\")     &gt;&gt;&gt; for name, param in self.named_parameters():     &gt;&gt;&gt;     if name in ['bias']:     &gt;&gt;&gt;         print(param.size())</p> <p>Parameters</p> <ul> <li>prefix     (str)     \u2013 defaults to ``    </li> <li>recurse     (bool)     \u2013 defaults to <code>True</code> </li> <li>remove_duplicate     (bool)     \u2013 defaults to <code>True</code> </li> </ul> parameters <p>Returns an iterator over module parameters.</p> <p>This is typically passed to an optimizer.  Args:     recurse (bool): if True, then yields parameters of this module         and all submodules. Otherwise, yields only parameters that         are direct members of this module.  Yields:     Parameter: module parameter  Example::      &gt;&gt;&gt; # xdoctest: +SKIP(\"undefined vars\")     &gt;&gt;&gt; for param in model.parameters():     &gt;&gt;&gt;     print(type(param), param.size())      (20L,)      (20L, 1L, 5L, 5L) <p>Parameters</p> <ul> <li>recurse     (bool)     \u2013 defaults to <code>True</code> </li> </ul> register_backward_hook <p>Registers a backward hook on the module.</p> <p>This function is deprecated in favor of :meth:<code>~torch.nn.Module.register_full_backward_hook</code> and the behavior of this function will change in future versions.  Returns:     :class:<code>torch.utils.hooks.RemovableHandle</code>:         a handle that can be used to remove the added hook by calling         <code>handle.remove()</code></p> <p>Parameters</p> <ul> <li>hook     (Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, Tuple[torch.Tensor, ...], torch.Tensor]])    </li> </ul> register_buffer <p>Adds a buffer to the module.</p> <p>This is typically used to register a buffer that should not to be considered a model parameter. For example, BatchNorm's <code>running_mean</code> is not a parameter, but is part of the module's state. Buffers, by default, are persistent and will be saved alongside parameters. This behavior can be changed by setting :attr:<code>persistent</code> to <code>False</code>. The only difference between a persistent buffer and a non-persistent buffer is that the latter will not be a part of this module's :attr:<code>state_dict</code>.  Buffers can be accessed as attributes using given names.  Args:     name (str): name of the buffer. The buffer can be accessed         from this module using the given name     tensor (Tensor or None): buffer to be registered. If <code>None</code>, then operations         that run on buffers, such as :attr:<code>cuda</code>, are ignored. If <code>None</code>,         the buffer is not included in the module's :attr:<code>state_dict</code>.     persistent (bool): whether the buffer is part of this module's         :attr:<code>state_dict</code>.  Example::      &gt;&gt;&gt; # xdoctest: +SKIP(\"undefined vars\")     &gt;&gt;&gt; self.register_buffer('running_mean', torch.zeros(num_features))</p> <p>Parameters</p> <ul> <li>name     (str)    </li> <li>tensor     (Optional[torch.Tensor])    </li> <li>persistent     (bool)     \u2013 defaults to <code>True</code> </li> </ul> register_forward_hook <p>Registers a forward hook on the module.</p> <p>The hook will be called every time after :func:<code>forward</code> has computed an output.  If <code>with_kwargs</code> is <code>False</code> or not specified, the input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the <code>forward</code>. The hook can modify the output. It can modify the input inplace but it will not have effect on forward since this is called after :func:<code>forward</code> is called. The hook should have the following signature::      hook(module, args, output) -&gt; None or modified output  If <code>with_kwargs</code> is <code>True</code>, the forward hook will be passed the <code>kwargs</code> given to the forward function and be expected to return the output possibly modified. The hook should have the following signature::      hook(module, args, kwargs, output) -&gt; None or modified output  Args:     hook (Callable): The user defined hook to be registered.     prepend (bool): If <code>True</code>, the provided <code>hook</code> will be fired         before all existing <code>forward</code> hooks on this         :class:<code>torch.nn.modules.Module</code>. Otherwise, the provided         <code>hook</code> will be fired after all existing <code>forward</code> hooks on         this :class:<code>torch.nn.modules.Module</code>. Note that global         <code>forward</code> hooks registered with         :func:<code>register_module_forward_hook</code> will fire before all hooks         registered by this method.         Default: <code>False</code>     with_kwargs (bool): If <code>True</code>, the <code>hook</code> will be passed the         kwargs given to the forward function.         Default: <code>False</code>  Returns:     :class:<code>torch.utils.hooks.RemovableHandle</code>:         a handle that can be used to remove the added hook by calling         <code>handle.remove()</code></p> <p>Parameters</p> <ul> <li>hook     (Union[Callable[[~T, Tuple[Any, ...], Any], Optional[Any]], Callable[[~T, Tuple[Any, ...], Dict[str, Any], Any], Optional[Any]]])    </li> <li>prepend     (bool)     \u2013 defaults to <code>False</code> </li> <li>with_kwargs     (bool)     \u2013 defaults to <code>False</code> </li> </ul> register_forward_pre_hook <p>Registers a forward pre-hook on the module.</p> <p>The hook will be called every time before :func:<code>forward</code> is invoked.  If <code>with_kwargs</code> is false or not specified, the input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the <code>forward</code>. The hook can modify the input. User can either return a tuple or a single modified value in the hook. We will wrap the value into a tuple if a single value is returned (unless that value is already a tuple). The hook should have the following signature::      hook(module, args) -&gt; None or modified input  If <code>with_kwargs</code> is true, the forward pre-hook will be passed the kwargs given to the forward function. And if the hook modifies the input, both the args and kwargs should be returned. The hook should have the following signature::      hook(module, args, kwargs) -&gt; None or a tuple of modified input and kwargs  Args:     hook (Callable): The user defined hook to be registered.     prepend (bool): If true, the provided <code>hook</code> will be fired before         all existing <code>forward_pre</code> hooks on this         :class:<code>torch.nn.modules.Module</code>. Otherwise, the provided         <code>hook</code> will be fired after all existing <code>forward_pre</code> hooks         on this :class:<code>torch.nn.modules.Module</code>. Note that global         <code>forward_pre</code> hooks registered with         :func:<code>register_module_forward_pre_hook</code> will fire before all         hooks registered by this method.         Default: <code>False</code>     with_kwargs (bool): If true, the <code>hook</code> will be passed the kwargs         given to the forward function.         Default: <code>False</code>  Returns:     :class:<code>torch.utils.hooks.RemovableHandle</code>:         a handle that can be used to remove the added hook by calling         <code>handle.remove()</code></p> <p>Parameters</p> <ul> <li>hook     (Union[Callable[[~T, Tuple[Any, ...]], Optional[Any]], Callable[[~T, Tuple[Any, ...], Dict[str, Any]], Optional[Tuple[Any, Dict[str, Any]]]]])    </li> <li>prepend     (bool)     \u2013 defaults to <code>False</code> </li> <li>with_kwargs     (bool)     \u2013 defaults to <code>False</code> </li> </ul> register_full_backward_hook <p>Registers a backward hook on the module.</p> <p>The hook will be called every time the gradients with respect to a module are computed, i.e. the hook will execute if and only if the gradients with respect to module outputs are computed. The hook should have the following signature::      hook(module, grad_input, grad_output) -&gt; tuple(Tensor) or None  The :attr:<code>grad_input</code> and :attr:<code>grad_output</code> are tuples that contain the gradients with respect to the inputs and outputs respectively. The hook should not modify its arguments, but it can optionally return a new gradient with respect to the input that will be used in place of :attr:<code>grad_input</code> in subsequent computations. :attr:<code>grad_input</code> will only correspond to the inputs given as positional arguments and all kwarg arguments are ignored. Entries in :attr:<code>grad_input</code> and :attr:<code>grad_output</code> will be <code>None</code> for all non-Tensor arguments.  For technical reasons, when this hook is applied to a Module, its forward function will receive a view of each Tensor passed to the Module. Similarly the caller will receive a view of each Tensor returned by the Module's forward function.  .. warning ::     Modifying inputs or outputs inplace is not allowed when using backward hooks and     will raise an error.  Args:     hook (Callable): The user-defined hook to be registered.     prepend (bool): If true, the provided <code>hook</code> will be fired before         all existing <code>backward</code> hooks on this         :class:<code>torch.nn.modules.Module</code>. Otherwise, the provided         <code>hook</code> will be fired after all existing <code>backward</code> hooks on         this :class:<code>torch.nn.modules.Module</code>. Note that global         <code>backward</code> hooks registered with         :func:<code>register_module_full_backward_hook</code> will fire before         all hooks registered by this method.  Returns:     :class:<code>torch.utils.hooks.RemovableHandle</code>:         a handle that can be used to remove the added hook by calling         <code>handle.remove()</code></p> <p>Parameters</p> <ul> <li>hook     (Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, Tuple[torch.Tensor, ...], torch.Tensor]])    </li> <li>prepend     (bool)     \u2013 defaults to <code>False</code> </li> </ul> register_full_backward_pre_hook <p>Registers a backward pre-hook on the module.</p> <p>The hook will be called every time the gradients for the module are computed. The hook should have the following signature::      hook(module, grad_output) -&gt; Tensor or None  The :attr:<code>grad_output</code> is a tuple. The hook should not modify its arguments, but it can optionally return a new gradient with respect to the output that will be used in place of :attr:<code>grad_output</code> in subsequent computations. Entries in :attr:<code>grad_output</code> will be <code>None</code> for all non-Tensor arguments.  For technical reasons, when this hook is applied to a Module, its forward function will receive a view of each Tensor passed to the Module. Similarly the caller will receive a view of each Tensor returned by the Module's forward function.  .. warning ::     Modifying inputs inplace is not allowed when using backward hooks and     will raise an error.  Args:     hook (Callable): The user-defined hook to be registered.     prepend (bool): If true, the provided <code>hook</code> will be fired before         all existing <code>backward_pre</code> hooks on this         :class:<code>torch.nn.modules.Module</code>. Otherwise, the provided         <code>hook</code> will be fired after all existing <code>backward_pre</code> hooks         on this :class:<code>torch.nn.modules.Module</code>. Note that global         <code>backward_pre</code> hooks registered with         :func:<code>register_module_full_backward_pre_hook</code> will fire before         all hooks registered by this method.  Returns:     :class:<code>torch.utils.hooks.RemovableHandle</code>:         a handle that can be used to remove the added hook by calling         <code>handle.remove()</code></p> <p>Parameters</p> <ul> <li>hook     (Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, Tuple[torch.Tensor, ...], torch.Tensor]])    </li> <li>prepend     (bool)     \u2013 defaults to <code>False</code> </li> </ul> register_load_state_dict_post_hook <p>Registers a post hook to be run after module's <code>load_state_dict</code> is called.</p> <p>It should have the following signature::     hook(module, incompatible_keys) -&gt; None  The <code>module</code> argument is the current module that this hook is registered on, and the <code>incompatible_keys</code> argument is a <code>NamedTuple</code> consisting of attributes <code>missing_keys</code> and <code>unexpected_keys</code>. <code>missing_keys</code> is a <code>list</code> of <code>str</code> containing the missing keys and <code>unexpected_keys</code> is a <code>list</code> of <code>str</code> containing the unexpected keys.  The given incompatible_keys can be modified inplace if needed.  Note that the checks performed when calling :func:<code>load_state_dict</code> with <code>strict=True</code> are affected by modifications the hook makes to <code>missing_keys</code> or <code>unexpected_keys</code>, as expected. Additions to either set of keys will result in an error being thrown when <code>strict=True</code>, and clearing out both missing and unexpected keys will avoid an error.  Returns:     :class:<code>torch.utils.hooks.RemovableHandle</code>:         a handle that can be used to remove the added hook by calling         <code>handle.remove()</code></p> <p>Parameters</p> <ul> <li>hook </li> </ul> register_module <p>Alias for :func:<code>add_module</code>.</p> <p>Parameters</p> <ul> <li>name     (str)    </li> <li>module     (Optional[ForwardRef('Module')])    </li> </ul> register_parameter <p>Adds a parameter to the module.</p> <p>The parameter can be accessed as an attribute using given name.  Args:     name (str): name of the parameter. The parameter can be accessed         from this module using the given name     param (Parameter or None): parameter to be added to the module. If         <code>None</code>, then operations that run on parameters, such as :attr:<code>cuda</code>,         are ignored. If <code>None</code>, the parameter is not included in the         module's :attr:<code>state_dict</code>.</p> <p>Parameters</p> <ul> <li>name     (str)    </li> <li>param     (Optional[torch.nn.parameter.Parameter])    </li> </ul> register_state_dict_pre_hook <p>These hooks will be called with arguments: <code>self</code>, <code>prefix</code>, and <code>keep_vars</code> before calling <code>state_dict</code> on <code>self</code>. The registered hooks can be used to perform pre-processing before the <code>state_dict</code> call is made.</p> <p>Parameters</p> <ul> <li>hook </li> </ul> requires_grad_ <p>Change if autograd should record operations on parameters in this module.</p> <p>This method sets the parameters' :attr:<code>requires_grad</code> attributes in-place.  This method is helpful for freezing part of the module for finetuning or training parts of a model individually (e.g., GAN training).  See :ref:<code>locally-disable-grad-doc</code> for a comparison between <code>.requires_grad_()</code> and several similar mechanisms that may be confused with it.  Args:     requires_grad (bool): whether autograd should record operations on                           parameters in this module. Default: <code>True</code>.  Returns:     Module: self</p> <p>Parameters</p> <ul> <li>requires_grad     (bool)     \u2013 defaults to <code>True</code> </li> </ul> save_pretrained <p>Save model the model.</p> <p>Parameters</p> <ul> <li>path     (str)    </li> </ul> scores <p>Compute similarity scores between queries and documents.</p> <p>Parameters</p> <ul> <li>queries     (list[str])    </li> <li>documents     (list[str])    </li> <li>batch_size     (int)     \u2013 defaults to <code>32</code> </li> <li>tqdm_bar     (bool)     \u2013 defaults to <code>True</code> </li> <li>kwargs      Additional parameters to the pre-trained model.</li> </ul> set_extra_state <p>This function is called from :func:<code>load_state_dict</code> to handle any extra state found within the <code>state_dict</code>. Implement this function and a corresponding :func:<code>get_extra_state</code> for your module if you need to store extra state within its <code>state_dict</code>.</p> <p>Args:     state (dict): Extra state from the <code>state_dict</code></p> <p>Parameters</p> <ul> <li>state     (Any)    </li> </ul> share_memory <p>See :meth:<code>torch.Tensor.share_memory_</code></p> state_dict <p>Returns a dictionary containing references to the whole state of the module.</p> <p>Both parameters and persistent buffers (e.g. running averages) are included. Keys are corresponding parameter and buffer names. Parameters and buffers set to <code>None</code> are not included.  .. note::     The returned object is a shallow copy. It contains references     to the module's parameters and buffers.  .. warning::     Currently <code>state_dict()</code> also accepts positional arguments for     <code>destination</code>, <code>prefix</code> and <code>keep_vars</code> in order. However,     this is being deprecated and keyword arguments will be enforced in     future releases.  .. warning::     Please avoid the use of argument <code>destination</code> as it is not     designed for end-users.  Args:     destination (dict, optional): If provided, the state of module will         be updated into the dict and the same object is returned.         Otherwise, an <code>OrderedDict</code> will be created and returned.         Default: <code>None</code>.     prefix (str, optional): a prefix added to parameter and buffer         names to compose the keys in state_dict. Default: <code>''</code>.     keep_vars (bool, optional): by default the :class:<code>~torch.Tensor</code> s         returned in the state dict are detached from autograd. If it's         set to <code>True</code>, detaching will not be performed.         Default: <code>False</code>.  Returns:     dict:         a dictionary containing a whole state of the module  Example::      &gt;&gt;&gt; # xdoctest: +SKIP(\"undefined vars\")     &gt;&gt;&gt; module.state_dict().keys()     ['bias', 'weight']</p> <p>Parameters</p> <ul> <li>args </li> <li>destination     \u2013 defaults to <code>None</code> </li> <li>prefix     \u2013 defaults to ``    </li> <li>keep_vars     \u2013 defaults to <code>False</code> </li> </ul> to <p>Moves and/or casts the parameters and buffers.</p> <p>This can be called as  .. function:: to(device=None, dtype=None, non_blocking=False)    :noindex:  .. function:: to(dtype, non_blocking=False)    :noindex:  .. function:: to(tensor, non_blocking=False)    :noindex:  .. function:: to(memory_format=torch.channels_last)    :noindex:  Its signature is similar to :meth:<code>torch.Tensor.to</code>, but only accepts floating point or complex :attr:<code>dtype</code>\\ s. In addition, this method will only cast the floating point or complex parameters and buffers to :attr:<code>dtype</code> (if given). The integral parameters and buffers will be moved :attr:<code>device</code>, if that is given, but with dtypes unchanged. When :attr:<code>non_blocking</code> is set, it tries to convert/move asynchronously with respect to the host if possible, e.g., moving CPU Tensors with pinned memory to CUDA devices.  See below for examples.  .. note::     This method modifies the module in-place.  Args:     device (:class:<code>torch.device</code>): the desired device of the parameters         and buffers in this module     dtype (:class:<code>torch.dtype</code>): the desired floating point or complex dtype of         the parameters and buffers in this module     tensor (torch.Tensor): Tensor whose dtype and device are the desired         dtype and device for all parameters and buffers in this module     memory_format (:class:<code>torch.memory_format</code>): the desired memory         format for 4D parameters and buffers in this module (keyword         only argument)  Returns:     Module: self  Examples::      &gt;&gt;&gt; # xdoctest: +IGNORE_WANT(\"non-deterministic\")     &gt;&gt;&gt; linear = nn.Linear(2, 2)     &gt;&gt;&gt; linear.weight     Parameter containing:     tensor([[ 0.1913, -0.3420],             [-0.5113, -0.2325]])     &gt;&gt;&gt; linear.to(torch.double)     Linear(in_features=2, out_features=2, bias=True)     &gt;&gt;&gt; linear.weight     Parameter containing:     tensor([[ 0.1913, -0.3420],             [-0.5113, -0.2325]], dtype=torch.float64)     &gt;&gt;&gt; # xdoctest: +REQUIRES(env:TORCH_DOCTEST_CUDA1)     &gt;&gt;&gt; gpu1 = torch.device(\"cuda:1\")     &gt;&gt;&gt; linear.to(gpu1, dtype=torch.half, non_blocking=True)     Linear(in_features=2, out_features=2, bias=True)     &gt;&gt;&gt; linear.weight     Parameter containing:     tensor([[ 0.1914, -0.3420],             [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1')     &gt;&gt;&gt; cpu = torch.device(\"cpu\")     &gt;&gt;&gt; linear.to(cpu)     Linear(in_features=2, out_features=2, bias=True)     &gt;&gt;&gt; linear.weight     Parameter containing:     tensor([[ 0.1914, -0.3420],             [-0.5112, -0.2324]], dtype=torch.float16)      &gt;&gt;&gt; linear = nn.Linear(2, 2, bias=None).to(torch.cdouble)     &gt;&gt;&gt; linear.weight     Parameter containing:     tensor([[ 0.3741+0.j,  0.2382+0.j],             [ 0.5593+0.j, -0.4443+0.j]], dtype=torch.complex128)     &gt;&gt;&gt; linear(torch.ones(3, 2, dtype=torch.cdouble))     tensor([[0.6122+0.j, 0.1150+0.j],             [0.6122+0.j, 0.1150+0.j],             [0.6122+0.j, 0.1150+0.j]], dtype=torch.complex128)</p> <p>Parameters</p> <ul> <li>args </li> <li>kwargs      Additional parameters to the pre-trained model.</li> </ul> to_empty <p>Moves the parameters and buffers to the specified device without copying storage.</p> <p>Args:     device (:class:<code>torch.device</code>): The desired device of the parameters         and buffers in this module.  Returns:     Module: self</p> <p>Parameters</p> <ul> <li>device     (Union[str, torch.device])    </li> </ul> train <p>Sets the module in training mode.</p> <p>This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class:<code>Dropout</code>, :class:<code>BatchNorm</code>, etc.  Args:     mode (bool): whether to set training mode (<code>True</code>) or evaluation                  mode (<code>False</code>). Default: <code>True</code>.  Returns:     Module: self</p> <p>Parameters</p> <ul> <li>mode     (bool)     \u2013 defaults to <code>True</code> </li> </ul> type <p>Casts all parameters and buffers to :attr:<code>dst_type</code>.</p> <p>.. note::     This method modifies the module in-place.  Args:     dst_type (type or string): the desired type  Returns:     Module: self</p> <p>Parameters</p> <ul> <li>dst_type     (Union[torch.dtype, str])    </li> </ul> xpu <p>Moves all model parameters and buffers to the XPU.</p> <p>This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on XPU while being optimized.  .. note::     This method modifies the module in-place.  Arguments:     device (int, optional): if specified, all parameters will be         copied to that device  Returns:     Module: self</p> <p>Parameters</p> <ul> <li>device     (Union[int, torch.device, NoneType])     \u2013 defaults to <code>None</code> </li> </ul> zero_grad <p>Sets gradients of all model parameters to zero. See similar function under :class:<code>torch.optim.Optimizer</code> for more context.</p> <p>Args:     set_to_none (bool): instead of setting to zero, set the grads to None.         See :meth:<code>torch.optim.Optimizer.zero_grad</code> for details.</p> <p>Parameters</p> <ul> <li>set_to_none     (bool)     \u2013 defaults to <code>True</code> </li> </ul>"},{"location":"api/models/SparseEmbed/#references","title":"References","text":"<ol> <li>SparseEmbed: Learning Sparse Lexical Representations with Contextual Embeddings for Retrieval</li> </ol>"},{"location":"api/models/Splade/","title":"Splade","text":"<p>SpladeV1 model.</p>"},{"location":"api/models/Splade/#parameters","title":"Parameters","text":"<ul> <li> <p>model_name_or_path (str) \u2013 defaults to <code>None</code></p> </li> <li> <p>device (str) \u2013 defaults to <code>None</code></p> </li> <li> <p>max_length_query (int) \u2013 defaults to <code>128</code></p> </li> <li> <p>max_length_document (int) \u2013 defaults to <code>256</code></p> </li> <li> <p>extra_files_to_load (list[str]) \u2013 defaults to <code>['metadata.json']</code></p> </li> <li> <p>kwargs</p> <p>Additional parameters to the SentenceTransformer model.</p> </li> </ul>"},{"location":"api/models/Splade/#examples","title":"Examples","text":"<pre><code>&gt;&gt;&gt; from neural_cherche import models\n&gt;&gt;&gt; import torch\n&gt;&gt;&gt; _ = torch.manual_seed(42)\n&gt;&gt;&gt; model = models.Splade(\n...     model_name_or_path=\"raphaelsty/neural-cherche-sparse-embed\",\n...     device=\"mps\",\n... )\n&gt;&gt;&gt; queries_activations = model.encode(\n...     [\"Sports\", \"Music\"],\n... )\n&gt;&gt;&gt; documents_activations = model.encode(\n...    [\"Music is great.\", \"Sports is great.\"],\n...    query_mode=False,\n... )\n&gt;&gt;&gt; queries_activations[\"sparse_activations\"].shape\ntorch.Size([2, 30522])\n&gt;&gt;&gt; model.scores(\n...     queries=[\"Sports\", \"Music\"],\n...     documents=[\"Sports is great.\", \"Music is great.\"],\n...     batch_size=1\n... )\ntensor([318.1384, 271.8006], device='mps:0')\n&gt;&gt;&gt; _ = model.save_pretrained(\"checkpoint\")\n&gt;&gt;&gt; model = models.Splade(\n...     model_name_or_path=\"checkpoint\",\n...     device=\"mps\",\n... )\n&gt;&gt;&gt; model.scores(\n...     queries=[\"Sports\", \"Music\"],\n...     documents=[\"Sports is great.\", \"Music is great.\"],\n...     batch_size=1\n... )\ntensor([318.1384, 271.8006], device='mps:0')\n</code></pre>"},{"location":"api/models/Splade/#methods","title":"Methods","text":"call <p>Call self as a function.</p> <p>Parameters</p> <ul> <li>args </li> <li>kwargs      Additional parameters to the SentenceTransformer model.</li> </ul> add_module <p>Adds a child module to the current module.</p> <p>The module can be accessed as an attribute using the given name.  Args:     name (str): name of the child module. The child module can be         accessed from this module using the given name     module (Module): child module to be added to the module.</p> <p>Parameters</p> <ul> <li>name     (str)    </li> <li>module     (Optional[ForwardRef('Module')])    </li> </ul> apply <p>Applies <code>fn</code> recursively to every submodule (as returned by <code>.children()</code>) as well as self. Typical use includes initializing the parameters of a model (see also :ref:<code>nn-init-doc</code>).</p> <p>Args:     fn (:class:<code>Module</code> -&gt; None): function to be applied to each submodule  Returns:     Module: self  Example::      &gt;&gt;&gt; @torch.no_grad()     &gt;&gt;&gt; def init_weights(m):     &gt;&gt;&gt;     print(m)     &gt;&gt;&gt;     if type(m) == nn.Linear:     &gt;&gt;&gt;         m.weight.fill_(1.0)     &gt;&gt;&gt;         print(m.weight)     &gt;&gt;&gt; net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2))     &gt;&gt;&gt; net.apply(init_weights)     Linear(in_features=2, out_features=2, bias=True)     Parameter containing:     tensor([[1., 1.],             [1., 1.]], requires_grad=True)     Linear(in_features=2, out_features=2, bias=True)     Parameter containing:     tensor([[1., 1.],             [1., 1.]], requires_grad=True)     Sequential(       (0): Linear(in_features=2, out_features=2, bias=True)       (1): Linear(in_features=2, out_features=2, bias=True)     )</p> <p>Parameters</p> <ul> <li>fn     (Callable[[ForwardRef('Module')], NoneType])    </li> </ul> bfloat16 <p>Casts all floating point parameters and buffers to <code>bfloat16</code> datatype.</p> <p>.. note::     This method modifies the module in-place.  Returns:     Module: self</p> buffers <p>Returns an iterator over module buffers.</p> <p>Args:     recurse (bool): if True, then yields buffers of this module         and all submodules. Otherwise, yields only buffers that         are direct members of this module.  Yields:     torch.Tensor: module buffer  Example::      &gt;&gt;&gt; # xdoctest: +SKIP(\"undefined vars\")     &gt;&gt;&gt; for buf in model.buffers():     &gt;&gt;&gt;     print(type(buf), buf.size())      (20L,)      (20L, 1L, 5L, 5L) <p>Parameters</p> <ul> <li>recurse     (bool)     \u2013 defaults to <code>True</code> </li> </ul> children <p>Returns an iterator over immediate children modules.</p> <p>Yields:     Module: a child module</p> cpu <p>Moves all model parameters and buffers to the CPU.</p> <p>.. note::     This method modifies the module in-place.  Returns:     Module: self</p> cuda <p>Moves all model parameters and buffers to the GPU.</p> <p>This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on GPU while being optimized.  .. note::     This method modifies the module in-place.  Args:     device (int, optional): if specified, all parameters will be         copied to that device  Returns:     Module: self</p> <p>Parameters</p> <ul> <li>device     (Union[int, torch.device, NoneType])     \u2013 defaults to <code>None</code> </li> </ul> decode <p>Decode activated tokens ids where activated value &gt; 0.</p> <p>Parameters</p> <ul> <li>sparse_activations     (torch.Tensor)    </li> <li>clean_up_tokenization_spaces     (bool)     \u2013 defaults to <code>False</code> </li> <li>skip_special_tokens     (bool)     \u2013 defaults to <code>True</code> </li> <li>k_tokens     (int)     \u2013 defaults to <code>96</code> </li> </ul> double <p>Casts all floating point parameters and buffers to <code>double</code> datatype.</p> <p>.. note::     This method modifies the module in-place.  Returns:     Module: self</p> encode <p>Encode documents</p> <p>Parameters</p> <ul> <li>texts     (list[str])    </li> <li>query_mode     (bool)     \u2013 defaults to <code>True</code> </li> <li>kwargs      Additional parameters to the SentenceTransformer model.</li> </ul> eval <p>Sets the module in evaluation mode.</p> <p>This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class:<code>Dropout</code>, :class:<code>BatchNorm</code>, etc.  This is equivalent with :meth:<code>self.train(False) &lt;torch.nn.Module.train&gt;</code>.  See :ref:<code>locally-disable-grad-doc</code> for a comparison between <code>.eval()</code> and several similar mechanisms that may be confused with it.  Returns:     Module: self</p> extra_repr <p>Set the extra representation of the module</p> <p>To print customized extra information, you should re-implement this method in your own modules. Both single-line and multi-line strings are acceptable.</p> float <p>Casts all floating point parameters and buffers to <code>float</code> datatype.</p> <p>.. note::     This method modifies the module in-place.  Returns:     Module: self</p> forward <p>Pytorch forward method.</p> <p>Parameters</p> <ul> <li>texts     (list[str])    </li> <li>query_mode     (bool)    </li> <li>kwargs      Additional parameters to the SentenceTransformer model.</li> </ul> get_buffer <p>Returns the buffer given by <code>target</code> if it exists, otherwise throws an error.</p> <p>See the docstring for <code>get_submodule</code> for a more detailed explanation of this method's functionality as well as how to correctly specify <code>target</code>.  Args:     target: The fully-qualified string name of the buffer         to look for. (See <code>get_submodule</code> for how to specify a         fully-qualified string.)  Returns:     torch.Tensor: The buffer referenced by <code>target</code>  Raises:     AttributeError: If the target string references an invalid         path or resolves to something that is not a         buffer</p> <p>Parameters</p> <ul> <li>target     (str)    </li> </ul> get_extra_state <p>Returns any extra state to include in the module's state_dict. Implement this and a corresponding :func:<code>set_extra_state</code> for your module if you need to store extra state. This function is called when building the module's <code>state_dict()</code>.</p> <p>Note that extra state should be picklable to ensure working serialization of the state_dict. We only provide provide backwards compatibility guarantees for serializing Tensors; other objects may break backwards compatibility if their serialized pickled form changes.  Returns:     object: Any extra state to store in the module's state_dict</p> get_parameter <p>Returns the parameter given by <code>target</code> if it exists, otherwise throws an error.</p> <p>See the docstring for <code>get_submodule</code> for a more detailed explanation of this method's functionality as well as how to correctly specify <code>target</code>.  Args:     target: The fully-qualified string name of the Parameter         to look for. (See <code>get_submodule</code> for how to specify a         fully-qualified string.)  Returns:     torch.nn.Parameter: The Parameter referenced by <code>target</code>  Raises:     AttributeError: If the target string references an invalid         path or resolves to something that is not an         <code>nn.Parameter</code></p> <p>Parameters</p> <ul> <li>target     (str)    </li> </ul> get_submodule <p>Returns the submodule given by <code>target</code> if it exists, otherwise throws an error.</p> <p>For example, let's say you have an <code>nn.Module</code> <code>A</code> that looks like this:  .. code-block:: text      A(         (net_b): Module(             (net_c): Module(                 (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2))             )             (linear): Linear(in_features=100, out_features=200, bias=True)         )     )  (The diagram shows an <code>nn.Module</code> <code>A</code>. <code>A</code> has a nested submodule <code>net_b</code>, which itself has two submodules <code>net_c</code> and <code>linear</code>. <code>net_c</code> then has a submodule <code>conv</code>.)  To check whether or not we have the <code>linear</code> submodule, we would call <code>get_submodule(\"net_b.linear\")</code>. To check whether we have the <code>conv</code> submodule, we would call <code>get_submodule(\"net_b.net_c.conv\")</code>.  The runtime of <code>get_submodule</code> is bounded by the degree of module nesting in <code>target</code>. A query against <code>named_modules</code> achieves the same result, but it is O(N) in the number of transitive modules. So, for a simple check to see if some submodule exists, <code>get_submodule</code> should always be used.  Args:     target: The fully-qualified string name of the submodule         to look for. (See above example for how to specify a         fully-qualified string.)  Returns:     torch.nn.Module: The submodule referenced by <code>target</code>  Raises:     AttributeError: If the target string references an invalid         path or resolves to something that is not an         <code>nn.Module</code></p> <p>Parameters</p> <ul> <li>target     (str)    </li> </ul> half <p>Casts all floating point parameters and buffers to <code>half</code> datatype.</p> <p>.. note::     This method modifies the module in-place.  Returns:     Module: self</p> ipu <p>Moves all model parameters and buffers to the IPU.</p> <p>This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on IPU while being optimized.  .. note::     This method modifies the module in-place.  Arguments:     device (int, optional): if specified, all parameters will be         copied to that device  Returns:     Module: self</p> <p>Parameters</p> <ul> <li>device     (Union[int, torch.device, NoneType])     \u2013 defaults to <code>None</code> </li> </ul> load_state_dict <p>Copies parameters and buffers from :attr:<code>state_dict</code> into this module and its descendants. If :attr:<code>strict</code> is <code>True</code>, then the keys of :attr:<code>state_dict</code> must exactly match the keys returned by this module's :meth:<code>~torch.nn.Module.state_dict</code> function.</p> <p>Args:     state_dict (dict): a dict containing parameters and         persistent buffers.     strict (bool, optional): whether to strictly enforce that the keys         in :attr:<code>state_dict</code> match the keys returned by this module's         :meth:<code>~torch.nn.Module.state_dict</code> function. Default: <code>True</code>  Returns:     <code>NamedTuple</code> with <code>missing_keys</code> and <code>unexpected_keys</code> fields:         * missing_keys is a list of str containing the missing keys         * unexpected_keys is a list of str containing the unexpected keys  Note:     If a parameter or buffer is registered as <code>None</code> and its corresponding key     exists in :attr:<code>state_dict</code>, :meth:<code>load_state_dict</code> will raise a     <code>RuntimeError</code>.</p> <p>Parameters</p> <ul> <li>state_dict     (Mapping[str, Any])    </li> <li>strict     (bool)     \u2013 defaults to <code>True</code> </li> </ul> modules <p>Returns an iterator over all modules in the network.</p> <p>Yields:     Module: a module in the network  Note:     Duplicate modules are returned only once. In the following     example, <code>l</code> will be returned only once.  Example::      &gt;&gt;&gt; l = nn.Linear(2, 2)     &gt;&gt;&gt; net = nn.Sequential(l, l)     &gt;&gt;&gt; for idx, m in enumerate(net.modules()):     ...     print(idx, '-&gt;', m)      0 -&gt; Sequential(       (0): Linear(in_features=2, out_features=2, bias=True)       (1): Linear(in_features=2, out_features=2, bias=True)     )     1 -&gt; Linear(in_features=2, out_features=2, bias=True)</p> named_buffers <p>Returns an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself.</p> <p>Args:     prefix (str): prefix to prepend to all buffer names.     recurse (bool, optional): if True, then yields buffers of this module         and all submodules. Otherwise, yields only buffers that         are direct members of this module. Defaults to True.     remove_duplicate (bool, optional): whether to remove the duplicated buffers in the result. Defaults to True.  Yields:     (str, torch.Tensor): Tuple containing the name and buffer  Example::      &gt;&gt;&gt; # xdoctest: +SKIP(\"undefined vars\")     &gt;&gt;&gt; for name, buf in self.named_buffers():     &gt;&gt;&gt;     if name in ['running_var']:     &gt;&gt;&gt;         print(buf.size())</p> <p>Parameters</p> <ul> <li>prefix     (str)     \u2013 defaults to ``    </li> <li>recurse     (bool)     \u2013 defaults to <code>True</code> </li> <li>remove_duplicate     (bool)     \u2013 defaults to <code>True</code> </li> </ul> named_children <p>Returns an iterator over immediate children modules, yielding both the name of the module as well as the module itself.</p> <p>Yields:     (str, Module): Tuple containing a name and child module  Example::      &gt;&gt;&gt; # xdoctest: +SKIP(\"undefined vars\")     &gt;&gt;&gt; for name, module in model.named_children():     &gt;&gt;&gt;     if name in ['conv4', 'conv5']:     &gt;&gt;&gt;         print(module)</p> named_modules <p>Returns an iterator over all modules in the network, yielding both the name of the module as well as the module itself.</p> <p>Args:     memo: a memo to store the set of modules already added to the result     prefix: a prefix that will be added to the name of the module     remove_duplicate: whether to remove the duplicated module instances in the result         or not  Yields:     (str, Module): Tuple of name and module  Note:     Duplicate modules are returned only once. In the following     example, <code>l</code> will be returned only once.  Example::      &gt;&gt;&gt; l = nn.Linear(2, 2)     &gt;&gt;&gt; net = nn.Sequential(l, l)     &gt;&gt;&gt; for idx, m in enumerate(net.named_modules()):     ...     print(idx, '-&gt;', m)      0 -&gt; ('', Sequential(       (0): Linear(in_features=2, out_features=2, bias=True)       (1): Linear(in_features=2, out_features=2, bias=True)     ))     1 -&gt; ('0', Linear(in_features=2, out_features=2, bias=True))</p> <p>Parameters</p> <ul> <li>memo     (Optional[Set[ForwardRef('Module')]])     \u2013 defaults to <code>None</code> </li> <li>prefix     (str)     \u2013 defaults to ``    </li> <li>remove_duplicate     (bool)     \u2013 defaults to <code>True</code> </li> </ul> named_parameters <p>Returns an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself.</p> <p>Args:     prefix (str): prefix to prepend to all parameter names.     recurse (bool): if True, then yields parameters of this module         and all submodules. Otherwise, yields only parameters that         are direct members of this module.     remove_duplicate (bool, optional): whether to remove the duplicated         parameters in the result. Defaults to True.  Yields:     (str, Parameter): Tuple containing the name and parameter  Example::      &gt;&gt;&gt; # xdoctest: +SKIP(\"undefined vars\")     &gt;&gt;&gt; for name, param in self.named_parameters():     &gt;&gt;&gt;     if name in ['bias']:     &gt;&gt;&gt;         print(param.size())</p> <p>Parameters</p> <ul> <li>prefix     (str)     \u2013 defaults to ``    </li> <li>recurse     (bool)     \u2013 defaults to <code>True</code> </li> <li>remove_duplicate     (bool)     \u2013 defaults to <code>True</code> </li> </ul> parameters <p>Returns an iterator over module parameters.</p> <p>This is typically passed to an optimizer.  Args:     recurse (bool): if True, then yields parameters of this module         and all submodules. Otherwise, yields only parameters that         are direct members of this module.  Yields:     Parameter: module parameter  Example::      &gt;&gt;&gt; # xdoctest: +SKIP(\"undefined vars\")     &gt;&gt;&gt; for param in model.parameters():     &gt;&gt;&gt;     print(type(param), param.size())      (20L,)      (20L, 1L, 5L, 5L) <p>Parameters</p> <ul> <li>recurse     (bool)     \u2013 defaults to <code>True</code> </li> </ul> register_backward_hook <p>Registers a backward hook on the module.</p> <p>This function is deprecated in favor of :meth:<code>~torch.nn.Module.register_full_backward_hook</code> and the behavior of this function will change in future versions.  Returns:     :class:<code>torch.utils.hooks.RemovableHandle</code>:         a handle that can be used to remove the added hook by calling         <code>handle.remove()</code></p> <p>Parameters</p> <ul> <li>hook     (Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, Tuple[torch.Tensor, ...], torch.Tensor]])    </li> </ul> register_buffer <p>Adds a buffer to the module.</p> <p>This is typically used to register a buffer that should not to be considered a model parameter. For example, BatchNorm's <code>running_mean</code> is not a parameter, but is part of the module's state. Buffers, by default, are persistent and will be saved alongside parameters. This behavior can be changed by setting :attr:<code>persistent</code> to <code>False</code>. The only difference between a persistent buffer and a non-persistent buffer is that the latter will not be a part of this module's :attr:<code>state_dict</code>.  Buffers can be accessed as attributes using given names.  Args:     name (str): name of the buffer. The buffer can be accessed         from this module using the given name     tensor (Tensor or None): buffer to be registered. If <code>None</code>, then operations         that run on buffers, such as :attr:<code>cuda</code>, are ignored. If <code>None</code>,         the buffer is not included in the module's :attr:<code>state_dict</code>.     persistent (bool): whether the buffer is part of this module's         :attr:<code>state_dict</code>.  Example::      &gt;&gt;&gt; # xdoctest: +SKIP(\"undefined vars\")     &gt;&gt;&gt; self.register_buffer('running_mean', torch.zeros(num_features))</p> <p>Parameters</p> <ul> <li>name     (str)    </li> <li>tensor     (Optional[torch.Tensor])    </li> <li>persistent     (bool)     \u2013 defaults to <code>True</code> </li> </ul> register_forward_hook <p>Registers a forward hook on the module.</p> <p>The hook will be called every time after :func:<code>forward</code> has computed an output.  If <code>with_kwargs</code> is <code>False</code> or not specified, the input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the <code>forward</code>. The hook can modify the output. It can modify the input inplace but it will not have effect on forward since this is called after :func:<code>forward</code> is called. The hook should have the following signature::      hook(module, args, output) -&gt; None or modified output  If <code>with_kwargs</code> is <code>True</code>, the forward hook will be passed the <code>kwargs</code> given to the forward function and be expected to return the output possibly modified. The hook should have the following signature::      hook(module, args, kwargs, output) -&gt; None or modified output  Args:     hook (Callable): The user defined hook to be registered.     prepend (bool): If <code>True</code>, the provided <code>hook</code> will be fired         before all existing <code>forward</code> hooks on this         :class:<code>torch.nn.modules.Module</code>. Otherwise, the provided         <code>hook</code> will be fired after all existing <code>forward</code> hooks on         this :class:<code>torch.nn.modules.Module</code>. Note that global         <code>forward</code> hooks registered with         :func:<code>register_module_forward_hook</code> will fire before all hooks         registered by this method.         Default: <code>False</code>     with_kwargs (bool): If <code>True</code>, the <code>hook</code> will be passed the         kwargs given to the forward function.         Default: <code>False</code>  Returns:     :class:<code>torch.utils.hooks.RemovableHandle</code>:         a handle that can be used to remove the added hook by calling         <code>handle.remove()</code></p> <p>Parameters</p> <ul> <li>hook     (Union[Callable[[~T, Tuple[Any, ...], Any], Optional[Any]], Callable[[~T, Tuple[Any, ...], Dict[str, Any], Any], Optional[Any]]])    </li> <li>prepend     (bool)     \u2013 defaults to <code>False</code> </li> <li>with_kwargs     (bool)     \u2013 defaults to <code>False</code> </li> </ul> register_forward_pre_hook <p>Registers a forward pre-hook on the module.</p> <p>The hook will be called every time before :func:<code>forward</code> is invoked.  If <code>with_kwargs</code> is false or not specified, the input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the <code>forward</code>. The hook can modify the input. User can either return a tuple or a single modified value in the hook. We will wrap the value into a tuple if a single value is returned (unless that value is already a tuple). The hook should have the following signature::      hook(module, args) -&gt; None or modified input  If <code>with_kwargs</code> is true, the forward pre-hook will be passed the kwargs given to the forward function. And if the hook modifies the input, both the args and kwargs should be returned. The hook should have the following signature::      hook(module, args, kwargs) -&gt; None or a tuple of modified input and kwargs  Args:     hook (Callable): The user defined hook to be registered.     prepend (bool): If true, the provided <code>hook</code> will be fired before         all existing <code>forward_pre</code> hooks on this         :class:<code>torch.nn.modules.Module</code>. Otherwise, the provided         <code>hook</code> will be fired after all existing <code>forward_pre</code> hooks         on this :class:<code>torch.nn.modules.Module</code>. Note that global         <code>forward_pre</code> hooks registered with         :func:<code>register_module_forward_pre_hook</code> will fire before all         hooks registered by this method.         Default: <code>False</code>     with_kwargs (bool): If true, the <code>hook</code> will be passed the kwargs         given to the forward function.         Default: <code>False</code>  Returns:     :class:<code>torch.utils.hooks.RemovableHandle</code>:         a handle that can be used to remove the added hook by calling         <code>handle.remove()</code></p> <p>Parameters</p> <ul> <li>hook     (Union[Callable[[~T, Tuple[Any, ...]], Optional[Any]], Callable[[~T, Tuple[Any, ...], Dict[str, Any]], Optional[Tuple[Any, Dict[str, Any]]]]])    </li> <li>prepend     (bool)     \u2013 defaults to <code>False</code> </li> <li>with_kwargs     (bool)     \u2013 defaults to <code>False</code> </li> </ul> register_full_backward_hook <p>Registers a backward hook on the module.</p> <p>The hook will be called every time the gradients with respect to a module are computed, i.e. the hook will execute if and only if the gradients with respect to module outputs are computed. The hook should have the following signature::      hook(module, grad_input, grad_output) -&gt; tuple(Tensor) or None  The :attr:<code>grad_input</code> and :attr:<code>grad_output</code> are tuples that contain the gradients with respect to the inputs and outputs respectively. The hook should not modify its arguments, but it can optionally return a new gradient with respect to the input that will be used in place of :attr:<code>grad_input</code> in subsequent computations. :attr:<code>grad_input</code> will only correspond to the inputs given as positional arguments and all kwarg arguments are ignored. Entries in :attr:<code>grad_input</code> and :attr:<code>grad_output</code> will be <code>None</code> for all non-Tensor arguments.  For technical reasons, when this hook is applied to a Module, its forward function will receive a view of each Tensor passed to the Module. Similarly the caller will receive a view of each Tensor returned by the Module's forward function.  .. warning ::     Modifying inputs or outputs inplace is not allowed when using backward hooks and     will raise an error.  Args:     hook (Callable): The user-defined hook to be registered.     prepend (bool): If true, the provided <code>hook</code> will be fired before         all existing <code>backward</code> hooks on this         :class:<code>torch.nn.modules.Module</code>. Otherwise, the provided         <code>hook</code> will be fired after all existing <code>backward</code> hooks on         this :class:<code>torch.nn.modules.Module</code>. Note that global         <code>backward</code> hooks registered with         :func:<code>register_module_full_backward_hook</code> will fire before         all hooks registered by this method.  Returns:     :class:<code>torch.utils.hooks.RemovableHandle</code>:         a handle that can be used to remove the added hook by calling         <code>handle.remove()</code></p> <p>Parameters</p> <ul> <li>hook     (Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, Tuple[torch.Tensor, ...], torch.Tensor]])    </li> <li>prepend     (bool)     \u2013 defaults to <code>False</code> </li> </ul> register_full_backward_pre_hook <p>Registers a backward pre-hook on the module.</p> <p>The hook will be called every time the gradients for the module are computed. The hook should have the following signature::      hook(module, grad_output) -&gt; Tensor or None  The :attr:<code>grad_output</code> is a tuple. The hook should not modify its arguments, but it can optionally return a new gradient with respect to the output that will be used in place of :attr:<code>grad_output</code> in subsequent computations. Entries in :attr:<code>grad_output</code> will be <code>None</code> for all non-Tensor arguments.  For technical reasons, when this hook is applied to a Module, its forward function will receive a view of each Tensor passed to the Module. Similarly the caller will receive a view of each Tensor returned by the Module's forward function.  .. warning ::     Modifying inputs inplace is not allowed when using backward hooks and     will raise an error.  Args:     hook (Callable): The user-defined hook to be registered.     prepend (bool): If true, the provided <code>hook</code> will be fired before         all existing <code>backward_pre</code> hooks on this         :class:<code>torch.nn.modules.Module</code>. Otherwise, the provided         <code>hook</code> will be fired after all existing <code>backward_pre</code> hooks         on this :class:<code>torch.nn.modules.Module</code>. Note that global         <code>backward_pre</code> hooks registered with         :func:<code>register_module_full_backward_pre_hook</code> will fire before         all hooks registered by this method.  Returns:     :class:<code>torch.utils.hooks.RemovableHandle</code>:         a handle that can be used to remove the added hook by calling         <code>handle.remove()</code></p> <p>Parameters</p> <ul> <li>hook     (Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, Tuple[torch.Tensor, ...], torch.Tensor]])    </li> <li>prepend     (bool)     \u2013 defaults to <code>False</code> </li> </ul> register_load_state_dict_post_hook <p>Registers a post hook to be run after module's <code>load_state_dict</code> is called.</p> <p>It should have the following signature::     hook(module, incompatible_keys) -&gt; None  The <code>module</code> argument is the current module that this hook is registered on, and the <code>incompatible_keys</code> argument is a <code>NamedTuple</code> consisting of attributes <code>missing_keys</code> and <code>unexpected_keys</code>. <code>missing_keys</code> is a <code>list</code> of <code>str</code> containing the missing keys and <code>unexpected_keys</code> is a <code>list</code> of <code>str</code> containing the unexpected keys.  The given incompatible_keys can be modified inplace if needed.  Note that the checks performed when calling :func:<code>load_state_dict</code> with <code>strict=True</code> are affected by modifications the hook makes to <code>missing_keys</code> or <code>unexpected_keys</code>, as expected. Additions to either set of keys will result in an error being thrown when <code>strict=True</code>, and clearing out both missing and unexpected keys will avoid an error.  Returns:     :class:<code>torch.utils.hooks.RemovableHandle</code>:         a handle that can be used to remove the added hook by calling         <code>handle.remove()</code></p> <p>Parameters</p> <ul> <li>hook </li> </ul> register_module <p>Alias for :func:<code>add_module</code>.</p> <p>Parameters</p> <ul> <li>name     (str)    </li> <li>module     (Optional[ForwardRef('Module')])    </li> </ul> register_parameter <p>Adds a parameter to the module.</p> <p>The parameter can be accessed as an attribute using given name.  Args:     name (str): name of the parameter. The parameter can be accessed         from this module using the given name     param (Parameter or None): parameter to be added to the module. If         <code>None</code>, then operations that run on parameters, such as :attr:<code>cuda</code>,         are ignored. If <code>None</code>, the parameter is not included in the         module's :attr:<code>state_dict</code>.</p> <p>Parameters</p> <ul> <li>name     (str)    </li> <li>param     (Optional[torch.nn.parameter.Parameter])    </li> </ul> register_state_dict_pre_hook <p>These hooks will be called with arguments: <code>self</code>, <code>prefix</code>, and <code>keep_vars</code> before calling <code>state_dict</code> on <code>self</code>. The registered hooks can be used to perform pre-processing before the <code>state_dict</code> call is made.</p> <p>Parameters</p> <ul> <li>hook </li> </ul> requires_grad_ <p>Change if autograd should record operations on parameters in this module.</p> <p>This method sets the parameters' :attr:<code>requires_grad</code> attributes in-place.  This method is helpful for freezing part of the module for finetuning or training parts of a model individually (e.g., GAN training).  See :ref:<code>locally-disable-grad-doc</code> for a comparison between <code>.requires_grad_()</code> and several similar mechanisms that may be confused with it.  Args:     requires_grad (bool): whether autograd should record operations on                           parameters in this module. Default: <code>True</code>.  Returns:     Module: self</p> <p>Parameters</p> <ul> <li>requires_grad     (bool)     \u2013 defaults to <code>True</code> </li> </ul> save_pretrained <p>Save model the model.</p> <p>Parameters</p> <ul> <li>path     (str)    </li> </ul> scores <p>Compute similarity scores between queries and documents.</p> <p>Parameters</p> <ul> <li>queries     (list[str])    </li> <li>documents     (list[str])    </li> <li>batch_size     (int)     \u2013 defaults to <code>32</code> </li> <li>tqdm_bar     (bool)     \u2013 defaults to <code>True</code> </li> <li>kwargs      Additional parameters to the SentenceTransformer model.</li> </ul> set_extra_state <p>This function is called from :func:<code>load_state_dict</code> to handle any extra state found within the <code>state_dict</code>. Implement this function and a corresponding :func:<code>get_extra_state</code> for your module if you need to store extra state within its <code>state_dict</code>.</p> <p>Args:     state (dict): Extra state from the <code>state_dict</code></p> <p>Parameters</p> <ul> <li>state     (Any)    </li> </ul> share_memory <p>See :meth:<code>torch.Tensor.share_memory_</code></p> state_dict <p>Returns a dictionary containing references to the whole state of the module.</p> <p>Both parameters and persistent buffers (e.g. running averages) are included. Keys are corresponding parameter and buffer names. Parameters and buffers set to <code>None</code> are not included.  .. note::     The returned object is a shallow copy. It contains references     to the module's parameters and buffers.  .. warning::     Currently <code>state_dict()</code> also accepts positional arguments for     <code>destination</code>, <code>prefix</code> and <code>keep_vars</code> in order. However,     this is being deprecated and keyword arguments will be enforced in     future releases.  .. warning::     Please avoid the use of argument <code>destination</code> as it is not     designed for end-users.  Args:     destination (dict, optional): If provided, the state of module will         be updated into the dict and the same object is returned.         Otherwise, an <code>OrderedDict</code> will be created and returned.         Default: <code>None</code>.     prefix (str, optional): a prefix added to parameter and buffer         names to compose the keys in state_dict. Default: <code>''</code>.     keep_vars (bool, optional): by default the :class:<code>~torch.Tensor</code> s         returned in the state dict are detached from autograd. If it's         set to <code>True</code>, detaching will not be performed.         Default: <code>False</code>.  Returns:     dict:         a dictionary containing a whole state of the module  Example::      &gt;&gt;&gt; # xdoctest: +SKIP(\"undefined vars\")     &gt;&gt;&gt; module.state_dict().keys()     ['bias', 'weight']</p> <p>Parameters</p> <ul> <li>args </li> <li>destination     \u2013 defaults to <code>None</code> </li> <li>prefix     \u2013 defaults to ``    </li> <li>keep_vars     \u2013 defaults to <code>False</code> </li> </ul> to <p>Moves and/or casts the parameters and buffers.</p> <p>This can be called as  .. function:: to(device=None, dtype=None, non_blocking=False)    :noindex:  .. function:: to(dtype, non_blocking=False)    :noindex:  .. function:: to(tensor, non_blocking=False)    :noindex:  .. function:: to(memory_format=torch.channels_last)    :noindex:  Its signature is similar to :meth:<code>torch.Tensor.to</code>, but only accepts floating point or complex :attr:<code>dtype</code>\\ s. In addition, this method will only cast the floating point or complex parameters and buffers to :attr:<code>dtype</code> (if given). The integral parameters and buffers will be moved :attr:<code>device</code>, if that is given, but with dtypes unchanged. When :attr:<code>non_blocking</code> is set, it tries to convert/move asynchronously with respect to the host if possible, e.g., moving CPU Tensors with pinned memory to CUDA devices.  See below for examples.  .. note::     This method modifies the module in-place.  Args:     device (:class:<code>torch.device</code>): the desired device of the parameters         and buffers in this module     dtype (:class:<code>torch.dtype</code>): the desired floating point or complex dtype of         the parameters and buffers in this module     tensor (torch.Tensor): Tensor whose dtype and device are the desired         dtype and device for all parameters and buffers in this module     memory_format (:class:<code>torch.memory_format</code>): the desired memory         format for 4D parameters and buffers in this module (keyword         only argument)  Returns:     Module: self  Examples::      &gt;&gt;&gt; # xdoctest: +IGNORE_WANT(\"non-deterministic\")     &gt;&gt;&gt; linear = nn.Linear(2, 2)     &gt;&gt;&gt; linear.weight     Parameter containing:     tensor([[ 0.1913, -0.3420],             [-0.5113, -0.2325]])     &gt;&gt;&gt; linear.to(torch.double)     Linear(in_features=2, out_features=2, bias=True)     &gt;&gt;&gt; linear.weight     Parameter containing:     tensor([[ 0.1913, -0.3420],             [-0.5113, -0.2325]], dtype=torch.float64)     &gt;&gt;&gt; # xdoctest: +REQUIRES(env:TORCH_DOCTEST_CUDA1)     &gt;&gt;&gt; gpu1 = torch.device(\"cuda:1\")     &gt;&gt;&gt; linear.to(gpu1, dtype=torch.half, non_blocking=True)     Linear(in_features=2, out_features=2, bias=True)     &gt;&gt;&gt; linear.weight     Parameter containing:     tensor([[ 0.1914, -0.3420],             [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1')     &gt;&gt;&gt; cpu = torch.device(\"cpu\")     &gt;&gt;&gt; linear.to(cpu)     Linear(in_features=2, out_features=2, bias=True)     &gt;&gt;&gt; linear.weight     Parameter containing:     tensor([[ 0.1914, -0.3420],             [-0.5112, -0.2324]], dtype=torch.float16)      &gt;&gt;&gt; linear = nn.Linear(2, 2, bias=None).to(torch.cdouble)     &gt;&gt;&gt; linear.weight     Parameter containing:     tensor([[ 0.3741+0.j,  0.2382+0.j],             [ 0.5593+0.j, -0.4443+0.j]], dtype=torch.complex128)     &gt;&gt;&gt; linear(torch.ones(3, 2, dtype=torch.cdouble))     tensor([[0.6122+0.j, 0.1150+0.j],             [0.6122+0.j, 0.1150+0.j],             [0.6122+0.j, 0.1150+0.j]], dtype=torch.complex128)</p> <p>Parameters</p> <ul> <li>args </li> <li>kwargs      Additional parameters to the SentenceTransformer model.</li> </ul> to_empty <p>Moves the parameters and buffers to the specified device without copying storage.</p> <p>Args:     device (:class:<code>torch.device</code>): The desired device of the parameters         and buffers in this module.  Returns:     Module: self</p> <p>Parameters</p> <ul> <li>device     (Union[str, torch.device])    </li> </ul> train <p>Sets the module in training mode.</p> <p>This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class:<code>Dropout</code>, :class:<code>BatchNorm</code>, etc.  Args:     mode (bool): whether to set training mode (<code>True</code>) or evaluation                  mode (<code>False</code>). Default: <code>True</code>.  Returns:     Module: self</p> <p>Parameters</p> <ul> <li>mode     (bool)     \u2013 defaults to <code>True</code> </li> </ul> type <p>Casts all parameters and buffers to :attr:<code>dst_type</code>.</p> <p>.. note::     This method modifies the module in-place.  Args:     dst_type (type or string): the desired type  Returns:     Module: self</p> <p>Parameters</p> <ul> <li>dst_type     (Union[torch.dtype, str])    </li> </ul> xpu <p>Moves all model parameters and buffers to the XPU.</p> <p>This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on XPU while being optimized.  .. note::     This method modifies the module in-place.  Arguments:     device (int, optional): if specified, all parameters will be         copied to that device  Returns:     Module: self</p> <p>Parameters</p> <ul> <li>device     (Union[int, torch.device, NoneType])     \u2013 defaults to <code>None</code> </li> </ul> zero_grad <p>Sets gradients of all model parameters to zero. See similar function under :class:<code>torch.optim.Optimizer</code> for more context.</p> <p>Args:     set_to_none (bool): instead of setting to zero, set the grads to None.         See :meth:<code>torch.optim.Optimizer.zero_grad</code> for details.</p> <p>Parameters</p> <ul> <li>set_to_none     (bool)     \u2013 defaults to <code>True</code> </li> </ul>"},{"location":"api/models/Splade/#references","title":"References","text":"<ol> <li>SPLADE: Sparse Lexical and Expansion Model for First Stage Ranking</li> </ol>"},{"location":"api/rank/ColBERT/","title":"ColBERT","text":"<p>ColBERT ranker.</p>"},{"location":"api/rank/ColBERT/#parameters","title":"Parameters","text":"<ul> <li> <p>key (str)</p> </li> <li> <p>on (list[str])</p> </li> <li> <p>model (models.ColBERT)</p> <p>ColBERT model.</p> </li> </ul>"},{"location":"api/rank/ColBERT/#examples","title":"Examples","text":"<pre><code>&gt;&gt;&gt; from neural_cherche import models, rank\n&gt;&gt;&gt; from pprint import pprint\n&gt;&gt;&gt; import torch\n&gt;&gt;&gt; _ = torch.manual_seed(42)\n&gt;&gt;&gt; encoder = models.ColBERT(\n...     model_name_or_path=\"raphaelsty/neural-cherche-colbert\",\n...     device=\"mps\",\n... )\n&gt;&gt;&gt; documents = [\n...     {\"id\": 0, \"document\": \"Food\"},\n...     {\"id\": 1, \"document\": \"Sports\"},\n...     {\"id\": 2, \"document\": \"Cinema\"},\n... ]\n&gt;&gt;&gt; queries = [\"Food\", \"Sports\", \"Cinema\"]\n&gt;&gt;&gt; ranker = rank.ColBERT(\n...    key=\"id\",\n...    on=[\"document\"],\n...    model=encoder,\n... )\n&gt;&gt;&gt; queries_embeddings = ranker.encode_queries(\n...     queries=queries,\n...     batch_size=3,\n... )\n&gt;&gt;&gt; documents_embeddings = ranker.encode_documents(\n...     documents=documents,\n...     batch_size=3,\n... )\n&gt;&gt;&gt; scores = ranker(\n...     documents=[documents for _ in queries],\n...     queries_embeddings=queries_embeddings,\n...     documents_embeddings=documents_embeddings,\n...     batch_size=3,\n...     tqdm_bar=True,\n...     k=3,\n... )\n&gt;&gt;&gt; pprint(scores)\n[[{'document': 'Food', 'id': 0, 'similarity': 20.23601531982422},\n{'document': 'Cinema', 'id': 2, 'similarity': 7.255690574645996},\n{'document': 'Sports', 'id': 1, 'similarity': 6.666046142578125}],\n[{'document': 'Sports', 'id': 1, 'similarity': 21.373430252075195},\n{'document': 'Cinema', 'id': 2, 'similarity': 5.494492053985596},\n{'document': 'Food', 'id': 0, 'similarity': 4.814355850219727}],\n[{'document': 'Sports', 'id': 1, 'similarity': 9.25660228729248},\n{'document': 'Food', 'id': 0, 'similarity': 8.206350326538086},\n{'document': 'Cinema', 'id': 2, 'similarity': 5.496612548828125}]]\n</code></pre>"},{"location":"api/rank/ColBERT/#methods","title":"Methods","text":"call <p>Rank documents  givent queries.</p> <p>Parameters</p> <ul> <li>documents     (list[list[dict]])    </li> <li>queries_embeddings     (dict[str, torch.Tensor])    </li> <li>documents_embeddings     (dict[str, torch.Tensor])    </li> <li>batch_size     (int)     \u2013 defaults to <code>32</code> </li> <li>tqdm_bar     (bool)     \u2013 defaults to <code>True</code> </li> <li>k     (int)     \u2013 defaults to <code>None</code> </li> </ul> encode_documents <p>Encode documents.</p> <p>Parameters</p> <ul> <li>documents     (list[str])    </li> <li>batch_size     (int)     \u2013 defaults to <code>32</code> </li> <li>tqdm_bar     (bool)     \u2013 defaults to <code>True</code> </li> <li>query_mode     (bool)     \u2013 defaults to <code>False</code> </li> <li>kwargs </li> </ul> encode_queries <p>Encode queries.</p> <p>Parameters</p> <ul> <li>queries     (list[str])    </li> <li>batch_size     (int)     \u2013 defaults to <code>32</code> </li> <li>tqdm_bar     (bool)     \u2013 defaults to <code>True</code> </li> <li>query_mode     (bool)     \u2013 defaults to <code>True</code> </li> <li>kwargs </li> </ul>"},{"location":"api/retrieve/SparseEmbed/","title":"SparseEmbed","text":"<p>Retriever class.</p>"},{"location":"api/retrieve/SparseEmbed/#parameters","title":"Parameters","text":"<ul> <li> <p>key (str)</p> <p>Document unique identifier.</p> </li> <li> <p>on (list[str])</p> <p>Document texts.</p> </li> <li> <p>model (models.SparseEmbed)</p> <p>SparsEmbed model.</p> </li> <li> <p>tokenizer_parallelism (str) \u2013 defaults to <code>false</code></p> </li> </ul>"},{"location":"api/retrieve/SparseEmbed/#examples","title":"Examples","text":"<pre><code>&gt;&gt;&gt; from neural_cherche import models, retrieve\n&gt;&gt;&gt; from pprint import pprint\n&gt;&gt;&gt; import torch\n&gt;&gt;&gt; _ = torch.manual_seed(42)\n&gt;&gt;&gt; device = \"mps\"\n&gt;&gt;&gt; model = models.SparseEmbed(\n...     model_name_or_path=\"raphaelsty/neural-cherche-sparse-embed\",\n...     device=device,\n...     embedding_size=64,\n... )\n&gt;&gt;&gt; retriever = retrieve.SparseEmbed(\n...     key=\"id\",\n...     on=\"document\",\n...     model=model,\n... )\n&gt;&gt;&gt; documents = [\n...     {\"id\": 0, \"document\": \"Food\"},\n...     {\"id\": 1, \"document\": \"Sports\"},\n...     {\"id\": 2, \"document\": \"Cinema\"},\n... ]\n&gt;&gt;&gt; queries = [\"Food\", \"Sports\", \"Cinema\"]\n&gt;&gt;&gt; documents_embeddings = retriever.encode_documents(\n...     documents=documents,\n...     batch_size=1,\n... )\n&gt;&gt;&gt; queries_embeddings = retriever.encode_queries(\n...     queries=queries,\n...     batch_size=1,\n... )\n&gt;&gt;&gt; retriever = retriever.add(\n...     documents_embeddings=documents_embeddings,\n... )\n&gt;&gt;&gt; scores = retriever(\n...     queries_embeddings=queries_embeddings,\n...     batch_size=32\n... )\n&gt;&gt;&gt; pprint(scores)\n[[{'id': 0, 'similarity': 62.01531219482422},\n{'id': 1, 'similarity': 59.01810836791992},\n{'id': 2, 'similarity': 40.613182067871094}],\n[{'id': 1, 'similarity': 97.81436920166016},\n{'id': 2, 'similarity': 32.50034713745117},\n{'id': 0, 'similarity': 25.678363800048828}],\n[{'id': 2, 'similarity': 56.019283294677734},\n{'id': 1, 'similarity': 37.612735748291016},\n{'id': 0, 'similarity': 26.307708740234375}]]\n</code></pre>"},{"location":"api/retrieve/SparseEmbed/#methods","title":"Methods","text":"call <p>Retrieve documents.</p> <p>Parameters</p> <ul> <li>queries_embeddings     (dict[str, scipy.sparse._csr.csr_matrix])    </li> <li>k     (int)     \u2013 defaults to <code>None</code> </li> <li>batch_size     (int)     \u2013 defaults to <code>64</code> </li> <li>tqdm_bar     (bool)     \u2013 defaults to <code>True</code> </li> </ul> add <p>Add documents embeddings and activations to the retriever.</p> <p>Parameters</p> <ul> <li>documents_embeddings     (dict[dict[str, torch.Tensor]])    </li> </ul> encode_documents <p>Encode documents.</p> <p>Parameters</p> <ul> <li>documents     (list[dict])    </li> <li>batch_size     (int)     \u2013 defaults to <code>32</code> </li> <li>tqdm_bar     (bool)     \u2013 defaults to <code>True</code> </li> <li>query_mode     (bool)     \u2013 defaults to <code>False</code> </li> <li>kwargs </li> </ul> encode_queries <p>Encode queries.</p> <p>Parameters</p> <ul> <li>queries     (list[str])    </li> <li>batch_size     (int)     \u2013 defaults to <code>32</code> </li> <li>tqdm_bar     (bool)     \u2013 defaults to <code>True</code> </li> <li>query_mode     (bool)     \u2013 defaults to <code>True</code> </li> <li>kwargs </li> </ul> top_k <p>Return the top k documents for each query.</p> <p>Parameters</p> <ul> <li>similarities     (scipy.sparse._csc.csc_matrix)    </li> <li>k     (int)    </li> </ul>"},{"location":"api/retrieve/Splade/","title":"Splade","text":"<p>Retriever class.</p>"},{"location":"api/retrieve/Splade/#parameters","title":"Parameters","text":"<ul> <li> <p>key (str)</p> <p>Document unique identifier.</p> </li> <li> <p>on (list[str])</p> <p>Document texts.</p> </li> <li> <p>model (models.Splade)</p> <p>SparsEmbed model.</p> </li> <li> <p>tokenizer_parallelism (str) \u2013 defaults to <code>false</code></p> </li> </ul>"},{"location":"api/retrieve/Splade/#examples","title":"Examples","text":"<pre><code>&gt;&gt;&gt; from neural_cherche import models, retrieve\n&gt;&gt;&gt; from pprint import pprint\n&gt;&gt;&gt; import torch\n&gt;&gt;&gt; _ = torch.manual_seed(42)\n&gt;&gt;&gt; documents = [\n...     {\"id\": 0, \"document\": \"Food\"},\n...     {\"id\": 1, \"document\": \"Sports\"},\n...     {\"id\": 2, \"document\": \"Cinema\"},\n... ]\n&gt;&gt;&gt; queries = [\"Food\", \"Sports\", \"Cinema\"]\n&gt;&gt;&gt; model = models.Splade(\n...     model_name_or_path=\"raphaelsty/neural-cherche-sparse-embed\",\n...     device=\"mps\",\n... )\n&gt;&gt;&gt; retriever = retrieve.Splade(\n...     key=\"id\",\n...     on=\"document\",\n...     model=model\n... )\n&gt;&gt;&gt; documents_embeddings = retriever.encode_documents(\n...     documents=documents,\n...     batch_size=32,\n... )\n&gt;&gt;&gt; queries_embeddings = retriever.encode_queries(\n...     queries=queries,\n...     batch_size=32,\n... )\n&gt;&gt;&gt; retriever = retriever.add(\n...     documents_embeddings=documents_embeddings,\n... )\n&gt;&gt;&gt; scores = retriever(\n...     queries_embeddings=queries_embeddings,\n...     k=3,\n... )\n&gt;&gt;&gt; pprint(scores)\n[[{'id': 0, 'similarity': 489.65244},\n{'id': 2, 'similarity': 338.9705},\n{'id': 1, 'similarity': 332.3472}],\n[{'id': 1, 'similarity': 470.40497},\n{'id': 2, 'similarity': 301.56982},\n{'id': 0, 'similarity': 278.8062}],\n[{'id': 2, 'similarity': 472.487},\n{'id': 1, 'similarity': 341.8396},\n{'id': 0, 'similarity': 319.97287}]]\n</code></pre>"},{"location":"api/retrieve/Splade/#methods","title":"Methods","text":"call <p>Retrieve documents from batch of queries.</p> <p>Parameters</p> <ul> <li>queries_embeddings     (dict[str, scipy.sparse._csr.csr_matrix])    </li> <li>k     (int)     \u2013 defaults to <code>None</code> </li> <li>batch_size     (int)     \u2013 defaults to <code>2000</code> </li> <li>tqdm_bar     (bool)     \u2013 defaults to <code>True</code> </li> </ul> add <p>Add new documents to the TFIDF retriever. The tfidf won't be refitted.</p> <p>Parameters</p> <ul> <li>documents_embeddings     (dict[str, scipy.sparse._csr.csr_matrix])    </li> </ul> encode_documents <p>Encode queries into sparse matrix.</p> <p>Parameters</p> <ul> <li>documents     (list[dict])    </li> <li>batch_size     (int)     \u2013 defaults to <code>32</code> </li> <li>tqdm_bar     (bool)     \u2013 defaults to <code>True</code> </li> <li>query_mode     (bool)     \u2013 defaults to <code>False</code> </li> <li>kwargs </li> </ul> encode_queries <p>Encode queries into sparse matrix.</p> <p>Parameters</p> <ul> <li>queries     (list[str])    </li> <li>batch_size     (int)     \u2013 defaults to <code>32</code> </li> <li>tqdm_bar     (bool)     \u2013 defaults to <code>True</code> </li> <li>query_mode     (bool)     \u2013 defaults to <code>True</code> </li> <li>kwargs </li> </ul> top_k <p>Return the top k documents for each query.</p> <p>Parameters</p> <ul> <li>similarities     (scipy.sparse._csc.csc_matrix)    </li> <li>k     (int)    </li> </ul>"},{"location":"api/retrieve/TfIdf/","title":"TfIdf","text":"<p>TfIdf retriever based on cosine similarities.</p>"},{"location":"api/retrieve/TfIdf/#parameters","title":"Parameters","text":"<ul> <li> <p>key (str)</p> <p>Field identifier of each document.</p> </li> <li> <p>on (list[str])</p> <p>Fields to use to match the query to the documents.</p> </li> <li> <p>tfidf (sklearn.feature_extraction.text.TfidfVectorizer) \u2013 defaults to <code>None</code></p> <p>TfidfVectorizer class of Sklearn to create a custom TfIdf retriever.</p> </li> <li> <p>fit (bool) \u2013 defaults to <code>True</code></p> </li> </ul>"},{"location":"api/retrieve/TfIdf/#examples","title":"Examples","text":"<pre><code>&gt;&gt;&gt; from neural_cherche import retrieve\n&gt;&gt;&gt; from pprint import pprint\n&gt;&gt;&gt; documents = [\n...     {\"id\": 0, \"document\": \"Food\"},\n...     {\"id\": 1, \"document\": \"Sports\"},\n...     {\"id\": 2, \"document\": \"Cinema\"},\n... ]\n&gt;&gt;&gt; queries = [\"Food\", \"Sports\", \"Cinema\"]\n&gt;&gt;&gt; retriever = retrieve.TfIdf(\n...     key=\"id\",\n...     on=[\"document\"],\n... )\n&gt;&gt;&gt; documents_embeddings = retriever.encode_documents(\n...     documents=documents\n... )\n&gt;&gt;&gt; retriever = retriever.add(\n...     documents_embeddings=documents_embeddings,\n... )\n&gt;&gt;&gt; queries_embeddings = retriever.encode_queries(\n...     queries=queries\n... )\n&gt;&gt;&gt; scores = retriever(\n...     queries_embeddings=queries_embeddings,\n...     k=4\n... )\n&gt;&gt;&gt; pprint(scores)\n[[{'id': 0, 'similarity': 1.0}],\n[{'id': 1, 'similarity': 0.9999999999999999}],\n[{'id': 2, 'similarity': 0.9999999999999999}]]\n</code></pre>"},{"location":"api/retrieve/TfIdf/#methods","title":"Methods","text":"call <p>Retrieve documents from batch of queries.</p> <p>Parameters</p> <ul> <li>queries_embeddings     (dict[str, scipy.sparse._csr.csr_matrix])    </li> <li>k     (int)     \u2013 defaults to <code>None</code>      Number of documents to retrieve. Default is <code>None</code>, i.e all documents that match the query will be retrieved.</li> <li>batch_size     (int)     \u2013 defaults to <code>2000</code> </li> <li>tqdm_bar     (bool)     \u2013 defaults to <code>True</code> </li> </ul> add <p>Add new documents to the TFIDF retriever. The tfidf won't be refitted.</p> <p>Parameters</p> <ul> <li>documents_embeddings     (dict[str, scipy.sparse._csr.csr_matrix])    </li> </ul> encode_documents <p>Encode queries into sparse matrix.</p> <p>Parameters</p> <ul> <li>documents     (list[dict])       Documents in TFIdf retriever are static. The retriever must be reseted to index new documents.</li> </ul> encode_queries <p>Encode queries into sparse matrix.</p> <p>Parameters</p> <ul> <li>queries     (list[str])    </li> </ul> top_k <p>Return the top k documents for each query.</p> <p>Parameters</p> <ul> <li>similarities     (scipy.sparse._csc.csc_matrix)    </li> <li>k     (int)       Number of documents to retrieve. Default is <code>None</code>, i.e all documents that match the query will be retrieved.</li> </ul>"},{"location":"api/retrieve/TfIdf/#references","title":"References","text":"<ol> <li>sklearn.feature_extraction.text.TfidfVectorizer</li> <li>Python: tf-idf-cosine: to find document similarity</li> </ol>"},{"location":"api/train/train-colbert/","title":"train_colbert","text":"<p>Compute the ranking loss and the flops loss for a single step.</p>"},{"location":"api/train/train-colbert/#parameters","title":"Parameters","text":"<ul> <li> <p>model</p> <p>Colbert model.</p> </li> <li> <p>optimizer</p> <p>Optimizer.</p> </li> <li> <p>anchor (list[str])</p> <p>Anchor.</p> </li> <li> <p>positive (list[str])</p> <p>Positive.</p> </li> <li> <p>negative (list[str])</p> <p>Negative.</p> </li> <li> <p>in_batch_negatives (bool) \u2013 defaults to <code>False</code></p> <p>Whether to use in batch negatives or not. Defaults to True.</p> </li> <li> <p>kwargs</p> </li> </ul>"},{"location":"api/train/train-colbert/#examples","title":"Examples","text":"<pre><code>&gt;&gt;&gt; from neural_cherche import models, utils, train\n&gt;&gt;&gt; import torch\n&gt;&gt;&gt; _ = torch.manual_seed(42)\n&gt;&gt;&gt; device = \"mps\"\n&gt;&gt;&gt; model = models.ColBERT(\n...     model_name_or_path=\"raphaelsty/neural-cherche-colbert\",\n...     device=device\n... )\n&gt;&gt;&gt; optimizer = torch.optim.AdamW(model.parameters(), lr=1e-6)\n&gt;&gt;&gt; X = [\n...     (\"Sports\", \"Football\", \"Cinema\"),\n...     (\"Sports\", \"Rugby\", \"Cinema\"),\n...     (\"Sports\", \"Tennis\", \"Cinema\"),\n... ]\n&gt;&gt;&gt; for anchor, positive, negative in utils.iter(\n...         X,\n...         epochs=3,\n...         batch_size=3,\n...         shuffle=False\n...     ):\n...     loss = train.train_colbert(\n...         model=model,\n...         optimizer=optimizer,\n...         anchor=anchor,\n...         positive=positive,\n...         negative=negative,\n...         in_batch_negatives=False,\n...     )\n&gt;&gt;&gt; loss\n{'loss': tensor(0.0054, device='mps:0', grad_fn=&lt;ClampBackward1&gt;)}\n</code></pre>"},{"location":"api/train/train-sparse-embed/","title":"train_sparse_embed","text":"<p>Compute the ranking loss and the flops loss for a single step.</p>"},{"location":"api/train/train-sparse-embed/#parameters","title":"Parameters","text":"<ul> <li> <p>model</p> <p>Splade model.</p> </li> <li> <p>optimizer</p> <p>Optimizer.</p> </li> <li> <p>anchor (list[str])</p> <p>Anchor.</p> </li> <li> <p>positive (list[str])</p> <p>Positive.</p> </li> <li> <p>negative (list[str])</p> <p>Negative.</p> </li> <li> <p>flops_loss_weight (float) \u2013 defaults to <code>0.0001</code></p> <p>Flops loss weight. Defaults to 1e-5.</p> </li> <li> <p>sparse_loss_weight (float) \u2013 defaults to <code>0.1</code></p> <p>Sparse loss weight. Defaults to 1.0.</p> </li> <li> <p>dense_loss_weight (float) \u2013 defaults to <code>1.0</code></p> <p>Dense loss weight. Defaults to 1.0.</p> </li> <li> <p>in_batch_negatives (bool) \u2013 defaults to <code>False</code></p> <p>Whether to use in batch negatives or not. Defaults to True.</p> </li> <li> <p>threshold_flops (float) \u2013 defaults to <code>30</code></p> <p>Threshold margin for the flops loss. Defaults to 10.</p> </li> <li> <p>kwargs</p> </li> </ul>"},{"location":"api/train/train-sparse-embed/#examples","title":"Examples","text":"<pre><code>&gt;&gt;&gt; from neural_cherche import models, utils, train\n&gt;&gt;&gt; import torch\n&gt;&gt;&gt; _ = torch.manual_seed(42)\n&gt;&gt;&gt; model = models.SparseEmbed(\n...     model_name_or_path=\"raphaelsty/neural-cherche-sparse-embed\",\n...     device=\"mps\",\n... )\n&gt;&gt;&gt; optimizer = torch.optim.AdamW(model.parameters(), lr=1e-6)\n&gt;&gt;&gt; X = [\n...     (\"Sports\", \"Music\", \"Cinema\"),\n...     (\"Sports\", \"Music\", \"Cinema\"),\n...     (\"Sports\", \"Music\", \"Cinema\"),\n... ]\n&gt;&gt;&gt; flops_scheduler = losses.FlopsScheduler()\n&gt;&gt;&gt; for anchor, positive, negative in utils.iter(\n...         X,\n...         epochs=3,\n...         batch_size=3,\n...         shuffle=False\n...     ):\n...     loss = train.train_sparse_embed(\n...         model=model,\n...         optimizer=optimizer,\n...         anchor=anchor,\n...         positive=positive,\n...         negative=negative,\n...         flops_loss_weight=flops_scheduler.get(),\n...         in_batch_negatives=False,\n...     )\n&gt;&gt;&gt; loss\n{'dense': tensor(0.0015, device='mps:0', grad_fn=&lt;ClampBackward1&gt;), 'sparse': tensor(1.1921e-07, device='mps:0', grad_fn=&lt;ClampBackward1&gt;), 'flops': tensor(10., device='mps:0', grad_fn=&lt;ClampBackward1&gt;)}\n</code></pre>"},{"location":"api/train/train-splade/","title":"train_splade","text":"<p>Compute the ranking loss and the flops loss for a single step.</p>"},{"location":"api/train/train-splade/#parameters","title":"Parameters","text":"<ul> <li> <p>model</p> <p>Splade model.</p> </li> <li> <p>optimizer</p> <p>Optimizer.</p> </li> <li> <p>anchor (list[str])</p> <p>Anchor.</p> </li> <li> <p>positive (list[str])</p> <p>Positive.</p> </li> <li> <p>negative (list[str])</p> <p>Negative.</p> </li> <li> <p>flops_loss_weight (float) \u2013 defaults to <code>0.0001</code></p> <p>Flops loss weight. Defaults to 1e-4.</p> </li> <li> <p>sparse_loss_weight (float) \u2013 defaults to <code>1.0</code></p> </li> <li> <p>in_batch_negatives (bool) \u2013 defaults to <code>False</code></p> <p>Whether to use in batch negatives or not. Defaults to True.</p> </li> <li> <p>threshold_flops (float) \u2013 defaults to <code>30</code></p> </li> <li> <p>kwargs</p> </li> </ul>"},{"location":"api/train/train-splade/#examples","title":"Examples","text":"<pre><code>&gt;&gt;&gt; from neural_cherche import models, utils, train\n&gt;&gt;&gt; import torch\n&gt;&gt;&gt; _ = torch.manual_seed(42)\n&gt;&gt;&gt; model = models.Splade(\n...     model_name_or_path=\"raphaelsty/neural-cherche-sparse-embed\",\n...     device=\"mps\",\n... )\n&gt;&gt;&gt; optimizer = torch.optim.AdamW(\n...     model.parameters(),\n...     lr=1e-6,\n... )\n&gt;&gt;&gt; X = [\n...     (\"Sports\", \"Music\", \"Cinema\"),\n...     (\"Sports\", \"Music\", \"Cinema\"),\n...     (\"Sports\", \"Music\", \"Cinema\"),\n... ]\n&gt;&gt;&gt; flops_scheduler = losses.FlopsScheduler()\n&gt;&gt;&gt; for anchor, positive, negative in utils.iter(\n...         X,\n...         epochs=3,\n...         batch_size=3,\n...         shuffle=False\n...     ):\n...     loss = train.train_splade(\n...         model=model,\n...         optimizer=optimizer,\n...         anchor=anchor,\n...         positive=positive,\n...         negative=negative,\n...         flops_loss_weight=flops_scheduler.get(),\n...         in_batch_negatives=False,\n...     )\n&gt;&gt;&gt; loss\n{'sparse': tensor(0., device='mps:0', grad_fn=&lt;ClampBackward1&gt;), 'flops': tensor(10., device='mps:0', grad_fn=&lt;ClampBackward1&gt;)}\n</code></pre>"},{"location":"api/utils/batchify/","title":"batchify","text":""},{"location":"api/utils/batchify/#parameters","title":"Parameters","text":"<ul> <li> <p>X (list[str])</p> </li> <li> <p>batch_size (int)</p> </li> <li> <p>desc (str) \u2013 defaults to ``</p> </li> <li> <p>tqdm_bar (bool) \u2013 defaults to <code>True</code></p> </li> </ul>"},{"location":"api/utils/colbert-scores/","title":"colbert_scores","text":"<p>ColBERT scoring function for training.</p>"},{"location":"api/utils/colbert-scores/#parameters","title":"Parameters","text":"<ul> <li> <p>anchor_embeddings (torch.Tensor)</p> <p>Anchor embeddings.</p> </li> <li> <p>positive_embeddings (torch.Tensor)</p> <p>Positive embeddings.</p> </li> <li> <p>negative_embeddings (torch.Tensor)</p> <p>Negative embeddings.</p> </li> <li> <p>in_batch_negatives (bool) \u2013 defaults to <code>False</code></p> <p>Whether to use in batch negatives or not. Defaults to False.</p> </li> </ul>"},{"location":"api/utils/colbert-scores/#examples","title":"Examples","text":"<pre><code>&gt;&gt;&gt; from neural_cherche import models, utils\n&gt;&gt;&gt; import torch\n&gt;&gt;&gt; _ = torch.manual_seed(42)\n&gt;&gt;&gt; model = models.ColBERT(\n...     model_name_or_path=\"raphaelsty/neural-cherche-colbert\",\n...     device=\"mps\",\n... )\n&gt;&gt;&gt; anchor_embeddings = model(\n...     [\"Paris\", \"Toulouse\"],\n...     query_mode=True,\n... )\n&gt;&gt;&gt; positive_embeddings = model(\n...    [\"Paris\", \"Toulouse\"],\n...    query_mode=False,\n... )\n&gt;&gt;&gt; negative_embeddings = model(\n...    [\"Toulouse\", \"Paris\"],\n...    query_mode=False,\n... )\n&gt;&gt;&gt; scores = utils.colbert_scores(\n...     anchor_embeddings=anchor_embeddings[\"embeddings\"],\n...     positive_embeddings=positive_embeddings[\"embeddings\"],\n...     negative_embeddings=negative_embeddings[\"embeddings\"],\n... )\n&gt;&gt;&gt; scores\n{'positive_scores': tensor([24.7555, 26.4455], device='mps:0', grad_fn=&lt;SumBackward1&gt;), 'negative_scores': tensor([18.3089, 17.1017], device='mps:0', grad_fn=&lt;SumBackward1&gt;)}\n</code></pre>"},{"location":"api/utils/dense-scores/","title":"dense_scores","text":"<p>Computes score between queries and documents intersected activated tokens.</p>"},{"location":"api/utils/dense-scores/#parameters","title":"Parameters","text":"<ul> <li> <p>anchor_activations (torch.Tensor)</p> </li> <li> <p>positive_activations (torch.Tensor)</p> </li> <li> <p>negative_activations (torch.Tensor)</p> </li> <li> <p>anchor_embeddings (torch.Tensor)</p> </li> <li> <p>positive_embeddings (torch.Tensor)</p> </li> <li> <p>negative_embeddings (torch.Tensor)</p> </li> <li> <p>func \u2013 defaults to <code>&lt;built-in method sum of type object at 0x107e07280&gt;</code></p> <p>Either torch.sum or torch.mean. torch.mean is dedicated to training and torch.sum is dedicated to inference.</p> </li> </ul>"},{"location":"api/utils/dense-scores/#examples","title":"Examples","text":"<pre><code>&gt;&gt;&gt; from neural_cherche import models, utils\n&gt;&gt;&gt; import torch\n&gt;&gt;&gt; _ = torch.manual_seed(42)\n&gt;&gt;&gt; model = models.SparseEmbed(\n...     model_name_or_path=\"raphaelsty/neural-cherche-sparse-embed\",\n...     device=\"mps\",\n... )\n&gt;&gt;&gt; anchor_embeddings = model(\n...     [\"Paris\", \"Toulouse\"],\n...     query_mode=True,\n... )\n&gt;&gt;&gt; positive_embeddings = model(\n...    [\"Paris\", \"Toulouse\"],\n...    query_mode=False,\n... )\n&gt;&gt;&gt; negative_embeddings = model(\n...    [\"Toulouse\", \"Paris\"],\n...    query_mode=False,\n... )\n&gt;&gt;&gt; scores = utils.dense_scores(\n...     anchor_activations=anchor_embeddings[\"activations\"],\n...     positive_activations=positive_embeddings[\"activations\"],\n...     negative_activations=negative_embeddings[\"activations\"],\n...     anchor_embeddings=anchor_embeddings[\"embeddings\"],\n...     positive_embeddings=positive_embeddings[\"embeddings\"],\n...     negative_embeddings=negative_embeddings[\"embeddings\"],\n...     func=torch.sum,\n... )\n&gt;&gt;&gt; scores\n{'positive_scores': tensor([144.4106, 155.5398], device='mps:0', grad_fn=&lt;StackBackward0&gt;), 'negative_scores': tensor([173.4966,  99.9521], device='mps:0', grad_fn=&lt;StackBackward0&gt;)}\n</code></pre>"},{"location":"api/utils/evaluate/","title":"evaluate","text":"<p>Evaluate candidates matchs.</p>"},{"location":"api/utils/evaluate/#parameters","title":"Parameters","text":"<ul> <li> <p>scores (list[list[dict]])</p> </li> <li> <p>qrels (dict)</p> <p>Qrels.</p> </li> <li> <p>queries_ids (list[str])</p> </li> <li> <p>metrics (list) \u2013 defaults to <code>[]</code></p> <p>Metrics to compute.</p> </li> </ul>"},{"location":"api/utils/evaluate/#examples","title":"Examples","text":"<pre><code>&gt;&gt;&gt; from neural_cherche import models, retrieve, utils\n&gt;&gt;&gt; import torch\n&gt;&gt;&gt; _ = torch.manual_seed(42)\n&gt;&gt;&gt; model = models.Splade(\n...     model_name_or_path=\"raphaelsty/neural-cherche-sparse-embed\",\n...     device=\"cpu\",\n... )\n&gt;&gt;&gt; documents, queries_ids, queries, qrels = utils.load_beir(\n...     \"scifact\",\n...     split=\"test\",\n... )\n&gt;&gt;&gt; documents = documents[:10]\n&gt;&gt;&gt; retriever = retrieve.Splade(\n...     key=\"id\",\n...     on=[\"title\", \"text\"],\n...     model=model\n... )\n&gt;&gt;&gt; documents_embeddings = retriever.encode_documents(\n...     documents=documents,\n...     batch_size=1,\n... )\n&gt;&gt;&gt; documents_embeddings = retriever.add(\n...     documents_embeddings=documents_embeddings,\n... )\n&gt;&gt;&gt; queries_embeddings = retriever.encode_queries(\n...     queries=queries,\n...     batch_size=1,\n... )\n&gt;&gt;&gt; scores = retriever(\n...     queries_embeddings=queries_embeddings,\n...     k=30,\n...     batch_size=1,\n... )\n&gt;&gt;&gt; utils.evaluate(\n...     scores=scores,\n...     qrels=qrels,\n...     queries_ids=queries_ids,\n...     metrics=[\"map\", \"ndcg@10\", \"ndcg@100\", \"recall@10\", \"recall@100\"]\n... )\n{'map': 0.0033333333333333335, 'ndcg@10': 0.0033333333333333335, 'ndcg@100': 0.0033333333333333335, 'recall@10': 0.0033333333333333335, 'recall@100': 0.0033333333333333335}\n</code></pre>"},{"location":"api/utils/in-batch-sparse-scores/","title":"in_batch_sparse_scores","text":"<p>Computes dot product between anchor, positive and negative activations.</p>"},{"location":"api/utils/in-batch-sparse-scores/#parameters","title":"Parameters","text":"<ul> <li>activations</li> </ul>"},{"location":"api/utils/in-batch-sparse-scores/#examples","title":"Examples","text":"<pre><code>&gt;&gt;&gt; from neural_cherche import utils\n&gt;&gt;&gt; activations = torch.tensor([\n...     [0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n...     [0, 0, 0, 0, 0, 0, 0, 0, 0, 2],\n...     [0, 0, 0, 0, 0, 0, 0, 0, 0, 3],\n... ], device=\"mps\")\n&gt;&gt;&gt; utils.in_batch_sparse_scores(\n...     activations=activations,\n... )\ntensor([5, 8, 9], device='mps:0')\n</code></pre>"},{"location":"api/utils/iter/","title":"iter","text":"<p>Iterate over a list of tuples (query, document, score) by batch of size batch_size.</p>"},{"location":"api/utils/iter/#parameters","title":"Parameters","text":"<ul> <li> <p>X (list[tuple[str, str, float]])</p> <p>List of tuples (query, document, score).</p> </li> <li> <p>epochs (int)</p> <p>Number of epochs.</p> </li> <li> <p>batch_size (int)</p> <p>Batch size.</p> </li> <li> <p>shuffle (bool) \u2013 defaults to <code>True</code></p> <p>Shuffle the data.</p> </li> </ul>"},{"location":"api/utils/iter/#examples","title":"Examples","text":"<pre><code>&gt;&gt;&gt; from neural_cherche import utils\n&gt;&gt;&gt; X = [\n...    (\"Apple\", \"\ud83c\udf4f\", \"cherry\"),\n...    (\"Banana\", \"\ud83c\udf4c\", \"cherry\"),\n... ]\n&gt;&gt;&gt; for anchor, positive, negative in utils.iter(\n...         X,\n...         epochs=1,\n...         batch_size=3,\n...         shuffle=False\n...     ):\n...     break\n&gt;&gt;&gt; print(anchor)\n['Apple', 'Banana']\n&gt;&gt;&gt; print(positive)\n['\ud83c\udf4f', '\ud83c\udf4c']\n&gt;&gt;&gt; print(negative)\n['cherry', 'cherry']\n</code></pre>"},{"location":"api/utils/load-beir/","title":"load_beir","text":"<p>Load BEIR dataset.</p>"},{"location":"api/utils/load-beir/#parameters","title":"Parameters","text":"<ul> <li> <p>dataset_name (str)</p> <p>Dataset name: scifact.</p> </li> <li> <p>split (str) \u2013 defaults to <code>test</code></p> </li> </ul>"},{"location":"api/utils/pairs-dense-scores/","title":"pairs_dense_scores","text":"<p>Scores pairs of queries and documents based on activated tokens.</p>"},{"location":"api/utils/pairs-dense-scores/#parameters","title":"Parameters","text":"<ul> <li> <p>queries_activations (torch.Tensor)</p> </li> <li> <p>documents_activations (torch.Tensor)</p> </li> <li> <p>queries_embeddings (torch.Tensor)</p> </li> <li> <p>documents_embeddings (torch.Tensor)</p> </li> </ul>"},{"location":"api/utils/sparse-scores/","title":"sparse_scores","text":"<p>Computes dot product between anchor, positive and negative activations.</p>"},{"location":"api/utils/sparse-scores/#parameters","title":"Parameters","text":"<ul> <li> <p>anchor_activations (torch.Tensor)</p> <p>Activations of the anchors.</p> </li> <li> <p>positive_activations (torch.Tensor)</p> <p>Activations of the positive documents.</p> </li> <li> <p>negative_activations (torch.Tensor)</p> <p>Activations of the negative documents.</p> </li> <li> <p>in_batch_negatives (bool) \u2013 defaults to <code>False</code></p> <p>Whether to use in batch negatives or not. Defaults to True. Sum up with negative scores the dot product.</p> </li> </ul>"},{"location":"api/utils/sparse-scores/#examples","title":"Examples","text":"<pre><code>&gt;&gt;&gt; from neural_cherche import models\n&gt;&gt;&gt; model = models.Splade(\n...     model_name_or_path=\"raphaelsty/neural-cherche-sparse-embed\",\n...     device=\"mps\"\n... )\n&gt;&gt;&gt; anchor_activations = model(\n...     [\"Sports\", \"Music\"],\n...     query_mode=True,\n... )\n&gt;&gt;&gt; positive_activations = model(\n...    [\"Sports\", \"Music\"],\n...    query_mode=False,\n... )\n&gt;&gt;&gt; negative_activations = model(\n...    [\"Cinema\", \"Movie\"],\n...    query_mode=False,\n... )\n&gt;&gt;&gt; sparse_scores(\n...     anchor_activations=anchor_activations[\"sparse_activations\"],\n...     positive_activations=positive_activations[\"sparse_activations\"],\n...     negative_activations=negative_activations[\"sparse_activations\"],\n... )\n{'positive_scores': tensor([470.4049, 435.0986], device='mps:0', grad_fn=&lt;SumBackward1&gt;), 'negative_scores': tensor([301.5698, 353.6218], device='mps:0', grad_fn=&lt;SumBackward1&gt;)}\n</code></pre>"},{"location":"evaluate/evaluate/","title":"Evaluate","text":"<p>Neural-Cherche evaluation is based on RANX. We can also download datasets of BEIR Benchmark with the <code>utils.load_beir</code> function.</p>"},{"location":"evaluate/evaluate/#installation","title":"Installation","text":"<pre><code>pip install \"neural-cherche[eval]\"\n</code></pre>"},{"location":"evaluate/evaluate/#usage","title":"Usage","text":"<p>Let\"s first create a pipeline which output candidates and scores:</p> <pre><code>from neural_cherche import retrieve, utils\n# Input dataset for evaluation\ndocuments, queries, qrels = utils.load_beir(\n\"scifact\",\nsplit=\"test\",\n)\nretriever = retrieve.BM25(key=\"id\", on=[\"title\", \"text\"])\ndocuments_embeddings = retriever.encode_documents(\ndocuments=documents,\n)\ndocuments_embeddings = retriever.add(\ndocuments_embeddings=documents_embeddings,\n)\nqueries_embeddings = retriever.encode_queries(\nqueries=queries,\n)\nscores = retriever(\nqueries_embeddings=queries_embeddings,\nk=30,\n)\nutils.evaluate(\nscores=scores,\nqrels=qrels,\nqueries=queries,\nmetrics=[\"map\", \"ndcg@10\", \"ndcg@100\", \"recall@10\", \"recall@100\"],\n)\n</code></pre> <pre><code>{\n\"map\": 0.6433690206955331,\n\"ndcg@10\": 0.6848343124746807,\n\"ndcg@100\": 0.7046426757236496,\n\"recall@10\": 0.8167222222222221,\n\"recall@100\": 0.8933333333333333,\n}\n</code></pre>"},{"location":"evaluate/evaluate/#evaluation-dataset","title":"Evaluation dataset","text":"<p>Here are what documents should looks like (an id with multiples fields, no matter the name):</p> <pre><code>[\n{\n\"id\": \"document_0\",\n\"title\": \"title 0\",\n\"text\": \"text 0\",\n},\n{\n\"id\": \"document_1\",\n\"title\": \"title 1\",\n\"text\": \"text 1\",\n},\n...\n{\n\"id\": \"document_n\",\n\"title\": \"title n\",\n\"text\": \"text n\",\n},\n]\n</code></pre> <p>Queries is a list of strings:</p> <pre><code>[\n\"first query\",\n\"second query\",\n\"third query\",\n\"fourth query\",\n\"fifth query\",\n]\n</code></pre> <p>Qrels is the mapping between queries ids as key and dict of relevant documents with 1 as value:</p> <pre><code>{\n\"first query\": {\"document_0\": 1},\n\"second query\": {\"document_10\": 1},\n\"third query\": {\"document_5\": 1},\n\"fourth query\": {\"document_22\": 1},\n\"fifth query\": {\"document_23\": 1, \"document_0\": 1},\n}\n</code></pre>"},{"location":"evaluate/evaluate/#metrics","title":"Metrics","text":"<p>We can evaluate our model with various metrics detailed here.</p>"},{"location":"fine_tune/colbert/","title":"Colbert","text":"<p>Training the ColBERT Model with PyTorch and Neural Cherche Library. The model is updated each time we call <code>train.train_colbert</code> function. It's higly recommended to use a GPU and to use a Sentence Transformer model as the base model.</p> <pre><code>from neural_cherche import models, utils, train\nimport torch\nmodel = models.ColBERT(\nmodel_name_or_path=\"raphaelsty/neural-cherche-colbert\",\ndevice=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n)\noptimizer = torch.optim.AdamW(model.parameters(), lr=3e-6)\nX = [\n(\"query\", \"positive document\", \"negative document\"),\n(\"query\", \"positive document\", \"negative document\"),\n(\"query\", \"positive document\", \"negative document\"),\n]\nfor step, (anchor, positive, negative) in enumerate(utils.iter(\nX,\nepochs=2,\nbatch_size=32,\nshuffle=True\n)):\nloss = train.train_colbert(\nmodel=model,\noptimizer=optimizer,\nanchor=anchor,\npositive=positive,\nnegative=negative,\nstep=step,\ngradient_accumulation_steps=50,\n)\nif (step + 1) % 1000 == 0:\n# Save the model every 1000 steps\nmodel.save_pretrained(\"checkpoint\")\n</code></pre> <p>We can load the checkpoint using:</p> <pre><code>from neural_cherche import models\nimport torch\nmodel = models.ColBERT(\nmodel_name_or_path=\"checkpoint\",\ndevice=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n)\n</code></pre>"},{"location":"fine_tune/sparse_embed/","title":"SparseEmbed","text":"<p>Training the Splade Model with PyTorch and Neural Cherche Library. The model is updated each time we call <code>train.train_sparse_embed</code> function. It's higly recommended to use a GPU and to use a Masked Language Model as the base model.</p> <pre><code>from neural_cherche import models, utils, train, losses\nimport torch\nmodel = models.SparseEmbed(\nmodel_name_or_path=\"raphaelsty/neural-cherche-sparse-embed\",\ndevice=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n)\noptimizer = torch.optim.AdamW(model.parameters(), lr=3e-6)\nflops_scheduler = losses.FlopsScheduler()\nX = [\n(\"query\", \"positive document\", \"negative document\"),\n(\"query\", \"positive document\", \"negative document\"),\n(\"query\", \"positive document\", \"negative document\"),\n]\nfor step, (anchor, positive, negative) in enumerate(utils.iter(\nX,\nepochs=2,\nbatch_size=32,\nshuffle=True\n)):\nloss = train.train_sparse_embed(\nmodel=model,\noptimizer=optimizer,\nanchor=anchor,\npositive=positive,\nnegative=negative,\nthreshold_flops=30,\nflops_loss_weight=flops_scheduler.get(),\nstep=step,\ngradient_accumulation_steps=50,\n)\nif (step + 1) % 1000 == 0:\n# Save the model every 1000 steps\nmodel.save_pretrained(\"checkpoint\")\n</code></pre> <p>We can load the checkpoint using:</p> <pre><code>from neural_cherche import models\nimport torch\nmodel = models.SparseEmbed(\nmodel_name_or_path=\"checkpoint\",\ndevice=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n)\n</code></pre>"},{"location":"fine_tune/splade/","title":"Splade","text":"<p>Training the Splade Model with PyTorch and Neural Cherche Library. The model is updated each time we call <code>train.train_splade</code> function. It's higly recommended to use a GPU and to use a Masked Language Model as the base model.</p> <pre><code>from neural_cherche import models, utils, train, losses\nimport torch\nmodel = models.Splade(\nmodel_name_or_path=\"raphaelsty/neural-cherche-sparse-embed\",\ndevice=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n)\noptimizer = torch.optim.AdamW(model.parameters(), lr=3e-6)\nflops_scheduler = losses.FlopsScheduler()\nX = [\n(\"query\", \"positive document\", \"negative document\"),\n(\"query\", \"positive document\", \"negative document\"),\n(\"query\", \"positive document\", \"negative document\"),\n]\nfor step, (anchor, positive, negative) in enumerate(utils.iter(\nX,\nepochs=2,\nbatch_size=32,\nshuffle=True\n)):\nloss = train.train_splade(\nmodel=model,\noptimizer=optimizer,\nanchor=anchor,\npositive=positive,\nnegative=negative,\nthreshold_flops=30,\nflops_loss_weight=flops_scheduler.get(),\nstep=step,\ngradient_accumulation_steps=50,\n)\nif (step + 1) % 1000 == 0:\n# Save the model every 1000 steps\nmodel.save_pretrained(\"checkpoint\")\n</code></pre> <p>We can load the checkpoint using:</p> <pre><code>from neural_cherche import models\nimport torch\nmodel = models.Splade(\nmodel_name_or_path=\"checkpoint\",\ndevice=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n)\n</code></pre>"},{"location":"pre_trained_models/pre_trained_models/","title":"Pre-trained Models","text":"<p>Neural-Cherche models such as ColBERT and SparseEmbed should be initialized with a pre-trained sentence-transformer model. The pre-trained models names can be found in the sentence-transformers documentation or on HuggingFace hub.</p> <p>After having selected a pre-trained checkpoint, we should fine-tune it on our dataset. If we don't wan't to fine-tune the model, we can use the <code>raphaelsty/neural-cherche-sparse-embed</code> and <code>raphaelsty/neural-cherche-colbert</code> checkpoints.</p>"},{"location":"pre_trained_models/pre_trained_models/#fine-tuning-models-on-scifact","title":"Fine-tuning models on Scifact","text":"<p>Here is a sample code to fine-tune ColBERT on the Scifact Dataset. If we plan to run this code, we should install neural-cherche with the following command:</p> <pre><code>pip install \"neural-cherche[eval]\"\n</code></pre> <p>There are other dataset available from the BEIR Benchmark which can be used with the <code>utils.load_beir</code> function such as <code>scifact</code>, <code>trec-covid</code>, <code>cord19</code>, <code>fiqa</code>, <code>hotpotqa</code>, <code>natural-questions</code>, <code>msmarco</code>, <code>eli5</code>, <code>quora</code>. Of course, we can use our own dataset by providing triples. Then, by building queries, documents and qrels, we can evaluate the model using the <code>utils.evaluate</code> function.</p> <pre><code>import torch\nfrom neural_cherche import models, rank, retrieve, train, utils\ndataset_name = \"scifact\"\ndocuments, queries, qrels = utils.load_beir(\ndataset_name=dataset_name,\nsplit=\"train\",\n)\nmodel = models.ColBERT(\nmodel_name_or_path=\"sentence-transformers/all-mpnet-base-v2\",\ndevice=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n)\noptimizer = torch.optim.AdamW(model.parameters(), lr=3e-5)\ntriples = utils.get_beir_triples(\nkey=\"id\", on=[\"title\", \"text\"], documents=documents, queries=queries, qrels=qrels\n)\n# Training loop\nbatch_size = 10\nepochs = 10\nfor step, (anchor, positive, negative) in enumerate(\nutils.iter(triples, epochs=epochs, batch_size=batch_size, shuffle=True)\n):\nloss = train.train_colbert(\nmodel=model,\noptimizer=optimizer,\nanchor=anchor,\npositive=positive,\nnegative=negative,\nstep=step,\ngradient_accumulation_steps=50,\n)\n# Eval the model every 512 steps\nif (step + 1) % 512 == 0:\ntest_documents, test_queries, qrels = utils.load_beir(\ndataset_name=dataset_name,\nsplit=\"test\",\n)\n# Setting up the retriever\nretriever = retrieve.BM25(\nkey=\"id\",\non=[\"title\", \"text\"],\n)\nretriever_documents_embeddings = retriever.encode_documents(\ndocuments=test_documents,\n)\nretriever.add(\ndocuments_embeddings=retriever_documents_embeddings,\n)\nqueries_embeddings = retriever.encode_queries(\nqueries=test_queries,\n)\ncandidates = retriever(\nqueries_embeddings=queries_embeddings,\nk=100,\n)\n# Setting up the ranker\nranker = rank.ColBERT(key=\"id\", on=[\"title\", \"text\"], model=model)\nranker_queries_embeddings = ranker.encode_queries(\nqueries=test_queries,\nbatch_size=batch_size,\n)\nranker_documents_embeddings = ranker.encode_candidates_documents(\ndocuments=test_documents,\ncandidates=candidates,\nbatch_size=batch_size,\n)\nscores = ranker(\ndocuments=candidates,\nqueries_embeddings=ranker_queries_embeddings,\ndocuments_embeddings=ranker_documents_embeddings,\nk=10,\n)\n# Evaluate the pipeline\nscores = utils.evaluate(\nscores=scores,\nqrels=qrels,\nqueries=test_queries,\nmetrics=[\"ndcg@10\"] + [f\"hits@{k}\" for k in range(1, 10)],\n)\nprint(scores)\nmodel.save_pretrained(\"colbert-scifact\")\n</code></pre>"},{"location":"retrieve/bm25/","title":"BM25","text":"<p>Retrieving documents using BM25. We must always encode documents before queries when  using BM25 retriever in order to fit the vectorizer, otherwise the system will raise an error. BM25 takes as input two distinct parameters, <code>b</code> and <code>k1</code>. b is a float value that determines the impact of document length normalization. The default value is <code>0.75</code>. The higher the value, the more penalized longer documents will be. k1 is a float value that determines how quickly the impact of term frequency saturates. The default value is <code>1.5</code>. The higher the value, the more influential term frequency will be.</p> <pre><code>from neural_cherche import retrieve\nfrom lenlp import sparse\ndocuments = [\n{\"id\": \"doc1\", \"title\": \"Paris\", \"text\": \"Paris is the capital of France.\"},\n{\"id\": \"doc2\", \"title\": \"Montreal\", \"text\": \"Montreal is the largest city in Quebec.\"},\n{\"id\": \"doc3\", \"title\": \"Bordeaux\", \"text\": \"Bordeaux in Southwestern France.\"},\n]\nretriever = retrieve.BM25(\nkey=\"id\",\non=[\"title\", \"text\"],\ncount_vectorizer=sparse.CountVectorizer(normalize=True, ngram_range=(3, 5), analyzer=\"char_wb\"),\nk1=1.5,\nb=0.75,\nepsilon=0.,\n)\ndocuments_embeddings = retriever.encode_documents(\ndocuments=documents,\n)\nretriever.add(\ndocuments_embeddings=documents_embeddings,\n)\n</code></pre> <p>Once we have created our index, we can use the retriever to retrieve the candidates.</p> <pre><code>queries = [\n\"What is the capital of France?\",\n\"What is the largest city in Quebec?\",\n\"Where is Bordeaux?\",\n]\nqueries_embeddings = retriever.encode_queries(\nqueries=queries,\n)\nscores = retriever(\nqueries_embeddings=queries_embeddings,\nk=100,\n)\nscores\n</code></pre> <pre><code>[[{'id': 'doc1', 'similarity': 88.86143220961094},\n{'id': 'doc2', 'similarity': 8.409232541918755},\n{'id': 'doc3', 'similarity': 7.134543210268021}],\n[{'id': 'doc2', 'similarity': 107.05374336242676},\n{'id': 'doc1', 'similarity': 9.28911879658699},\n{'id': 'doc3', 'similarity': 1.9025448560714722}],\n[{'id': 'doc3', 'similarity': 18.506150543689728},\n{'id': 'doc1', 'similarity': 0.7961864173412323},\n{'id': 'doc2', 'similarity': 0.7676786482334137}]]\n</code></pre>"},{"location":"retrieve/colbert/","title":"Colbert","text":"<p>It's higly recommended to use a GPU with ColBERT and to use a fine-tuned model as the base  model. </p> <p>ColBERT can either act as a retriever or as a ranker. While it has the capability to be employed as a retriever, its primary design and strength lie in ranking documents rather than retrieving them. The retriever's key objectives is to eliminate irrelevant documents among a large collection. On the other hand, the ranker is designed to re-order documents (more relevant first) among a small subset of document pre-filtered by a retriever.</p> <p>In practical terms, if your document collection is relatively small, ColBERT can effectively function as a retriever. However, if you're dealing with a big document collection, ColBERT is better suited to operate as a ranker. In this case, you should use a retriever such as TF IDF, BM25, Sentence Transformers, Splade to pre-filter the documents and then use ColBERT to re-rank the documents.</p>"},{"location":"retrieve/colbert/#colbert-ranker-with-bm25-retriever","title":"Colbert ranker with BM25 retriever","text":"<p>ColBERT ranker can be used to re-rank candidates in output of a retriever following the code below. We can use a TfIdf retriever, a Splade retriever or a SparseEmbed retriever.</p> <pre><code>from neural_cherche import models, rank, retrieve\nimport torch\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\" # or mps\nbatch_size = 32\ndocuments = [\n{\"id\": \"doc1\", \"title\": \"Paris\", \"text\": \"Paris is the capital of France.\"},\n{\"id\": \"doc2\", \"title\": \"Montreal\", \"text\": \"Montreal is the largest city in Quebec.\"},\n{\"id\": \"doc3\", \"title\": \"Bordeaux\", \"text\": \"Bordeaux in Southwestern France.\"},\n]\nretriever = retrieve.BM25(\nkey=\"id\",\non=[\"title\", \"text\"],\n)\nmodel = models.ColBERT(\nmodel_name_or_path=\"raphaelsty/neural-cherche-colbert\",\ndevice=device,\n)\nranker = rank.ColBERT(\nkey=\"id\",\non=[\"title\", \"text\"],\nmodel=model\n)\nretriever_documents_embeddings = retriever.encode_documents(\ndocuments=documents,\n)\nretriever.add(\ndocuments_embeddings=retriever_documents_embeddings,\n)\n</code></pre> <p>Once we have created our indexes, we can use the ranker to re-rank the candidates retrieved by the retriever.</p> <pre><code>queries = [\n\"What is the capital of France?\",\n\"What is the largest city in Quebec?\",\n\"Where is Bordeaux?\",\n]\nretriever_queries_embeddings = retriever.encode_queries(\nqueries=queries,\n)\nranker_queries_embeddings = ranker.encode_queries(\nqueries=queries,\nbatch_size=batch_size,\n)\ncandidates = retriever(\nqueries_embeddings=retriever_queries_embeddings,\nk=1000,\n)\n# Compute the embeddings of the candidates with the ranker model:\nranker_documents_embeddings = ranker.encode_candidates_documents(\ndocuments=documents,\ncandidates=candidates,\nbatch_size=batch_size,\n)\nscores = ranker(\ndocuments=candidates,\nqueries_embeddings=ranker_queries_embeddings,\ndocuments_embeddings=ranker_documents_embeddings,\nk=100,\nbatch_size=32,\n)\nscores\n</code></pre> <pre><code>[[{'id': 'doc1', 'similarity': 9.37690258026123},\n{'id': 'doc3', 'similarity': 6.458564758300781},\n{'id': 'doc2', 'similarity': 6.071964263916016}],\n[{'id': 'doc2', 'similarity': 10.65597915649414},\n{'id': 'doc3', 'similarity': 6.5705132484436035},\n{'id': 'doc1', 'similarity': 5.962393283843994}],\n[{'id': 'doc3', 'similarity': 6.877983570098877},\n{'id': 'doc2', 'similarity': 4.163510799407959},\n{'id': 'doc1', 'similarity': 3.5986523628234863}]]\n</code></pre> <p>Note, we could also use the <code>encode_documents</code> method which allows to pre-compute all the embeddings of the documents. This is useful when we have a large number of documents and we want to pre-compute the embeddings of the documents to save time later.</p> <pre><code>ranker_documents_embeddings = ranker.encode_documents(\ndocuments=documents,\nbatch_size=batch_size,\n)\n</code></pre>"},{"location":"retrieve/sparse_embed/","title":"SparseEmbed","text":""},{"location":"retrieve/sparse_embed/#sparseembed-retriever","title":"SparseEmbed retriever","text":"<p>Retrieving documents using SparseEmbed. SparseEmbed first retrieve documents following Splade procedure and then computes dot products of embeddings between common activated tokens.</p> <pre><code>from neural_cherche import models, retrieve\nimport torch\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nbatch_size = 32\nmodel = models.SparseEmbed(\nmodel_name_or_path=\"raphaelsty/neural-cherche-sparse-embed\",\ndevice=device,\n)\ndocuments = [\n{\"id\": \"doc1\", \"title\": \"Paris\", \"text\": \"Paris is the capital of France.\"},\n{\"id\": \"doc2\", \"title\": \"Montreal\", \"text\": \"Montreal is the largest city in Quebec.\"},\n{\"id\": \"doc3\", \"title\": \"Bordeaux\", \"text\": \"Bordeaux in Southwestern France.\"},\n]\nretriever = retrieve.SparseEmbed(\nkey=\"id\",\non=[\"title\", \"text\"],\nmodel=model\n)\ndocuments_embeddings = retriever.encode_documents(\ndocuments=documents,\nbatch_size=batch_size,\n)\nretriever.add(\ndocuments_embeddings=documents_embeddings,\n)\nqueries = [\n\"What is the capital of France?\",\n\"What is the largest city in Quebec?\",\n\"Where is Bordeaux?\",\n]\nqueries_embeddings = retriever.encode_queries(\nqueries=queries,\nbatch_size=batch_size,\n)\nscores = retriever(\nqueries_embeddings=queries_embeddings,\nk=100,\n)\nscores\n</code></pre> <pre><code>[[{'id': 'doc1', 'similarity': 144.48985290527344},\n{'id': 'doc2', 'similarity': 111.0398941040039},\n{'id': 'doc3', 'similarity': 80.72007751464844}],\n[{'id': 'doc2', 'similarity': 169.8221435546875},\n{'id': 'doc1', 'similarity': 125.84573364257812},\n{'id': 'doc3', 'similarity': 77.57147216796875}],\n[{'id': 'doc1', 'similarity': 103.0795669555664},\n{'id': 'doc2', 'similarity': 81.4903564453125},\n{'id': 'doc3', 'similarity': 77.25212097167969}]]\n</code></pre>"},{"location":"retrieve/sparse_embed/#sparseembed-ranker","title":"SparseEmbed ranker","text":"<p>Ranking documents using SparseEmbed. The following code use BM25 to retrieve documents and then rank them using SparseEmbed.</p> <pre><code>from neural_cherche import models, rank, retrieve\nimport torch\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nbatch_size = 32\ndocuments = [\n{\"id\": \"doc1\", \"title\": \"Paris\", \"text\": \"Paris is the capital of France.\"},\n{\"id\": \"doc2\", \"title\": \"Montreal\", \"text\": \"Montreal is the largest city in Quebec.\"},\n{\"id\": \"doc3\", \"title\": \"Bordeaux\", \"text\": \"Bordeaux in Southwestern France.\"},\n]\nretriever = retrieve.BM25(\nkey=\"id\",\non=[\"title\", \"text\"],\n)\nmodel = models.SparseEmbed(\nmodel_name_or_path=\"raphaelsty/neural-cherche-sparse-embed\",\ndevice=device,\n)\nranker = rank.SparseEmbed(\nkey=\"id\",\non=[\"title\", \"text\"],\nmodel=model\n)\nretriever_documents_embeddings = retriever.encode_documents(\ndocuments=documents,\n)\nretriever.add(\ndocuments_embeddings=retriever_documents_embeddings,\n)\nranker_documents_embeddings = ranker.encode_documents(\ndocuments=documents,\nbatch_size=batch_size,\n)\n</code></pre> <p>Once we have created our indexes, we can use the ranker to re-rank the candidates retrieved by the retriever.</p> <pre><code>queries = [\n\"What is the capital of France?\",\n\"What is the largest city in Quebec?\",\n\"Where is Bordeaux?\",\n]\nretriever_queries_embeddings = retriever.encode_queries(\nqueries=queries,\n)\nranker_queries_embeddings = ranker.encode_queries(\nqueries=queries,\nbatch_size=batch_size,\n)\ncandidates = retriever(\nqueries_embeddings=retriever_queries_embeddings,\nk=1000,\n)\nscores = ranker(\ndocuments=candidates,\nqueries_embeddings=ranker_queries_embeddings,\ndocuments_embeddings=ranker_documents_embeddings,\nk=100,\nbatch_size=32,\n)\nscores\n</code></pre> <pre><code>[[{'id': 'doc1', 'similarity': 450.2735},\n{'id': 'doc3', 'similarity': 184.59885},\n{'id': 'doc2', 'similarity': 98.53701}],\n[{'id': 'doc2', 'similarity': 391.74216},\n{'id': 'doc1', 'similarity': 111.45184},\n{'id': 'doc3', 'similarity': 51.19094}],\n[{'id': 'doc3', 'similarity': 349.82397},\n{'id': 'doc1', 'similarity': 74.993576},\n{'id': 'doc2', 'similarity': 33.37598}]]\n</code></pre>"},{"location":"retrieve/splade/","title":"Splade","text":"<p>Retrieving documents using Splade. Splade activations are stored in a sparse matrix.</p> <pre><code>from neural_cherche import models, retrieve\nimport torch\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nbatch_size = 32\nmodel = models.Splade(\nmodel_name_or_path=\"raphaelsty/neural-cherche-sparse-embed\",\ndevice=device,\n)\ndocuments = [\n{\"id\": \"doc1\", \"title\": \"Paris\", \"text\": \"Paris is the capital of France.\"},\n{\"id\": \"doc2\", \"title\": \"Montreal\", \"text\": \"Montreal is the largest city in Quebec.\"},\n{\"id\": \"doc3\", \"title\": \"Bordeaux\", \"text\": \"Bordeaux in Southwestern France.\"},\n]\nretriever = retrieve.Splade(\nkey=\"id\",\non=[\"title\", \"text\"],\nmodel=model\n)\ndocuments_embeddings = retriever.encode_documents(\ndocuments=documents,\nbatch_size=batch_size,\n)\nretriever.add(\ndocuments_embeddings=documents_embeddings,\n)\nqueries = [\n\"What is the capital of France?\",\n\"What is the largest city in Quebec?\",\n\"Where is Bordeaux?\",\n]\nqueries_embeddings = retriever.encode_queries(\nqueries=queries,\nbatch_size=batch_size,\n)\nscores = retriever(\nqueries_embeddings=queries_embeddings,\nk=100,\n)\nscores\n</code></pre> <pre><code>[[{'id': 'doc1', 'similarity': 509.41132},\n{'id': 'doc2', 'similarity': 416.7586},\n{'id': 'doc3', 'similarity': 391.12656}],\n[{'id': 'doc2', 'similarity': 539.30164},\n{'id': 'doc1', 'similarity': 438.1915},\n{'id': 'doc3', 'similarity': 366.35565}],\n[{'id': 'doc3', 'similarity': 402.1179},\n{'id': 'doc1', 'similarity': 382.23434},\n{'id': 'doc2', 'similarity': 357.77188}]]\n</code></pre>"},{"location":"retrieve/tfidf/","title":"TfIdf","text":"<p>Retrieving documents using TfIdf. We must always encode documents before queries when  using TfIdf retriever to fit the vectorizer, otherwise the system will raise an error.</p> <pre><code>from neural_cherche import retrieve\nfrom lenlp import sparse\ndocuments = [\n{\"id\": \"doc1\", \"title\": \"Paris\", \"text\": \"Paris is the capital of France.\"},\n{\"id\": \"doc2\", \"title\": \"Montreal\", \"text\": \"Montreal is the largest city in Quebec.\"},\n{\"id\": \"doc3\", \"title\": \"Bordeaux\", \"text\": \"Bordeaux in Southwestern France.\"},\n]\nretriever = retrieve.TfIdf(\nkey=\"id\",\non=[\"title\", \"text\"],\ntfidf=sparse.TfidfVectorizer(normalize=True, ngram_range=(3, 5), analyzer=\"char_wb\"),\n)\ndocuments_embeddings = retriever.encode_documents(\ndocuments=documents,\n)\nretriever.add(\ndocuments_embeddings=documents_embeddings,\n)\n</code></pre> <p>Once we have created our index, we can use the retriever to retrieve the candidates.</p> <pre><code>queries = [\n\"What is the capital of France?\",\n\"What is the largest city in Quebec?\",\n\"Where is Bordeaux?\",\n]\nqueries_embeddings = retriever.encode_queries(\nqueries=queries,\n)\nscores = retriever(\nqueries_embeddings=queries_embeddings,\nk=100,\n)\nscores\n</code></pre> <pre><code>[[{'id': 'doc1', 'similarity': 0.7398676589821398},\n{'id': 'doc2', 'similarity': 0.0835572631633293},\n{'id': 'doc3', 'similarity': 0.0610449729335254}],\n[{'id': 'doc2', 'similarity': 0.681400067648393},\n{'id': 'doc1', 'similarity': 0.08957331010686152},\n{'id': 'doc3', 'similarity': 0.014090768382163084}],\n[{'id': 'doc3', 'similarity': 0.5803551728233277},\n{'id': 'doc1', 'similarity': 0.043108258977414556},\n{'id': 'doc2', 'similarity': 0.02088494592973491}]]\n</code></pre>"},{"location":"scoring/scoring/","title":"Scoring","text":"<p>We can compute similarity between queries and documents without using a retriever using  the <code>scores</code> method.</p>"},{"location":"scoring/scoring/#colbert","title":"ColBERT","text":"<pre><code>import torch\nfrom neural_cherche import models\nmodel = models.ColBERT(\nmodel_name_or_path=\"raphaelsty/neural-cherche-colbert\",\ndevice=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n)\nmodel.scores(\nqueries=[\n\"What is the capital of France?\",\n\"What is the largest city in Quebec?\",\n\"Where is Bordeaux?\",\n],\ndocuments=[\n\"Paris is the capital of France.\",\n\"Montreal is the largest city in Quebec.\",\n\"Bordeaux in Southwestern France.\",\n],\nbatch_size=32,\n)\n</code></pre> <pre><code>tensor([20.6498, 26.2132, 23.7048])\n</code></pre>"},{"location":"scoring/scoring/#splade","title":"Splade","text":"<pre><code>import torch\nfrom neural_cherche import models\nmodel = models.Splade(\nmodel_name_or_path=\"raphaelsty/neural-cherche-sparse-embed\",\ndevice=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n)\nmodel.scores(\nqueries=[\n\"What is the capital of France?\",\n\"What is the largest city in Quebec?\",\n\"Where is Bordeaux?\",\n],\ndocuments=[\n\"Paris is the capital of France.\",\n\"Montreal is the largest city in Quebec.\",\n\"Bordeaux in Southwestern France.\",\n],\nbatch_size=32,\n)\n</code></pre> <pre><code>tensor([517.9335, 526.4659, 395.0022])\n</code></pre>"},{"location":"scoring/scoring/#sparseembed","title":"SparseEmbed","text":"<pre><code>import torch\nfrom neural_cherche import models\nmodel = models.SparseEmbed(\nmodel_name_or_path=\"raphaelsty/neural-cherche-sparse-embed\",\ndevice=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n)\nmodel.scores(\nqueries=[\n\"What is the capital of France?\",\n\"What is the largest city in Quebec?\",\n\"Where is Bordeaux?\",\n],\ndocuments=[\n\"Paris is the capital of France.\",\n\"Montreal is the largest city in Quebec.\",\n\"Bordeaux in Southwestern France.\",\n],\nbatch_size=32,\n)\n</code></pre> <pre><code>tensor([150.6469, 161.5140, 109.4595])\n</code></pre>"},{"location":"scripts/","title":"Index","text":"In\u00a0[\u00a0]: Copied! <pre>\"\"\"This script is responsible for building the API reference. The API reference is located in\ndocs/api. The script scans through all the modules, classes, and functions. It processes\nthe __doc__ of each object and formats it so that MkDocs can process it in turn.\n\"\"\"\nimport functools\nimport importlib\nimport inspect\nimport os\nimport pathlib\nimport re\nimport shutil\n</pre> \"\"\"This script is responsible for building the API reference. The API reference is located in docs/api. The script scans through all the modules, classes, and functions. It processes the __doc__ of each object and formats it so that MkDocs can process it in turn. \"\"\" import functools import importlib import inspect import os import pathlib import re import shutil In\u00a0[\u00a0]: Copied! <pre>from numpydoc.docscrape import ClassDoc, FunctionDoc\n</pre> from numpydoc.docscrape import ClassDoc, FunctionDoc In\u00a0[\u00a0]: Copied! <pre>package = \"neural_cherche\"\n</pre> package = \"neural_cherche\" <p>shutil.copy(\"README.md\", \"docs/index.md\")</p> In\u00a0[\u00a0]: Copied! <pre>def paragraph(text):\n    return f\"{text}\\n\"\n</pre> def paragraph(text):     return f\"{text}\\n\" In\u00a0[\u00a0]: Copied! <pre>def h1(text):\n    return paragraph(f\"# {text}\")\n</pre> def h1(text):     return paragraph(f\"# {text}\") In\u00a0[\u00a0]: Copied! <pre>def h2(text):\n    return paragraph(f\"## {text}\")\n</pre> def h2(text):     return paragraph(f\"## {text}\") In\u00a0[\u00a0]: Copied! <pre>def h3(text):\n    return paragraph(f\"### {text}\")\n</pre> def h3(text):     return paragraph(f\"### {text}\") In\u00a0[\u00a0]: Copied! <pre>def h4(text):\n    return paragraph(f\"#### {text}\")\n</pre> def h4(text):     return paragraph(f\"#### {text}\") In\u00a0[\u00a0]: Copied! <pre>def link(caption, href):\n    return f\"[{caption}]({href})\"\n</pre> def link(caption, href):     return f\"[{caption}]({href})\" In\u00a0[\u00a0]: Copied! <pre>def code(text):\n    return f\"`{text}`\"\n</pre> def code(text):     return f\"`{text}`\" In\u00a0[\u00a0]: Copied! <pre>def li(text):\n    return f\"- {text}\\n\"\n</pre> def li(text):     return f\"- {text}\\n\" In\u00a0[\u00a0]: Copied! <pre>def snake_to_kebab(text):\n    return text.replace(\"_\", \"-\")\n</pre> def snake_to_kebab(text):     return text.replace(\"_\", \"-\") In\u00a0[\u00a0]: Copied! <pre>def inherit_docstring(c, meth):\n\"\"\"Since Python 3.5, inspect.getdoc is supposed to return the docstring from a parent class\n    if a class has none. However this doesn't seem to work for Cython classes.\n    \"\"\"\n\n    doc = None\n\n    for ancestor in inspect.getmro(c):\n        try:\n            ancestor_meth = getattr(ancestor, meth)\n        except AttributeError:\n            break\n        doc = inspect.getdoc(ancestor_meth)\n        if doc:\n            break\n\n    return doc\n</pre> def inherit_docstring(c, meth):     \"\"\"Since Python 3.5, inspect.getdoc is supposed to return the docstring from a parent class     if a class has none. However this doesn't seem to work for Cython classes.     \"\"\"      doc = None      for ancestor in inspect.getmro(c):         try:             ancestor_meth = getattr(ancestor, meth)         except AttributeError:             break         doc = inspect.getdoc(ancestor_meth)         if doc:             break      return doc In\u00a0[\u00a0]: Copied! <pre>def inherit_signature(c, method_name):\n    m = getattr(c, method_name)\n    sig = inspect.signature(m)\n\n    params = []\n\n    for param in sig.parameters.values():\n        if param.name == \"self\" or param.annotation is not param.empty:\n            params.append(param)\n            continue\n\n        for ancestor in inspect.getmro(c):\n            try:\n                ancestor_meth = inspect.signature(getattr(ancestor, m.__name__))\n            except AttributeError:\n                break\n            try:\n                ancestor_param = ancestor_meth.parameters[param.name]\n            except KeyError:\n                break\n            if ancestor_param.annotation is not param.empty:\n                param = param.replace(annotation=ancestor_param.annotation)\n                break\n\n        params.append(param)\n\n    return_annotation = sig.return_annotation\n    if return_annotation is inspect._empty:\n        for ancestor in inspect.getmro(c):\n            try:\n                ancestor_meth = inspect.signature(getattr(ancestor, m.__name__))\n            except AttributeError:\n                break\n            if ancestor_meth.return_annotation is not inspect._empty:\n                return_annotation = ancestor_meth.return_annotation\n                break\n\n    return sig.replace(parameters=params, return_annotation=return_annotation)\n</pre> def inherit_signature(c, method_name):     m = getattr(c, method_name)     sig = inspect.signature(m)      params = []      for param in sig.parameters.values():         if param.name == \"self\" or param.annotation is not param.empty:             params.append(param)             continue          for ancestor in inspect.getmro(c):             try:                 ancestor_meth = inspect.signature(getattr(ancestor, m.__name__))             except AttributeError:                 break             try:                 ancestor_param = ancestor_meth.parameters[param.name]             except KeyError:                 break             if ancestor_param.annotation is not param.empty:                 param = param.replace(annotation=ancestor_param.annotation)                 break          params.append(param)      return_annotation = sig.return_annotation     if return_annotation is inspect._empty:         for ancestor in inspect.getmro(c):             try:                 ancestor_meth = inspect.signature(getattr(ancestor, m.__name__))             except AttributeError:                 break             if ancestor_meth.return_annotation is not inspect._empty:                 return_annotation = ancestor_meth.return_annotation                 break      return sig.replace(parameters=params, return_annotation=return_annotation) In\u00a0[\u00a0]: Copied! <pre>def snake_to_kebab(snake: str) -&gt; str:\n    return snake.replace(\"_\", \"-\")\n</pre> def snake_to_kebab(snake: str) -&gt; str:     return snake.replace(\"_\", \"-\") In\u00a0[\u00a0]: Copied! <pre>def pascal_to_kebab(string):\n    string = re.sub(\"(.)([A-Z][a-z]+)\", r\"\\1-\\2\", string)\n    string = re.sub(\"(.)([0-9]+)\", r\"\\1-\\2\", string)\n    return re.sub(\"([a-z0-9])([A-Z])\", r\"\\1-\\2\", string).lower()\n</pre> def pascal_to_kebab(string):     string = re.sub(\"(.)([A-Z][a-z]+)\", r\"\\1-\\2\", string)     string = re.sub(\"(.)([0-9]+)\", r\"\\1-\\2\", string)     return re.sub(\"([a-z0-9])([A-Z])\", r\"\\1-\\2\", string).lower() In\u00a0[\u00a0]: Copied! <pre>class Linkifier:\n    def __init__(self):\n        path_index = {}\n        name_index = {}\n\n        modules = {\n            module: importlib.import_module(f\"{package}.{module}\")\n            for module in importlib.import_module(f\"{package}\").__all__\n        }\n\n        def index_module(mod_name, mod, path):\n            path = os.path.join(path, mod_name)\n            dotted_path = path.replace(\"/\", \".\")\n\n            for func_name, func in inspect.getmembers(mod, inspect.isfunction):\n                for e in (\n                    f\"{mod_name}.{func_name}\",\n                    f\"{dotted_path}.{func_name}\",\n                    f\"{func.__module__}.{func_name}\",\n                ):\n                    path_index[e] = os.path.join(path, snake_to_kebab(func_name))\n                    name_index[e] = f\"{dotted_path}.{func_name}\"\n\n            for klass_name, klass in inspect.getmembers(mod, inspect.isclass):\n                for e in (\n                    f\"{mod_name}.{klass_name}\",\n                    f\"{dotted_path}.{klass_name}\",\n                    f\"{klass.__module__}.{klass_name}\",\n                ):\n                    path_index[e] = os.path.join(path, klass_name)\n                    name_index[e] = f\"{dotted_path}.{klass_name}\"\n\n            for submod_name, submod in inspect.getmembers(mod, inspect.ismodule):\n                if submod_name not in mod.__all__ or submod_name == \"typing\":\n                    continue\n                for e in (f\"{mod_name}.{submod_name}\", f\"{dotted_path}.{submod_name}\"):\n                    path_index[e] = os.path.join(path, snake_to_kebab(submod_name))\n\n                # Recurse\n                index_module(submod_name, submod, path=path)\n\n        for mod_name, mod in modules.items():\n            index_module(mod_name, mod, path=\"\")\n\n        # Prepend {package} to each index entry\n        for k in list(path_index.keys()):\n            path_index[f\"{package}.{k}\"] = path_index[k]\n        for k in list(name_index.keys()):\n            name_index[f\"{package}.{k}\"] = name_index[k]\n\n        self.path_index = path_index\n        self.name_index = name_index\n\n    def linkify(self, text, use_fences, depth):\n        path = self.path_index.get(text)\n        name = self.name_index.get(text)\n        if path and name:\n            backwards = \"../\" * (depth + 1)\n            if use_fences:\n                return f\"[`{name}`]({backwards}{path})\"\n            return f\"[{name}]({backwards}{path})\"\n        return None\n\n    def linkify_fences(self, text, depth):\n        between_fences = re.compile(\"`[\\w\\.]+\\.\\w+`\")\n        return between_fences.sub(\n            lambda x: self.linkify(x.group().strip(\"`\"), True, depth) or x.group(), text\n        )\n\n    def linkify_dotted(self, text, depth):\n        dotted = re.compile(\"\\w+\\.[\\.\\w]+\")\n        return dotted.sub(\n            lambda x: self.linkify(x.group(), False, depth) or x.group(), text\n        )\n</pre> class Linkifier:     def __init__(self):         path_index = {}         name_index = {}          modules = {             module: importlib.import_module(f\"{package}.{module}\")             for module in importlib.import_module(f\"{package}\").__all__         }          def index_module(mod_name, mod, path):             path = os.path.join(path, mod_name)             dotted_path = path.replace(\"/\", \".\")              for func_name, func in inspect.getmembers(mod, inspect.isfunction):                 for e in (                     f\"{mod_name}.{func_name}\",                     f\"{dotted_path}.{func_name}\",                     f\"{func.__module__}.{func_name}\",                 ):                     path_index[e] = os.path.join(path, snake_to_kebab(func_name))                     name_index[e] = f\"{dotted_path}.{func_name}\"              for klass_name, klass in inspect.getmembers(mod, inspect.isclass):                 for e in (                     f\"{mod_name}.{klass_name}\",                     f\"{dotted_path}.{klass_name}\",                     f\"{klass.__module__}.{klass_name}\",                 ):                     path_index[e] = os.path.join(path, klass_name)                     name_index[e] = f\"{dotted_path}.{klass_name}\"              for submod_name, submod in inspect.getmembers(mod, inspect.ismodule):                 if submod_name not in mod.__all__ or submod_name == \"typing\":                     continue                 for e in (f\"{mod_name}.{submod_name}\", f\"{dotted_path}.{submod_name}\"):                     path_index[e] = os.path.join(path, snake_to_kebab(submod_name))                  # Recurse                 index_module(submod_name, submod, path=path)          for mod_name, mod in modules.items():             index_module(mod_name, mod, path=\"\")          # Prepend {package} to each index entry         for k in list(path_index.keys()):             path_index[f\"{package}.{k}\"] = path_index[k]         for k in list(name_index.keys()):             name_index[f\"{package}.{k}\"] = name_index[k]          self.path_index = path_index         self.name_index = name_index      def linkify(self, text, use_fences, depth):         path = self.path_index.get(text)         name = self.name_index.get(text)         if path and name:             backwards = \"../\" * (depth + 1)             if use_fences:                 return f\"[`{name}`]({backwards}{path})\"             return f\"[{name}]({backwards}{path})\"         return None      def linkify_fences(self, text, depth):         between_fences = re.compile(\"`[\\w\\.]+\\.\\w+`\")         return between_fences.sub(             lambda x: self.linkify(x.group().strip(\"`\"), True, depth) or x.group(), text         )      def linkify_dotted(self, text, depth):         dotted = re.compile(\"\\w+\\.[\\.\\w]+\")         return dotted.sub(             lambda x: self.linkify(x.group(), False, depth) or x.group(), text         ) In\u00a0[\u00a0]: Copied! <pre>def concat_lines(lines):\n    return inspect.cleandoc(\" \".join(\"\\n\\n\" if line == \"\" else line for line in lines))\n</pre> def concat_lines(lines):     return inspect.cleandoc(\" \".join(\"\\n\\n\" if line == \"\" else line for line in lines)) In\u00a0[\u00a0]: Copied! <pre>def print_docstring(obj, file, depth):\n\"\"\"Prints a classes's docstring to a file.\"\"\"\n\n    doc = ClassDoc(obj) if inspect.isclass(obj) else FunctionDoc(obj)\n\n    printf = functools.partial(print, file=file)\n\n    printf(h1(obj.__name__))\n    printf(linkifier.linkify_fences(paragraph(concat_lines(doc[\"Summary\"])), depth))\n    printf(\n        linkifier.linkify_fences(\n            paragraph(concat_lines(doc[\"Extended Summary\"])), depth\n        )\n    )\n\n    # We infer the type annotations from the signatures, and therefore rely on the signature\n    # instead of the docstring for documenting parameters\n    try:\n        signature = inspect.signature(obj)\n    except ValueError:\n        signature = (\n            inspect.Signature()\n        )  # TODO: this is necessary for Cython classes, but it's not correct\n    params_desc = {param.name: \" \".join(param.desc) for param in doc[\"Parameters\"]}\n\n    # Parameters\n    if signature.parameters:\n        printf(h2(\"Parameters\"))\n    for param in signature.parameters.values():\n        # Name\n        printf(f\"- **{param.name}**\", end=\"\")\n        # Type annotation\n        if param.annotation is not param.empty:\n            anno = inspect.formatannotation(param.annotation)\n            anno = linkifier.linkify_dotted(anno, depth)\n            printf(f\" (*{anno}*)\", end=\"\")\n        # Default value\n        if param.default is not param.empty:\n            printf(f\" \u2013 defaults to `{param.default}`\", end=\"\")\n        printf(\"\\n\", file=file)\n        # Description\n        if param.name in params_desc:\n            desc = params_desc[param.name]\n            if desc:\n                printf(f\"    {desc}\\n\")\n    printf(\"\")\n\n    # Attributes\n    if doc[\"Attributes\"]:\n        printf(h2(\"Attributes\"))\n    for attr in doc[\"Attributes\"]:\n        # Name\n        printf(f\"- **{attr.name}**\", end=\"\")\n        # Type annotation\n        if attr.type:\n            printf(f\" (*{attr.type}*)\", end=\"\")\n        printf(\"\\n\", file=file)\n        # Description\n        desc = \" \".join(attr.desc)\n        if desc:\n            printf(f\"    {desc}\\n\")\n    printf(\"\")\n\n    # Examples\n    if doc[\"Examples\"]:\n        printf(h2(\"Examples\"))\n\n        in_code = False\n        after_space = False\n\n        for line in inspect.cleandoc(\"\\n\".join(doc[\"Examples\"])).splitlines():\n            if (\n                in_code\n                and after_space\n                and line\n                and not line.startswith(\"&gt;&gt;&gt;\")\n                and not line.startswith(\"...\")\n            ):\n                printf(\"```\\n\")\n                in_code = False\n                after_space = False\n\n            if not in_code and line.startswith(\"&gt;&gt;&gt;\"):\n                printf(\"```python\")\n                in_code = True\n\n            after_space = False\n            if not line:\n                after_space = True\n\n            printf(line)\n\n        if in_code:\n            printf(\"```\")\n    printf(\"\")\n\n    # Methods\n    if inspect.isclass(obj) and doc[\"Methods\"]:\n        printf(h2(\"Methods\"))\n        printf_indent = lambda x, **kwargs: printf(f\"    {x}\", **kwargs)\n\n        for meth in doc[\"Methods\"]:\n            printf(paragraph(f'???- note \"{meth.name}\"'))\n\n            # Parse method docstring\n            docstring = inherit_docstring(c=obj, meth=meth.name)\n            if not docstring:\n                continue\n            meth_doc = FunctionDoc(func=None, doc=docstring)\n\n            printf_indent(paragraph(\" \".join(meth_doc[\"Summary\"])))\n            if meth_doc[\"Extended Summary\"]:\n                printf_indent(paragraph(\" \".join(meth_doc[\"Extended Summary\"])))\n\n            # We infer the type annotations from the signatures, and therefore rely on the signature\n            # instead of the docstring for documenting parameters\n            signature = inherit_signature(obj, meth.name)\n            params_desc = {\n                param.name: \" \".join(param.desc) for param in doc[\"Parameters\"]\n            }\n\n            # Parameters\n            if (\n                len(signature.parameters) &gt; 1\n            ):  # signature is never empty, but self doesn't count\n                printf_indent(\"**Parameters**\\n\")\n            for param in signature.parameters.values():\n                if param.name == \"self\":\n                    continue\n                # Name\n                printf_indent(f\"- **{param.name}**\", end=\"\")\n                # Type annotation\n                if param.annotation is not param.empty:\n                    printf_indent(\n                        f\" (*{inspect.formatannotation(param.annotation)}*)\", end=\"\"\n                    )\n                # Default value\n                if param.default is not param.empty:\n                    printf_indent(f\" \u2013 defaults to `{param.default}`\", end=\"\")\n                printf_indent(\"\", file=file)\n                # Description\n                desc = params_desc.get(param.name)\n                if desc:\n                    printf_indent(f\"    {desc}\")\n            printf_indent(\"\")\n\n            # Returns\n            if meth_doc[\"Returns\"]:\n                printf_indent(\"**Returns**\\n\")\n                return_val = meth_doc[\"Returns\"][0]\n                if signature.return_annotation is not inspect._empty:\n                    if inspect.isclass(signature.return_annotation):\n                        printf_indent(\n                            f\"*{signature.return_annotation.__name__}*: \", end=\"\"\n                        )\n                    else:\n                        printf_indent(f\"*{signature.return_annotation}*: \", end=\"\")\n                printf_indent(return_val.type)\n                printf_indent(\"\")\n\n    # Notes\n    if doc[\"Notes\"]:\n        printf(h2(\"Notes\"))\n        printf(paragraph(\"\\n\".join(doc[\"Notes\"])))\n\n    # References\n    if doc[\"References\"]:\n        printf(h2(\"References\"))\n        printf(paragraph(\"\\n\".join(doc[\"References\"])))\n</pre> def print_docstring(obj, file, depth):     \"\"\"Prints a classes's docstring to a file.\"\"\"      doc = ClassDoc(obj) if inspect.isclass(obj) else FunctionDoc(obj)      printf = functools.partial(print, file=file)      printf(h1(obj.__name__))     printf(linkifier.linkify_fences(paragraph(concat_lines(doc[\"Summary\"])), depth))     printf(         linkifier.linkify_fences(             paragraph(concat_lines(doc[\"Extended Summary\"])), depth         )     )      # We infer the type annotations from the signatures, and therefore rely on the signature     # instead of the docstring for documenting parameters     try:         signature = inspect.signature(obj)     except ValueError:         signature = (             inspect.Signature()         )  # TODO: this is necessary for Cython classes, but it's not correct     params_desc = {param.name: \" \".join(param.desc) for param in doc[\"Parameters\"]}      # Parameters     if signature.parameters:         printf(h2(\"Parameters\"))     for param in signature.parameters.values():         # Name         printf(f\"- **{param.name}**\", end=\"\")         # Type annotation         if param.annotation is not param.empty:             anno = inspect.formatannotation(param.annotation)             anno = linkifier.linkify_dotted(anno, depth)             printf(f\" (*{anno}*)\", end=\"\")         # Default value         if param.default is not param.empty:             printf(f\" \u2013 defaults to `{param.default}`\", end=\"\")         printf(\"\\n\", file=file)         # Description         if param.name in params_desc:             desc = params_desc[param.name]             if desc:                 printf(f\"    {desc}\\n\")     printf(\"\")      # Attributes     if doc[\"Attributes\"]:         printf(h2(\"Attributes\"))     for attr in doc[\"Attributes\"]:         # Name         printf(f\"- **{attr.name}**\", end=\"\")         # Type annotation         if attr.type:             printf(f\" (*{attr.type}*)\", end=\"\")         printf(\"\\n\", file=file)         # Description         desc = \" \".join(attr.desc)         if desc:             printf(f\"    {desc}\\n\")     printf(\"\")      # Examples     if doc[\"Examples\"]:         printf(h2(\"Examples\"))          in_code = False         after_space = False          for line in inspect.cleandoc(\"\\n\".join(doc[\"Examples\"])).splitlines():             if (                 in_code                 and after_space                 and line                 and not line.startswith(\"&gt;&gt;&gt;\")                 and not line.startswith(\"...\")             ):                 printf(\"```\\n\")                 in_code = False                 after_space = False              if not in_code and line.startswith(\"&gt;&gt;&gt;\"):                 printf(\"```python\")                 in_code = True              after_space = False             if not line:                 after_space = True              printf(line)          if in_code:             printf(\"```\")     printf(\"\")      # Methods     if inspect.isclass(obj) and doc[\"Methods\"]:         printf(h2(\"Methods\"))         printf_indent = lambda x, **kwargs: printf(f\"    {x}\", **kwargs)          for meth in doc[\"Methods\"]:             printf(paragraph(f'???- note \"{meth.name}\"'))              # Parse method docstring             docstring = inherit_docstring(c=obj, meth=meth.name)             if not docstring:                 continue             meth_doc = FunctionDoc(func=None, doc=docstring)              printf_indent(paragraph(\" \".join(meth_doc[\"Summary\"])))             if meth_doc[\"Extended Summary\"]:                 printf_indent(paragraph(\" \".join(meth_doc[\"Extended Summary\"])))              # We infer the type annotations from the signatures, and therefore rely on the signature             # instead of the docstring for documenting parameters             signature = inherit_signature(obj, meth.name)             params_desc = {                 param.name: \" \".join(param.desc) for param in doc[\"Parameters\"]             }              # Parameters             if (                 len(signature.parameters) &gt; 1             ):  # signature is never empty, but self doesn't count                 printf_indent(\"**Parameters**\\n\")             for param in signature.parameters.values():                 if param.name == \"self\":                     continue                 # Name                 printf_indent(f\"- **{param.name}**\", end=\"\")                 # Type annotation                 if param.annotation is not param.empty:                     printf_indent(                         f\" (*{inspect.formatannotation(param.annotation)}*)\", end=\"\"                     )                 # Default value                 if param.default is not param.empty:                     printf_indent(f\" \u2013 defaults to `{param.default}`\", end=\"\")                 printf_indent(\"\", file=file)                 # Description                 desc = params_desc.get(param.name)                 if desc:                     printf_indent(f\"    {desc}\")             printf_indent(\"\")              # Returns             if meth_doc[\"Returns\"]:                 printf_indent(\"**Returns**\\n\")                 return_val = meth_doc[\"Returns\"][0]                 if signature.return_annotation is not inspect._empty:                     if inspect.isclass(signature.return_annotation):                         printf_indent(                             f\"*{signature.return_annotation.__name__}*: \", end=\"\"                         )                     else:                         printf_indent(f\"*{signature.return_annotation}*: \", end=\"\")                 printf_indent(return_val.type)                 printf_indent(\"\")      # Notes     if doc[\"Notes\"]:         printf(h2(\"Notes\"))         printf(paragraph(\"\\n\".join(doc[\"Notes\"])))      # References     if doc[\"References\"]:         printf(h2(\"References\"))         printf(paragraph(\"\\n\".join(doc[\"References\"]))) In\u00a0[\u00a0]: Copied! <pre>def print_module(mod, path, overview, is_submodule=False):\n    mod_name = mod.__name__.split(\".\")[-1]\n\n    # Create a directory for the module\n    mod_slug = snake_to_kebab(mod_name)\n    mod_path = path.joinpath(mod_slug)\n    mod_short_path = str(mod_path).replace(\"docs/api/\", \"\")\n    os.makedirs(mod_path, exist_ok=True)\n    with open(mod_path.joinpath(\".pages\"), \"w\") as f:\n        f.write(f\"title: {mod_name}\")\n\n    # Add the module to the overview\n    if is_submodule:\n        print(h3(mod_name), file=overview)\n    else:\n        print(h2(mod_name), file=overview)\n    if mod.__doc__:\n        print(paragraph(mod.__doc__), file=overview)\n\n    # Extract all public classes and functions\n    ispublic = lambda x: x.__name__ in mod.__all__ and not x.__name__.startswith(\"_\")\n    classes = inspect.getmembers(mod, lambda x: inspect.isclass(x) and ispublic(x))\n    funcs = inspect.getmembers(mod, lambda x: inspect.isfunction(x) and ispublic(x))\n\n    # Classes\n\n    if classes and funcs:\n        print(\"\\n**Classes**\\n\", file=overview)\n\n    for _, c in classes:\n        print(f\"{mod_name}.{c.__name__}\")\n\n        # Add the class to the overview\n        slug = snake_to_kebab(c.__name__)\n        print(\n            li(link(c.__name__, f\"../{mod_short_path}/{slug}\")), end=\"\", file=overview\n        )\n\n        # Write down the class' docstring\n        with open(mod_path.joinpath(slug).with_suffix(\".md\"), \"w\") as file:\n            print_docstring(obj=c, file=file, depth=mod_short_path.count(\"/\") + 1)\n\n    # Functions\n\n    if classes and funcs:\n        print(\"\\n**Functions**\\n\", file=overview)\n\n    for _, f in funcs:\n        print(f\"{mod_name}.{f.__name__}\")\n\n        # Add the function to the overview\n        slug = snake_to_kebab(f.__name__)\n        print(\n            li(link(f.__name__, f\"../{mod_short_path}/{slug}\")), end=\"\", file=overview\n        )\n\n        # Write down the function' docstring\n        with open(mod_path.joinpath(slug).with_suffix(\".md\"), \"w\") as file:\n            print_docstring(obj=f, file=file, depth=mod_short_path.count(\".\") + 1)\n\n    # Sub-modules\n    for name, submod in inspect.getmembers(mod, inspect.ismodule):\n        # We only want to go through the public submodules, such as optim.schedulers\n        if (\n            name in (\"tags\", \"typing\", \"inspect\", \"skmultiflow_utils\")\n            or name not in mod.__all__\n            or name.startswith(\"_\")\n        ):\n            continue\n        print_module(mod=submod, path=mod_path, overview=overview, is_submodule=True)\n\n    print(\"\", file=overview)\n</pre> def print_module(mod, path, overview, is_submodule=False):     mod_name = mod.__name__.split(\".\")[-1]      # Create a directory for the module     mod_slug = snake_to_kebab(mod_name)     mod_path = path.joinpath(mod_slug)     mod_short_path = str(mod_path).replace(\"docs/api/\", \"\")     os.makedirs(mod_path, exist_ok=True)     with open(mod_path.joinpath(\".pages\"), \"w\") as f:         f.write(f\"title: {mod_name}\")      # Add the module to the overview     if is_submodule:         print(h3(mod_name), file=overview)     else:         print(h2(mod_name), file=overview)     if mod.__doc__:         print(paragraph(mod.__doc__), file=overview)      # Extract all public classes and functions     ispublic = lambda x: x.__name__ in mod.__all__ and not x.__name__.startswith(\"_\")     classes = inspect.getmembers(mod, lambda x: inspect.isclass(x) and ispublic(x))     funcs = inspect.getmembers(mod, lambda x: inspect.isfunction(x) and ispublic(x))      # Classes      if classes and funcs:         print(\"\\n**Classes**\\n\", file=overview)      for _, c in classes:         print(f\"{mod_name}.{c.__name__}\")          # Add the class to the overview         slug = snake_to_kebab(c.__name__)         print(             li(link(c.__name__, f\"../{mod_short_path}/{slug}\")), end=\"\", file=overview         )          # Write down the class' docstring         with open(mod_path.joinpath(slug).with_suffix(\".md\"), \"w\") as file:             print_docstring(obj=c, file=file, depth=mod_short_path.count(\"/\") + 1)      # Functions      if classes and funcs:         print(\"\\n**Functions**\\n\", file=overview)      for _, f in funcs:         print(f\"{mod_name}.{f.__name__}\")          # Add the function to the overview         slug = snake_to_kebab(f.__name__)         print(             li(link(f.__name__, f\"../{mod_short_path}/{slug}\")), end=\"\", file=overview         )          # Write down the function' docstring         with open(mod_path.joinpath(slug).with_suffix(\".md\"), \"w\") as file:             print_docstring(obj=f, file=file, depth=mod_short_path.count(\".\") + 1)      # Sub-modules     for name, submod in inspect.getmembers(mod, inspect.ismodule):         # We only want to go through the public submodules, such as optim.schedulers         if (             name in (\"tags\", \"typing\", \"inspect\", \"skmultiflow_utils\")             or name not in mod.__all__             or name.startswith(\"_\")         ):             continue         print_module(mod=submod, path=mod_path, overview=overview, is_submodule=True)      print(\"\", file=overview) In\u00a0[\u00a0]: Copied! <pre>if __name__ == \"__main__\":\n    api_path = pathlib.Path(\"docs/api\")\n\n    # Create a directory for the API reference\n    shutil.rmtree(api_path, ignore_errors=True)\n    os.makedirs(api_path, exist_ok=True)\n    with open(api_path.joinpath(\".pages\"), \"w\") as f:\n        f.write(\"title: API reference\\narrange:\\n  - overview.md\\n  - ...\\n\")\n\n    overview = open(api_path.joinpath(\"overview.md\"), \"w\")\n    print(h1(\"Overview\"), file=overview)\n\n    linkifier = Linkifier()\n\n    for mod_name, mod in inspect.getmembers(\n        importlib.import_module(f\"{package}\"), inspect.ismodule\n    ):\n        if mod_name.startswith(\"_\"):\n            continue\n        print(mod_name)\n        print_module(mod, path=api_path, overview=overview)\n</pre> if __name__ == \"__main__\":     api_path = pathlib.Path(\"docs/api\")      # Create a directory for the API reference     shutil.rmtree(api_path, ignore_errors=True)     os.makedirs(api_path, exist_ok=True)     with open(api_path.joinpath(\".pages\"), \"w\") as f:         f.write(\"title: API reference\\narrange:\\n  - overview.md\\n  - ...\\n\")      overview = open(api_path.joinpath(\"overview.md\"), \"w\")     print(h1(\"Overview\"), file=overview)      linkifier = Linkifier()      for mod_name, mod in inspect.getmembers(         importlib.import_module(f\"{package}\"), inspect.ismodule     ):         if mod_name.startswith(\"_\"):             continue         print(mod_name)         print_module(mod, path=api_path, overview=overview)"}]}